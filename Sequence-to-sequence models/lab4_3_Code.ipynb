{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f4a5ac4",
      "metadata": {
        "deletable": false,
        "editable": false,
        "execution": {
          "iopub.execute_input": "2025-02-02T21:12:02.979322Z",
          "iopub.status.busy": "2025-02-02T21:12:02.979003Z",
          "iopub.status.idle": "2025-02-02T21:12:04.351945Z",
          "shell.execute_reply": "2025-02-02T21:12:04.350865Z"
        },
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": true
        },
        "id": "0f4a5ac4"
      },
      "outputs": [],
      "source": [
        "# Please do not change this cell because some hidden tests might depend on it.\n",
        "import os\n",
        "\n",
        "# Otter grader does not handle ! commands well, so we define and use our\n",
        "# own function to execute shell commands.\n",
        "def shell(commands, warn=True):\n",
        "    \"\"\"Executes the string `commands` as a sequence of shell commands.\n",
        "\n",
        "       Prints the result to stdout and returns the exit status.\n",
        "       Provides a printed warning on non-zero exit status unless `warn`\n",
        "       flag is unset.\n",
        "    \"\"\"\n",
        "    file = os.popen(commands)\n",
        "    print (file.read().rstrip('\\n'))\n",
        "    exit_status = file.close()\n",
        "    if warn and exit_status != None:\n",
        "        print(f\"Completed with errors. Exit status: {exit_status}\\n\")\n",
        "    return exit_status\n",
        "\n",
        "shell(\"\"\"\n",
        "ls requirements.txt >/dev/null 2>&1\n",
        "if [ ! $? = 0 ]; then\n",
        " rm -rf .tmp\n",
        " git clone https://github.com/cs236299-2024-winter/lab4-3.git .tmp\n",
        " mv .tmp/tests ./\n",
        " mv .tmp/requirements.txt ./\n",
        " rm -rf .tmp\n",
        "fi\n",
        "pip install -q -r requirements.txt\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Monter Google Drive pour accéder à vos fichiers\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Copier les fichiers depuis Google Drive vers l'environnement Colab\n",
        "!cp -r /content/drive/MyDrive/nlp_lab/lab4-3* .\n",
        "\n",
        "# Installer les dépendances depuis le fichier requirements.txt\n",
        "!pip install -r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt\n",
        "\n",
        "# Installer Otter Grader (si nécessaire)\n",
        "!pip install otter-grader\n",
        "\n",
        "# Fonction pour exécuter les commandes shell\n",
        "import os\n",
        "\n",
        "def shell(commands, warn=True):\n",
        "    \"\"\"Exécute des commandes shell et affiche les résultats.\"\"\"\n",
        "    file = os.popen(commands)\n",
        "    print(file.read().rstrip('\\n'))\n",
        "    exit_status = file.close()\n",
        "    if warn and exit_status is not None:\n",
        "        print(f\"Command failed with exit code {exit_status}\")\n",
        "    return exit_status\n",
        "\n",
        "# Vérifier si requirements.txt existe et télécharger le dépôt si nécessaire\n",
        "shell(\"\"\"\n",
        "ls requirements.txt >/dev/null 2>&1\n",
        "if [ ! $? = 0 ]; then\n",
        "    rm -rf .tmp\n",
        "    git clone https://github.com/cs236299-2024-winter/lab1-4.git .tmp\n",
        "    mv .tmp/tests ./tests\n",
        "    mv .tmp/requirements.txt ./requirements.txt\n",
        "    rm -rf .tmp\n",
        "fi\n",
        "pip install -q -r requirements.txt\n",
        "\"\"\")\n",
        "\n",
        "# Copier les fichiers tests depuis Google Drive si le dossier 'tests' est introuvable\n",
        "if not os.path.exists('tests/token_count.py'):\n",
        "    print(\"Téléchargement des fichiers nécessaires depuis Google Drive...\")\n",
        "    !mkdir -p tests\n",
        "    !cp -r /content/drive/MyDrive/nlp_lab/lab4-3/tests/* ./tests/\n",
        "else:\n",
        "    print(\"Les fichiers de tests sont déjà présents.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3BYHWZzHm2Q",
        "outputId": "77ac80ee-2bdd-4edb-8f21-8621bdb10ed8"
      },
      "id": "i3BYHWZzHm2Q",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Collecting otter-grader==1.0.0 (from -r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1))\n",
            "  Downloading otter_grader-1.0.0-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 2)) (2.5.1+cu124)\n",
            "Collecting wget (from -r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 3))\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 4)) (0.21.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 5)) (4.47.1)\n",
            "Collecting datasets>=2.14.6 (from -r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 6))\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1)) (6.0.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1)) (5.10.4)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (from otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1)) (7.34.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.11/dist-packages (from otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1)) (7.16.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1)) (4.67.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1)) (75.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1)) (2.2.2)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.11/dist-packages (from otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1)) (6.4.2)\n",
            "Collecting docker (from otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1))\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1)) (3.1.5)\n",
            "Collecting dill (from otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1))\n",
            "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pdfkit (from otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1))\n",
            "  Downloading pdfkit-1.0.0-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting PyPDF2 (from otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1))\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 2)) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 2)) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 2)) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 2)) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 2))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 2))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 2))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 2))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 2))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 2))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 2))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 2))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 2))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 2)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 2)) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 2))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 2)) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 2)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 4)) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 5)) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 5)) (24.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 5)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 5)) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 5)) (0.5.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.14.6->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 6)) (17.0.0)\n",
            "Collecting dill (from otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1))\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from datasets>=2.14.6->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 6))\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets>=2.14.6->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 6))\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec (from torch->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 2))\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.14.6->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 6)) (3.11.11)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.14.6->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 6)) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.14.6->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 6)) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.14.6->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 6)) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.14.6->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 6)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.14.6->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 6)) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.14.6->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 6)) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.14.6->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 6)) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 5)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 5)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 5)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 5)) (2024.12.14)\n",
            "Collecting jedi>=0.16 (from ipython->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1))\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1)) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1)) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1)) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1)) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1)) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1)) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1)) (3.0.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1)) (4.12.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1)) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1)) (0.7.1)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.11/dist-packages (from nbconvert->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1)) (5.7.2)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1)) (0.3.0)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1)) (3.1.1)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1)) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1)) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1)) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1)) (4.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1)) (2025.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1)) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1)) (1.4.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1)) (0.8.4)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1)) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1)) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1)) (0.22.3)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.7->nbconvert->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1)) (4.3.6)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from nbclient>=0.5.0->nbconvert->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1)) (6.1.12)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1)) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1)) (1.17.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1)) (2.6)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-3/requirements.txt (line 1)) (24.0.1)\n",
            "Downloading otter_grader-1.0.0-py3-none-any.whl (163 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.0/164.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m843.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfkit-1.0.0-py3-none-any.whl (12 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=d41c3f9d3e0adc780d8e2c93a90770b0420ceab7c86ac74b829039851636ebbe\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/b3/0f/a40dbd1c6861731779f62cc4babcb234387e11d697df70ee97\n",
            "Successfully built wget\n",
            "Installing collected packages: wget, pdfkit, xxhash, PyPDF2, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, jedi, fsspec, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, docker, nvidia-cusolver-cu12, datasets, otter-grader\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PyPDF2-3.0.1 datasets-3.2.0 dill-0.3.8 docker-7.1.0 fsspec-2024.9.0 jedi-0.19.2 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 otter-grader-1.0.0 pdfkit-1.0.0 wget-3.2 xxhash-3.5.0\n",
            "Requirement already satisfied: otter-grader in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from otter-grader) (6.0.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from otter-grader) (5.10.4)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (from otter-grader) (7.34.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.11/dist-packages (from otter-grader) (7.16.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from otter-grader) (4.67.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from otter-grader) (75.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from otter-grader) (2.2.2)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.11/dist-packages (from otter-grader) (6.4.2)\n",
            "Requirement already satisfied: docker in /usr/local/lib/python3.11/dist-packages (from otter-grader) (7.1.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from otter-grader) (3.1.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from otter-grader) (0.3.8)\n",
            "Requirement already satisfied: pdfkit in /usr/local/lib/python3.11/dist-packages (from otter-grader) (1.0.0)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (from otter-grader) (3.0.1)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from docker->otter-grader) (2.32.3)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker->otter-grader) (2.3.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->otter-grader) (3.0.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert->otter-grader) (4.12.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert->otter-grader) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert->otter-grader) (0.7.1)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.11/dist-packages (from nbconvert->otter-grader) (5.7.2)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert->otter-grader) (0.3.0)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert->otter-grader) (3.1.1)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert->otter-grader) (0.10.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from nbconvert->otter-grader) (24.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert->otter-grader) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->otter-grader) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->otter-grader) (4.23.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas->otter-grader) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->otter-grader) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->otter-grader) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->otter-grader) (2025.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->otter-grader) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert->otter-grader) (1.4.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython->otter-grader) (0.8.4)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->otter-grader) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->otter-grader) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->otter-grader) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->otter-grader) (0.22.3)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.7->nbconvert->otter-grader) (4.3.6)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from nbclient>=0.5.0->nbconvert->otter-grader) (6.1.12)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython->otter-grader) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->otter-grader) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->otter-grader) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->docker->otter-grader) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->docker->otter-grader) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->docker->otter-grader) (2024.12.14)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert->otter-grader) (2.6)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert->otter-grader) (24.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from referencing>=0.28.4->jsonschema>=2.6->nbformat->otter-grader) (4.12.2)\n",
            "\n",
            "Command failed with exit code 256\n",
            "Téléchargement des fichiers nécessaires depuis Google Drive...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8334fc56",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "8334fc56"
      },
      "outputs": [],
      "source": [
        "# Initialize Otter\n",
        "import otter\n",
        "grader = otter.Notebook()"
      ]
    },
    {
      "cell_type": "raw",
      "id": "39bbb02c",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "39bbb02c"
      },
      "source": [
        "%%latex\n",
        "\\newcommand{\\vect}[1]{\\mathbf{#1}}\n",
        "\\newcommand{\\cnt}[1]{\\sharp(#1)}\n",
        "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
        "\\newcommand{\\softmax}{\\operatorname{softmax}}\n",
        "\\newcommand{\\Prob}{\\Pr}\n",
        "\\newcommand{\\given}{\\,|\\,}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8dd3185",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "c8dd3185"
      },
      "source": [
        "$$\n",
        "\\renewcommand{\\vect}[1]{\\mathbf{#1}}\n",
        "\\renewcommand{\\cnt}[1]{\\sharp(#1)}\n",
        "\\renewcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
        "\\renewcommand{\\softmax}{\\operatorname{softmax}}\n",
        "\\renewcommand{\\Prob}{\\Pr}\n",
        "\\renewcommand{\\given}{\\,|\\,}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2636e77e",
      "metadata": {
        "tags": [
          "remove_for_latex"
        ],
        "id": "2636e77e"
      },
      "source": [
        "# Course 236299\n",
        "## Lab 4-3 - Sequence-to-sequence models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01b395a0",
      "metadata": {
        "id": "01b395a0"
      },
      "source": [
        "In lab 4-1, you used a syntactic-semantic grammar for semantic parsing to convert natural language to meaning representations. In this lab, we consider an alternative approach, sequence-to-sequence models, which can solve this task by directly learning the mapping from a sequence of inputs to a sequence of outputs. Since sequence-to-sequence models make few assumptions about the data, they can be applied to a variety of tasks, including machine translation, document summarization, and speech recognition.\n",
        "\n",
        "In this lab, you will implement a sequence-to-sequence model in its most basic form (as in [this seminal paper](https://arxiv.org/pdf/1409.3215.pdf)), and apply it to the task of converting English number phrases to numbers, as exemplified in the table below.\n",
        "\n",
        "|                 Input                                            |       Output         |\n",
        "|:-----------------------------------------------------------------|---------------------:|\n",
        "| seven thousand nine hundred and twenty nine                      |       7929           |\n",
        "| eight hundred and forty two thousand two hundred and fifty nine  |       842259         |\n",
        "| five hundred and eight thousand two hundred and seventeen        |       508217         |\n",
        "\n",
        "For this simple task, it is possible to write a rule-based program to do the conversion. However, here we take a learning-based approach and learn the mapping from demonstrations, with the benefit that the system we implement here can be applied to other sequence-to-sequence tasks as well (including the ATIS-to-SQL problem in project segment 4)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "baf76190",
      "metadata": {
        "id": "baf76190"
      },
      "source": [
        "New bits of Pytorch used in this lab, and which you may find useful include:\n",
        "\n",
        "* [torch.transpose](https://pytorch.org/docs/stable/generated/torch.transpose.html): Swaps two dimensions of a tensor.\n",
        "* [torch.reshape](https://pytorch.org/docs/stable/generated/torch.reshape.html): Redistributes the elements of a tensor to form a tensor of a different shape, e.g., from 3 x 4 to 6 x 2.\n",
        "* [torch.nn.utils.rnn.pack_padded_sequence](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html) (imported as `pack`): Handles paddings. A more detailed explanation can be found [here](https://stackoverflow.com/a/56211056)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d898eb5",
      "metadata": {
        "id": "0d898eb5"
      },
      "source": [
        "## Preparation - Loading data {-}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "0f1dda1e",
      "metadata": {
        "deletable": false,
        "execution": {
          "iopub.execute_input": "2025-02-02T21:12:04.355734Z",
          "iopub.status.busy": "2025-02-02T21:12:04.355326Z",
          "iopub.status.idle": "2025-02-02T21:12:07.115776Z",
          "shell.execute_reply": "2025-02-02T21:12:07.114962Z"
        },
        "id": "0f1dda1e"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import csv\n",
        "import math\n",
        "import os\n",
        "import wget\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.pre_tokenizers import WhitespaceSplit\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "from tokenizers import normalizers\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torch.nn.utils.rnn import pack_padded_sequence as pack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0b74c57a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-02-02T21:12:07.119301Z",
          "iopub.status.busy": "2025-02-02T21:12:07.118956Z",
          "iopub.status.idle": "2025-02-02T21:12:07.124657Z",
          "shell.execute_reply": "2025-02-02T21:12:07.123832Z"
        },
        "id": "0b74c57a",
        "outputId": "6a5f35e9-dbc1-4377-e40a-3b6e285cdaa4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# GPU check, make sure to use GPU where available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print (device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1c103a14",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-02T21:12:07.128682Z",
          "iopub.status.busy": "2025-02-02T21:12:07.128384Z",
          "iopub.status.idle": "2025-02-02T21:12:12.209818Z",
          "shell.execute_reply": "2025-02-02T21:12:12.209041Z"
        },
        "id": "1c103a14"
      },
      "outputs": [],
      "source": [
        "# Download data\n",
        "def download_if_needed(source, dest, filename):\n",
        "    os.makedirs(dest, exist_ok=True) # ensure destination\n",
        "    os.path.exists(f\"./{dest}{filename}\") or wget.download(source + filename, out=dest)\n",
        "\n",
        "local_dir = \"data/\"\n",
        "remote_dir = \"https://github.com/nlp-236299/data/raw/master/Words2Num/\"\n",
        "os.makedirs(local_dir, exist_ok=True)\n",
        "\n",
        "for filename in [\n",
        "    \"train.src\",\n",
        "    \"train.tgt\",\n",
        "    \"dev.src\",\n",
        "    \"dev.tgt\",\n",
        "    \"test.src\",\n",
        "    \"test.tgt\",\n",
        "]:\n",
        "    download_if_needed(remote_dir, local_dir, filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f403aa25",
      "metadata": {
        "id": "f403aa25"
      },
      "source": [
        "Next, we process the dataset by extracting the sequences and their corresponding labels and save it in CSV format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "93ba918e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-02T21:12:12.213160Z",
          "iopub.status.busy": "2025-02-02T21:12:12.212925Z",
          "iopub.status.idle": "2025-02-02T21:12:12.354408Z",
          "shell.execute_reply": "2025-02-02T21:12:12.353579Z"
        },
        "id": "93ba918e"
      },
      "outputs": [],
      "source": [
        "# Process data\n",
        "for split in ['train', 'dev', 'test']:\n",
        "    src_in_file = f'{local_dir}{split}.src'\n",
        "    tgt_in_file = f'{local_dir}{split}.tgt'\n",
        "    out_file = f'{local_dir}{split}.csv'\n",
        "\n",
        "    with open(src_in_file, 'r') as f_src_in, open(tgt_in_file, 'r') as f_tgt_in:\n",
        "        with open(out_file, 'w', newline='') as f_out:\n",
        "            src, tgt= [], []\n",
        "            writer = csv.writer(f_out)\n",
        "            writer.writerow(('src','tgt'))\n",
        "            for src_line, tgt_line in zip(f_src_in, f_tgt_in):\n",
        "                writer.writerow((src_line.strip(), tgt_line.strip()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b96f80bc",
      "metadata": {
        "id": "b96f80bc"
      },
      "source": [
        "Let's take a look at what each data file looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "5a6bc404",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-02T21:12:12.357838Z",
          "iopub.status.busy": "2025-02-02T21:12:12.357550Z",
          "iopub.status.idle": "2025-02-02T21:12:12.377749Z",
          "shell.execute_reply": "2025-02-02T21:12:12.376732Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a6bc404",
        "outputId": "6e1cefe4-8235-4111-b542-a76bd188d416"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src,tgt\n",
            "seven thousand nine hundred and twenty nine,7 9 2 9\n",
            "eight hundred and forty two thousand two hundred and fifty nine,8 4 2 2 5 9\n",
            "five hundred and eight thousand two hundred and seventeen,5 0 8 2 1 7\n",
            "one hundred and thirty three million three hundred and sixty two thousand five hundred and twenty,1 3 3 3 6 2 5 2 0\n",
            "seventy nine million five hundred and sixty four thousand six hundred and thirty six,7 9 5 6 4 6 3 6\n",
            "four million seven hundred and forty three thousand nine hundred and twenty nine,4 7 4 3 9 2 9\n",
            "nine hundred and eighty nine million five hundred and ninety five thousand five hundred and thirteen,9 8 9 5 9 5 5 1 3\n",
            "three hundred and forty four thousand three hundred and seventy three,3 4 4 3 7 3\n",
            "one thousand six hundred and sixty seven,1 6 6 7\n"
          ]
        }
      ],
      "source": [
        "shell('head \"data/dev.csv\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8926fdd",
      "metadata": {
        "id": "f8926fdd"
      },
      "source": [
        "# The dataset\n",
        "\n",
        "Let's take a first look at a few lines of the dataset of English number phrases and their translations into digit-sequence form."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "c4c8d447",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-02T21:12:12.381379Z",
          "iopub.status.busy": "2025-02-02T21:12:12.380902Z",
          "iopub.status.idle": "2025-02-02T21:12:12.386461Z",
          "shell.execute_reply": "2025-02-02T21:12:12.385628Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4c8d447",
        "outputId": "0139850f-1bcb-49d8-ec0e-4b84c49e84c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src                                                                             tgt\n",
            "seven thousand nine hundred and twenty nine                                 7 9 2 9\n",
            "eight hundred and forty two thousand two hundred and fifty nine         8 4 2 2 5 9\n",
            "five hundred and eight thousand two hundred and seventeen               5 0 8 2 1 7\n"
          ]
        }
      ],
      "source": [
        "with open(local_dir + \"dev.csv\") as f:\n",
        "    for line, _ in zip(f, range(4)):\n",
        "        src, tgt = line.strip().split(',')\n",
        "        print (f'{src.strip():70s} {tgt.strip():>12s}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ed594bf",
      "metadata": {
        "id": "7ed594bf"
      },
      "source": [
        "As before, we use HuggingFace's `datasets` to load data. We use two fields: `SRC` for processing the source side (the English number phrases) and `TGT` for processing the target side (the digit sequences)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8f646328",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-02T21:12:12.389588Z",
          "iopub.status.busy": "2025-02-02T21:12:12.389364Z",
          "iopub.status.idle": "2025-02-02T21:12:12.841123Z",
          "shell.execute_reply": "2025-02-02T21:12:12.840194Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371,
          "referenced_widgets": [
            "7858ea8e609a42a791d6c7fb2a2adc7b",
            "2735ef48034b4d6192b569166262e0d5",
            "f5873677159b43c7883c3638c3f04ac4",
            "061896529f7f4f30884a1911d96168f2",
            "315c69bcdf4d4809b9b4413728c353f3",
            "bcf156c0a54a45439996df7beadc7326",
            "cb124c76508a402782d4f1f83f89fcae",
            "4e50935035b64b84bc89185389b95480",
            "67151d11c4ee42a78ff66d8d5ad25db2",
            "bb677e2847ca4d0a942209399cc11d23",
            "f72fbc07ce5f4cc682697f5290e33116",
            "7e82a07c4df045f1a74ba0364bbcc55a",
            "890e7a0fb956433296819dc669dd5bf4",
            "2cb6f291b6a44ea4a951c14e7b67a8b9",
            "5191adcff46147d5b84e7c5816ca795f",
            "36094a8d542044b29a896f5425914b05",
            "0efdc19e8cdb446b81d4735fd15b0c95",
            "3fa1973979df47ac95837964880ba9a3",
            "08fc086d3d974cdb8a7d73e1bbd6a97e",
            "4d3f8b2729394f6fa6eb6ef08148c8a1",
            "8f9f97955dc34ae1be0b0c4062887cf6",
            "6c6f47c2312540d298e1908d0ae43909",
            "24278aa3a6b94d7c90d173ade7c3d842",
            "00d4c697177c48dab61d1858373468d5",
            "56afd2d0c99048c6bf43589974f97d46",
            "fbe1140ba1fe445fb51ce0bbe5df86b1",
            "797af22d48984c81a538915e35d8f16c",
            "aa80fef5905045e98afbc1990bbfdd5e",
            "01f19c85f67440559a2943934e870550",
            "50120b33eb784f53b89576bb9d278e27",
            "4bc59238ca894686b254f396b1f72eac",
            "163a913fadf648a18ab668c233d7e9a6",
            "aed2de9c28914d8db2e7aa63a8195b51"
          ]
        },
        "id": "8f646328",
        "outputId": "49b1e5bf-14ac-46d1-8d06-4df2dec20c4f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7858ea8e609a42a791d6c7fb2a2adc7b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating val split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7e82a07c4df045f1a74ba0364bbcc55a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "24278aa3a6b94d7c90d173ade7c3d842"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['src', 'tgt'],\n",
              "        num_rows: 65022\n",
              "    })\n",
              "    val: Dataset({\n",
              "        features: ['src', 'tgt'],\n",
              "        num_rows: 700\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['src', 'tgt'],\n",
              "        num_rows: 700\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "dataset = load_dataset('csv', data_files={'train':f'{local_dir}train.csv', \\\n",
        "                                          'val': f'{local_dir}dev.csv', \\\n",
        "                                          'test': f'{local_dir}test.csv'})\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b960f1e8",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-02T21:12:12.844141Z",
          "iopub.status.busy": "2025-02-02T21:12:12.843917Z",
          "iopub.status.idle": "2025-02-02T21:12:12.847599Z",
          "shell.execute_reply": "2025-02-02T21:12:12.846704Z"
        },
        "id": "b960f1e8"
      },
      "outputs": [],
      "source": [
        "train_data = dataset['train']\n",
        "val_data = dataset['val']\n",
        "test_data = dataset['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "0716b465",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-02T21:12:12.850370Z",
          "iopub.status.busy": "2025-02-02T21:12:12.850153Z",
          "iopub.status.idle": "2025-02-02T21:12:13.421439Z",
          "shell.execute_reply": "2025-02-02T21:12:13.420622Z"
        },
        "id": "0716b465"
      },
      "outputs": [],
      "source": [
        "unk_token = '[UNK]'\n",
        "pad_token = '[PAD]'\n",
        "bos_token = '<bos>'\n",
        "eos_token = '<eos>'\n",
        "src_tokenizer = Tokenizer(WordLevel(unk_token=unk_token))\n",
        "src_tokenizer.pre_tokenizer = WhitespaceSplit()\n",
        "\n",
        "src_trainer = WordLevelTrainer(special_tokens=[pad_token, unk_token])\n",
        "src_tokenizer.train_from_iterator(train_data['src'], trainer=src_trainer)\n",
        "\n",
        "tgt_tokenizer = Tokenizer(WordLevel(unk_token=unk_token))\n",
        "tgt_tokenizer.pre_tokenizer = WhitespaceSplit()\n",
        "\n",
        "tgt_trainer = WordLevelTrainer(special_tokens=[pad_token, unk_token, bos_token, eos_token])\n",
        "\n",
        "tgt_tokenizer.train_from_iterator(train_data['tgt'], trainer=tgt_trainer)\n",
        "\n",
        "tgt_tokenizer.post_processor = \\\n",
        "  TemplateProcessing(single=f\"{bos_token} $A {eos_token}\",\n",
        "                     special_tokens=[(bos_token,\n",
        "                                      tgt_tokenizer.token_to_id(bos_token)),\n",
        "                                     (eos_token,\n",
        "                                      tgt_tokenizer.token_to_id(eos_token))])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3524b08",
      "metadata": {
        "id": "c3524b08"
      },
      "source": [
        "Note that we prepended `<bos>` and appended `<eos>` to target sentences. The purpose for introducing them will become clear in later parts of this lab.\n",
        "\n",
        "We use `datasets.Dataset.map` to convert text into word ids. As shown in lab 1-5, first we need to wrap `tokenizer` with the `transformers.PreTrainedTokenizerFast` class to be compatible with the `datasets` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "cc958579",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-02T21:12:13.424940Z",
          "iopub.status.busy": "2025-02-02T21:12:13.424694Z",
          "iopub.status.idle": "2025-02-02T21:12:13.429998Z",
          "shell.execute_reply": "2025-02-02T21:12:13.429250Z"
        },
        "id": "cc958579"
      },
      "outputs": [],
      "source": [
        "hf_src_tokenizer = PreTrainedTokenizerFast(tokenizer_object=src_tokenizer,\n",
        "                                           pad_token=pad_token,\n",
        "                                           unk_token=unk_token)\n",
        "hf_tgt_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tgt_tokenizer,\n",
        "                                           pad_token=pad_token,\n",
        "                                           unk_token=unk_token,\n",
        "                                           bos_token=bos_token,\n",
        "                                           eos_token=eos_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "6551b02a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-02T21:12:13.432737Z",
          "iopub.status.busy": "2025-02-02T21:12:13.432518Z",
          "iopub.status.idle": "2025-02-02T21:12:27.358674Z",
          "shell.execute_reply": "2025-02-02T21:12:27.357752Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "251a4f2e14c24ab99fd459a4ffff5e19",
            "29cd6b57cd6347f8b823d9ca90bf1619",
            "4695b4224e1945bfb78ad264a0444183",
            "3d89c688ab144b19818b3227bd7c188a",
            "8b12b3726f414e52950e71b435aadee5",
            "45a86a96421b4009b7345160a1d66220",
            "9fae26f3cc1c4ec88e28ba784495ff0a",
            "d9b5b7ae96e24ad8b1db14c86fa275ee",
            "1bf20950292243918ec2085cc55da258",
            "6c757bb251654a1fbd7b1faf1682a05e",
            "0e44090808244703b6973007da9694b5",
            "61608a0b44bb42988672a561336ec95e",
            "b7309c39b6dd466fa2a691391de3381b",
            "f9d979f17dd54327913962fbb352b3c2",
            "331be9c2186f4b1688c0d5aa6ccb3479",
            "3e272d7774ee414aa90bcb2aca937b9b",
            "cbcf94f8a9764b42ac87182eda58444a",
            "2c3ba1a9d7b2404b9fb3e27388d13398",
            "91ed3319dae34d39b39401fc6f172e87",
            "ac98b7f9e20541c2835490db9caac781",
            "fff228b004094ec7bc0d489f129842f1",
            "e8b434242ca4455c9464036bd58e496e",
            "d2d055657e3c440ca238e4d9893feb03",
            "edfc2c34747f4f54b12534f00c417ef8",
            "d8ab479345054862991aecfc197d9d90",
            "8922d01d910c4ba48072958c66cf28e6",
            "b2ed7fc95cac4752a4bd0f67ace5546b",
            "bf11fd54a020424c9b1e83e41b8ac88d",
            "708bf08bacce485a81d5e2ed374329a4",
            "dae9c6967488407d88d95dd0ea54c6f1",
            "494bf533344c49e68a2c58373f7f25fe",
            "cafb7422aaa94edfb05b9e89a25b37b4",
            "5350afb6e2d040ebb2a0381c6ae66183"
          ]
        },
        "id": "6551b02a",
        "outputId": "dfafed81-cfce-4516-a0d6-faff8c25f62c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/65022 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "251a4f2e14c24ab99fd459a4ffff5e19"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/700 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "61608a0b44bb42988672a561336ec95e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/700 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d2d055657e3c440ca238e4d9893feb03"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "def encode(example):\n",
        "    example['src_ids'] = hf_src_tokenizer(example['src']).input_ids\n",
        "    example['tgt_ids'] = hf_tgt_tokenizer(example['tgt']).input_ids\n",
        "    return example\n",
        "\n",
        "train_data = train_data.map(encode)\n",
        "val_data = val_data.map(encode)\n",
        "test_data = test_data.map(encode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "1e179f26",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-02T21:12:27.362356Z",
          "iopub.status.busy": "2025-02-02T21:12:27.362132Z",
          "iopub.status.idle": "2025-02-02T21:12:27.367055Z",
          "shell.execute_reply": "2025-02-02T21:12:27.366315Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e179f26",
        "outputId": "11f4658a-db96-4171-d250-9afb50beef7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of src vocab: 34\n",
            "Size of tgt vocab: 14\n",
            "Index for src padding: 0\n",
            "Index for tgt padding: 0\n",
            "Index for start of sequence token: 2\n",
            "Index for end of sequence token: 3\n"
          ]
        }
      ],
      "source": [
        "# Compute size of vocabularies\n",
        "src_vocab = hf_src_tokenizer.get_vocab()\n",
        "tgt_vocab = hf_tgt_tokenizer.get_vocab()\n",
        "\n",
        "print(f\"Size of src vocab: {len(src_vocab)}\")\n",
        "print(f\"Size of tgt vocab: {len(tgt_vocab)}\")\n",
        "print(f\"Index for src padding: {src_vocab[pad_token]}\")\n",
        "print(f\"Index for tgt padding: {tgt_vocab[pad_token]}\")\n",
        "print(f\"Index for start of sequence token: {tgt_vocab[bos_token]}\")\n",
        "print(f\"Index for end of sequence token: {tgt_vocab[eos_token]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa43a9b7",
      "metadata": {
        "id": "fa43a9b7"
      },
      "source": [
        "+To load data in batched tensors, we use `torch.utils.data.DataLoader` for data splits, which enables us to iterate over the dataset under a given `BATCH_SIZE`. For the test set, we use a batch size of 1, to make the decoding implementation easier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "dc00731b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-02T21:12:27.370098Z",
          "iopub.status.busy": "2025-02-02T21:12:27.369879Z",
          "iopub.status.idle": "2025-02-02T21:12:27.377860Z",
          "shell.execute_reply": "2025-02-02T21:12:27.377111Z"
        },
        "id": "dc00731b"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32     # batch size for training and validation\n",
        "TEST_BATCH_SIZE = 1 # batch size for test; we use 1 to make implementation easier\n",
        "\n",
        "# Defines how to batch a list of examples together\n",
        "def collate_fn(examples):\n",
        "    batch = {}\n",
        "    bsz = len(examples)\n",
        "    src_ids, tgt_ids = [], []\n",
        "    for example in examples:\n",
        "        src_ids.append(example['src_ids'])\n",
        "        tgt_ids.append(example['tgt_ids'])\n",
        "\n",
        "    src_len = torch.LongTensor([len(word_ids) for word_ids in src_ids]).to(device)\n",
        "    src_max_length = max(src_len)\n",
        "    tgt_max_length = max([len(word_ids) for word_ids in tgt_ids])\n",
        "\n",
        "    src_batch = torch.zeros(bsz, src_max_length).long().fill_(src_vocab[pad_token]).to(device)\n",
        "    tgt_batch = torch.zeros(bsz, tgt_max_length).long().fill_(tgt_vocab[pad_token]).to(device)\n",
        "    for b in range(bsz):\n",
        "        src_batch[b][:len(src_ids[b])] = torch.LongTensor(src_ids[b]).to(device)\n",
        "        tgt_batch[b][:len(tgt_ids[b])] = torch.LongTensor(tgt_ids[b]).to(device)\n",
        "\n",
        "    batch['src_lengths'] = src_len\n",
        "    batch['src_ids'] = src_batch\n",
        "    batch['tgt_ids'] = tgt_batch\n",
        "    return batch\n",
        "\n",
        "train_iter = torch.utils.data.DataLoader(train_data,\n",
        "                                         batch_size=BATCH_SIZE,\n",
        "                                         shuffle=True,\n",
        "                                         collate_fn=collate_fn)\n",
        "val_iter = torch.utils.data.DataLoader(val_data,\n",
        "                                       batch_size=BATCH_SIZE,\n",
        "                                       shuffle=False,\n",
        "                                       collate_fn=collate_fn)\n",
        "test_iter = torch.utils.data.DataLoader(test_data,\n",
        "                                        batch_size=TEST_BATCH_SIZE,\n",
        "                                        shuffle=False,\n",
        "                                        collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "793009e9",
      "metadata": {
        "id": "793009e9"
      },
      "source": [
        "Let's take a look at a batch from these iterators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "bc036644",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-02T21:12:27.380675Z",
          "iopub.status.busy": "2025-02-02T21:12:27.380457Z",
          "iopub.status.idle": "2025-02-02T21:12:27.393307Z",
          "shell.execute_reply": "2025-02-02T21:12:27.392445Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc036644",
        "outputId": "ba5cc9bb-9068-4d71-dfcc-a71cf17e6ef1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of src batch: torch.Size([32, 19])\n",
            "Third src sentence in batch: tensor([ 9,  3,  2, 16, 10,  4, 12,  3,  2, 22, 12,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0], device='cuda:0')\n",
            "Length of the third src sentence in batch: 19\n",
            "Converted back to string: three hundred and ninety nine thousand one hundred and fifty one [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Size of tgt batch: torch.Size([32, 12])\n",
            "Third tgt sentence in batch: tensor([ 2,  4,  5,  5, 12,  9, 12,  3,  0,  0,  0,  0], device='cuda:0')\n",
            "Converted back to string: <bos> 3 9 9 1 5 1 <eos> [PAD] [PAD] [PAD] [PAD]\n"
          ]
        }
      ],
      "source": [
        "batch = next(iter(train_iter))\n",
        "src_ids = batch['src_ids']\n",
        "src_example = src_ids[2]\n",
        "print (f\"Size of src batch: {src_ids.size()}\")\n",
        "print (f\"Third src sentence in batch: {src_example}\")\n",
        "print (f\"Length of the third src sentence in batch: {len(src_example)}\")\n",
        "print (f\"Converted back to string: {hf_src_tokenizer.decode(src_example)}\")\n",
        "\n",
        "tgt_ids = batch['tgt_ids']\n",
        "tgt_example = tgt_ids[2]\n",
        "print (f\"Size of tgt batch: {tgt_ids.size()}\")\n",
        "print (f\"Third tgt sentence in batch: {tgt_example}\")\n",
        "print (f\"Converted back to string: {hf_tgt_tokenizer.decode(tgt_example)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ac2618b",
      "metadata": {
        "id": "5ac2618b"
      },
      "source": [
        "# Neural Encoder-Decoder Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0cb74b0",
      "metadata": {
        "id": "d0cb74b0"
      },
      "source": [
        "Sequence-to-sequence models are sometimes called neural encoder-decoder models, as they consist of an encoder, which maps a sequence of source tokens into some vector representations, and a decoder, which generates a sequence of output words from those encoded vectors.\n",
        "\n",
        "Formally, given a sequence of source tokens $\\vect{x} = x_1, \\ldots, x_S$, the goal is to map it to a sequence of target tokens $\\vect{y} = y_1, \\ldots, y_T$.\n",
        "\n",
        "In practice, we prepend a special beginning-of-sequence symbol $y_0$=`<bos>` to the target sequence. Further, in order to provide a way of knowing when to stop generating $\\vect{y}$, we append a special end-of-sequence symbol $y_{T+1}$=`<eos>` to the target sequence, such that when it is produced by the model, the generation process stops.\n",
        "\n",
        "The generation process is structured as a generative model:\n",
        "\n",
        "$$\n",
        "\\Pr(y_0, \\ldots, y_{T+1} \\mid x_1, \\ldots, x_S) = \\prod_{t=1}^{T+1} \\Pr(y_t \\mid y_{<t}, x_1, \\ldots, x_S),\n",
        "$$\n",
        "\n",
        "where $y_{<t}$ denotes the tokens before $y_t$ (that is, $y_0, \\ldots, y_{t-1}$).\n",
        "\n",
        "We use a recurrent neural network with parameters $\\theta$ to parameterize $\\Pr(y_t \\mid y_{<t}, x_1, \\ldots, x_S)$:\n",
        "\n",
        "$$\n",
        "\\Pr(y_t \\mid y_{<t}, x_1, \\ldots, x_S) \\approx {\\Pr}_\\theta(y_t \\mid y_{<t}, x_1, \\ldots, x_S),\n",
        "$$\n",
        "\n",
        "or equivalently,\n",
        "\n",
        "$$\n",
        "{\\Pr}_\\theta(y_1, \\ldots, y_T \\mid x_1, \\ldots, x_S) = \\prod_{t=1}^{T+1} {\\Pr}_\\theta(y_t \\mid y_{<t}, x_1, \\ldots, x_S)\n",
        "$$\n",
        "\n",
        "In neural encoder-decoder models, we first use an encoder to encode $\\vect{x}$ into some vectors (either of fixed length as we'll see in this lab, or of varying length as we'll see in the next lab). Based on the encoded vectors, we use a decoder to generate $\\vect{y}$:\n",
        "\n",
        "$$\n",
        "{\\Pr}_\\theta(y_t \\mid y_{<t}, x_1, \\ldots, x_S) = \\text{decode}(\\text{encode}(x_1, \\ldots, x_S), y_{<t})\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e0fce1e",
      "metadata": {
        "id": "2e0fce1e"
      },
      "source": [
        "### RNN Encoder-Decoders"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "003b8045",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "003b8045"
      },
      "source": [
        "We can use any recurrent neural networks such as LSTMs as encoders and decoders. In this lab, we will use a bidirectional LSTM as the encoder, and a unidirectional LSTM as the decoder, as shown in the illustration below.\n",
        "\n",
        "<img src=\"https://github.com/nlp-course/data/raw/master/img/encoder_decoder.png\" alt=\"encoder-decoder illustration\" />\n",
        "\n",
        "In the above illustration, $S=4$, $T=3$, and there are two encoder/decoder layers. Since we are using a bidirectional encoder, for each layer there are two final states, one for the cell running from left to right (such as $h_{0,4}$), and the other for the cell running from right to left (such as $h_{0,4}'$). We concatenate these two states and use the result to initialize the corresponding layer of the decoder. (In the example, we concatenate $h_{0,4}$ and $h_{0,4}'$ to initialize layer 0, and we concatenate $h_{1,4}$ and $h_{1,4}'$ to initialize layer 1.) Therefore, to make the sizes match, we set the hidden state size of the encoder to be half of that of the decoder.\n",
        "\n",
        "Note that in PyTorch's LSTM implementation, the final hidden state is represented as a tuple `(h, c)` ([documentation here](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)), so we want to apply the same operations to `c` to initialize the decoder.\n",
        "\n",
        "You'll implement `forward_encoder` and `forward_decoder` in the code below. The `forward_encoder` function will be reminiscent of a sequence model from labs 2-* and project segment 2. It operates on a batch of source examples and proceeds as follows:\n",
        "\n",
        "1. Map the input words to some word embeddings. You'll notice that the embedding size is an argument to the model.\n",
        "\n",
        "2. Optionally \"pack\" the sequences to save some computation using `torch.nn.utils.rnn.pack_padded_sequence`, imported above as `pack`.\n",
        "\n",
        "3. Run the encoder RNN (a bidirectional LSTM) over the batch, generating a batch of output states.\n",
        "\n",
        "4. Reshape the final state information (which will have `h` and `c` components each of half the size needed to initialize the decoder) so that it is appropriate to initialize the decoder with.\n",
        "\n",
        "The `forward_decoder` function takes the reshaped encoder final state information and the ground truth target sequences and returns logits (unnormalized log probs) for each target word. (These are ready to be converted to probability distributions via a softmax.)\n",
        "\n",
        "The steps in decoding are:\n",
        "\n",
        "1. Map the target words to word embeddings.\n",
        "\n",
        "2. Run the decoder RNN (a unidirectional LSTM) over the batch, initializing the hidden units from the encoder final states, generating a batch of output states.\n",
        "\n",
        "3. Map the RNN outputs to vectors of vocabulary size (so that they could be softmaxed into a distribution over the vocabulary).\n",
        "\n",
        "The components that you'll be plugging together to do all this are already established in the `__init__` method.\n",
        "\n",
        "> The major exception is the reshaping of the encoder output `h` and `c` to form the decoder input `h` and `c`. **This is the trickiest part.** As usual, your best strategy is to keep careful track of the shapes of each input and output of a layer or operation. We recommend that you try out just the reshaping code on small sample data to test it out before running any encodings or decodings.\n",
        "\n",
        "**Hint #1:** We've provided an auxiliary notebook in the course website, called `reshaping.ipynb`, that discusses the reshaping issue in some detail. You'll want to look it over. Another, summarized and more readable option, is to use [`einops.rearrange`](https://einops.rocks/api/rearrange/).\n",
        "\n",
        "**Hint #2:** The total number of `for` loops in our solution code for the parts you are to write is...zero.\n",
        "\n",
        "**Hint #3:** According to the documentation of [torch.nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html), its outputs are: `outputs, (h, c)`. `outputs` contains all the intermediate states, which you don't need in this lab. You will need `h` and `c`, both of them have the shape: `(num_layers * num_directions, batch_size, hidden_size)`.\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: encoder_decoder\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "abeb1a6e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-02T21:12:27.396562Z",
          "iopub.status.busy": "2025-02-02T21:12:27.396340Z",
          "iopub.status.idle": "2025-02-02T21:12:27.418443Z",
          "shell.execute_reply": "2025-02-02T21:12:27.417685Z"
        },
        "id": "abeb1a6e"
      },
      "outputs": [],
      "source": [
        "# TODO - finish implementing the `forward_encoder` and `forward_decoder` methods\n",
        "class EncoderDecoder(nn.Module):\n",
        "    def __init__(self, hf_src_tokenizer, hf_tgt_tokenizer, embedding_size=64, hidden_size=64, layers=3):\n",
        "        \"\"\"\n",
        "        Initializer. Creates network modules and loss function.\n",
        "        Arguments:\n",
        "            hf_src_tokenizer: src field information\n",
        "            hf_tgt_tokenizer: tgt field information\n",
        "            embedding_size: word embedding size\n",
        "            hidden_size: hidden layer size of both encoder and decoder\n",
        "            layers: number of layers of both encoder and decoder\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "        self.hf_src_tokenizer = hf_src_tokenizer\n",
        "        self.hf_tgt_tokenizer = hf_tgt_tokenizer\n",
        "\n",
        "        # Keep the vocabulary sizes available\n",
        "        self.V_src = len(hf_src_tokenizer)\n",
        "        self.V_tgt = len(hf_tgt_tokenizer)\n",
        "\n",
        "        # Get special word ids or tokens\n",
        "        self.padding_id_src = self.hf_src_tokenizer.pad_token_id\n",
        "        self.padding_id_tgt = self.hf_tgt_tokenizer.pad_token_id\n",
        "        self.bos_id = self.hf_tgt_tokenizer.bos_token_id\n",
        "        self.eos_id = self.hf_tgt_tokenizer.eos_token_id\n",
        "\n",
        "        # Keep hyper-parameters available\n",
        "        self.embedding_size = embedding_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.layers = layers\n",
        "\n",
        "        # Create essential modules\n",
        "        self.word_embeddings_src = nn.Embedding(self.V_src, embedding_size)\n",
        "        self.word_embeddings_tgt = nn.Embedding(self.V_tgt, embedding_size)\n",
        "\n",
        "        # RNN cells\n",
        "        self.encoder_rnn = nn.LSTM(\n",
        "            input_size=embedding_size,\n",
        "            hidden_size=hidden_size // 2,  # to match decoder hidden size\n",
        "            batch_first=True,\n",
        "            num_layers=layers,\n",
        "            bidirectional=True,            # bidirectional encoder\n",
        "        )\n",
        "        self.decoder_rnn = nn.LSTM(\n",
        "            input_size=embedding_size,\n",
        "            hidden_size=hidden_size,\n",
        "            batch_first=True,\n",
        "            num_layers=layers,\n",
        "            bidirectional=False,  # unidirectional decoder\n",
        "        )\n",
        "\n",
        "        # Final projection layer\n",
        "        self.hidden2output = nn.Linear(hidden_size, self.V_tgt)\n",
        "\n",
        "        # Create loss function\n",
        "        self.loss_function = nn.CrossEntropyLoss(\n",
        "            reduction=\"sum\", ignore_index=self.padding_id_tgt\n",
        "        )\n",
        "\n",
        "    def forward_encoder(self, src, src_lengths):\n",
        "        \"\"\"\n",
        "        Encodes source words `src`.\n",
        "        Arguments:\n",
        "            src: src batch of size (batch_size, max_src_len)\n",
        "            src_lengths: src lengths of size (batch_size)\n",
        "        Returns:\n",
        "            a tuple (h, c) where h/c is of size (layers, bsz, hidden_size)\n",
        "        \"\"\"\n",
        "        # TODO - implement this function\n",
        "        # Optional: use `pack` to deal with paddings (https://discuss.pytorch.org/t/simple-working-example-how-to-use-packing-for-variable-length-sequence-inputs-for-rnn/2120)\n",
        "        # Note that the batch size is the first dimension, and the sequences are not sorted.\n",
        "        src_embeddings = self.word_embeddings_src(src)  # (batch_size, max_src_len, embedding_size)\n",
        "        packed_src = pack(src_embeddings, src_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        _, (h, c) = self.encoder_rnn(packed_src)\n",
        "\n",
        "        h = torch.cat((h[0:h.size(0):2], h[1:h.size(0):2]), dim=-1)  # (layers, batch_size, hidden_size)\n",
        "        c = torch.cat((c[0:c.size(0):2], c[1:c.size(0):2]), dim=-1)  # (layers, batch_size, hidden_size)\n",
        "        return (h, c)\n",
        "\n",
        "    def forward_decoder(self, encoder_final_state, tgt_in):\n",
        "        \"\"\"\n",
        "        Decodes based on encoder final state and ground truth target words.\n",
        "        Arguments:\n",
        "            encoder_final_state: a tuple (h, c) where h/c is of size\n",
        "                                 (bsz, layers, hidden_size)\n",
        "            tgt_in: a tensor of size (tgt_len, bsz)\n",
        "        Returns:\n",
        "            Logits of size (tgt_len, bsz, V_tgt) (before the softmax operation)\n",
        "        \"\"\"\n",
        "        # Get embeddings for target words\n",
        "        tgt_embeddings = self.word_embeddings_tgt(tgt_in)\n",
        "\n",
        "        # Pass through decoder RNN\n",
        "        decoder_outs, _ = self.decoder_rnn(tgt_embeddings, encoder_final_state)\n",
        "\n",
        "        # Project decoder outputs to logits using hidden2output layer\n",
        "        logits = self.hidden2output(decoder_outs)  # Use decoder_outs instead of h\n",
        "\n",
        "        return logits  # Return the logits\n",
        "\n",
        "        ...\n",
        "\n",
        "    def forward(self, src, src_lengths, tgt_in):\n",
        "        \"\"\"\n",
        "        Performs forward computation, returns logits.\n",
        "        Arguments:\n",
        "            src: src batch of size (batch_size, max_src_len)\n",
        "            src_lengths: src lengths of size (batch_size)\n",
        "            tgt_in:  a tensor of size (batch_size, tgt_len)\n",
        "        \"\"\"\n",
        "        # Forward encoder\n",
        "        encoder_final_state = self.forward_encoder(src, src_lengths)  # tuple (h, c)\n",
        "        # Forward decoder\n",
        "        logits = self.forward_decoder(encoder_final_state, tgt_in)    # bsz, tgt_len, V_tgt\n",
        "        return logits\n",
        "\n",
        "    def forward_decoder_incrementally(self, decoder_state, tgt_in_token):\n",
        "        \"\"\"\n",
        "        Forward the decoder at `decoder_state` for a single step with token `tgt_in_token`.\n",
        "        This function will only be used in the beam search section.\n",
        "        Arguments:\n",
        "            decoder_state: a tuple (h, c) where h/c is of size (layers, 1, hidden_size)\n",
        "            tgt_in_token: a tensor of size (1), a single token\n",
        "        Returns:\n",
        "            `logits`: Log probabilities for `tgt_in_token` of size (V_tgt)\n",
        "            `decoder_state`: updated decoder state, ready for next incremental update\n",
        "        \"\"\"\n",
        "        bsz = decoder_state[0].size(1)\n",
        "        assert bsz == 1, \"forward_decoder_incrementally only supports batch size 1!\"\n",
        "        # Compute word embeddings\n",
        "        tgt_embeddings = self.word_embeddings_tgt(\n",
        "            tgt_in_token.view(1, 1)\n",
        "        )  # bsz, tgt_len, hidden\n",
        "        # Forward decoder RNN and return all hidden states\n",
        "        decoder_outs, decoder_state = self.decoder_rnn(tgt_embeddings, decoder_state)\n",
        "        # Project to get logits\n",
        "        logits = self.hidden2output(decoder_outs)  # bsz, tgt_len, V_tgt\n",
        "        # Get log probabilities\n",
        "        logits = torch.log_softmax(logits, -1)\n",
        "        return logits.view(-1), decoder_state\n",
        "\n",
        "    def evaluate_ppl(self, iterator):\n",
        "        \"\"\"Returns the model's perplexity on a given dataset `iterator`.\"\"\"\n",
        "        # Switch to eval mode\n",
        "        self.eval()\n",
        "        total_loss = 0\n",
        "        total_words = 0\n",
        "        for batch in iterator:\n",
        "            # Input and target\n",
        "            src = batch['src_ids']              # bsz, max_src_len\n",
        "            src_lengths = batch['src_lengths']  # bsz\n",
        "            tgt_in = batch['tgt_ids'][:, :-1].contiguous()  # remove <eos> for decoder input (y_0=<bos>, y_1, y_2)\n",
        "            tgt_out = batch['tgt_ids'][:, 1:].contiguous()  # remove <bos> as decoder output        (y_1, y_2, y_3=<eos>)\n",
        "            # Forward to get logits\n",
        "            logits = self.forward(src, src_lengths, tgt_in) # bsz, tgt_len, V_tgt\n",
        "            # Compute cross entropy loss\n",
        "            loss = self.loss_function(logits.view(-1, self.V_tgt), tgt_out.view(-1))\n",
        "            total_loss += loss.item()\n",
        "            total_words += tgt_out.ne(self.padding_id_tgt).float().sum().item()\n",
        "        return math.exp(total_loss / total_words)\n",
        "\n",
        "    def train_all(self, train_iter, val_iter, epochs=10, learning_rate=0.001):\n",
        "        \"\"\"Train the model.\"\"\"\n",
        "        # Switch the module to training mode\n",
        "        self.train()\n",
        "        # Use Adam to optimize the parameters\n",
        "        optim = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
        "        best_validation_ppl = float(\"inf\")\n",
        "        best_model = None\n",
        "        # Run the optimization for multiple epochs\n",
        "        for epoch in range(epochs):\n",
        "            total_words = 0\n",
        "            total_loss = 0.0\n",
        "            for batch in tqdm(train_iter):\n",
        "                # Zero the parameter gradients\n",
        "                self.zero_grad()\n",
        "                # Input and target\n",
        "                tgt = batch['tgt_ids']               # bsz, max_tgt_len\n",
        "                src = batch['src_ids']               # bsz, max_src_len\n",
        "                src_lengths = batch['src_lengths']   # bsz\n",
        "                tgt_in = tgt[:, :-1]                 # Remove <eos> for decoder input (y_0=<bos>, y_1, y_2)\n",
        "                tgt_out = tgt[:, 1:]                 # Remove <bos> as decoder output (y_1, y_2, y_3=<eos>)\n",
        "                batch_size = src.size(0)\n",
        "                # Run forward pass and compute loss along the way.\n",
        "                logits = self.forward(src, src_lengths, tgt_in) # bsz, tgt_len, V_tgt\n",
        "                loss = self.loss_function(logits.reshape(-1, self.V_tgt), tgt_out.reshape(-1))\n",
        "                # Training stats\n",
        "                num_tgt_words = tgt_out.ne(self.padding_id_tgt).float().sum().item()\n",
        "                total_words += num_tgt_words\n",
        "                total_loss += loss.item()\n",
        "                # Perform backpropagation\n",
        "                loss.div(batch_size).backward()\n",
        "                optim.step()\n",
        "\n",
        "            # Evaluate and track improvements on the validation dataset\n",
        "            validation_ppl = self.evaluate_ppl(val_iter)\n",
        "            self.train()\n",
        "            if validation_ppl < best_validation_ppl:\n",
        "                best_validation_ppl = validation_ppl\n",
        "                self.best_model = copy.deepcopy(self.state_dict())\n",
        "            epoch_loss = total_loss / total_words\n",
        "            print(f\"Epoch: {epoch} Training Perplexity: {math.exp(epoch_loss):.4f} \"\n",
        "                  f\"Validation Perplexity: {validation_ppl:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "34556a5f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-02T21:12:27.421282Z",
          "iopub.status.busy": "2025-02-02T21:12:27.421057Z",
          "iopub.status.idle": "2025-02-02T21:14:52.735921Z",
          "shell.execute_reply": "2025-02-02T21:14:52.735031Z"
        },
        "id": "34556a5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53ed54f7-7175-48d4-db61-f77ff425509d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2032/2032 [00:37<00:00, 54.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Training Perplexity: 1.6708 Validation Perplexity: 1.0580\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2032/2032 [00:36<00:00, 55.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 Training Perplexity: 1.0382 Validation Perplexity: 1.0409\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "EPOCHS = 2  # epochs, we highly recommend starting with a smaller number like 1\n",
        "LEARNING_RATE = 2e-3  # learning rate\n",
        "\n",
        "# Instantiate and train classifier\n",
        "model = EncoderDecoder(\n",
        "    hf_src_tokenizer,\n",
        "    hf_tgt_tokenizer,\n",
        "    embedding_size=64,\n",
        "    hidden_size=64,\n",
        "    layers=3,\n",
        ").to(device)\n",
        "\n",
        "model.train_all(train_iter, val_iter, epochs=EPOCHS, learning_rate=LEARNING_RATE)\n",
        "model.load_state_dict(model.best_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e119d5f9",
      "metadata": {
        "id": "e119d5f9"
      },
      "source": [
        "Since the task we consider here is very simple, we should expect a perplexity very close to 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "69bde15f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-02T21:14:52.739140Z",
          "iopub.status.busy": "2025-02-02T21:14:52.738761Z",
          "iopub.status.idle": "2025-02-02T21:14:55.398056Z",
          "shell.execute_reply": "2025-02-02T21:14:55.397122Z"
        },
        "id": "69bde15f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adc03f4c-0822-42d4-cfd3-4b43a96fc2cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test perplexity: 1.044\n"
          ]
        }
      ],
      "source": [
        "# Evaluate model performance\n",
        "print (f'Test perplexity: {model.evaluate_ppl(test_iter):.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "572e029d",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "572e029d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "338b72c2-aebb-4f4c-96f2-1ac09ae707ae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "grader.check(\"encoder_decoder_ppl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85138f2c",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "85138f2c"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "# Beam search decoding\n",
        "\n",
        "Now that we have a well-trained model, we need to consider how to use it to do the actual conversion. At decoding time, given a source sequence $x_1, \\ldots, x_S$, we want to find the target sequence $y_1^*, \\ldots, y_T^*, y_{T+1}^*$ (recall that $y _{T+1} = \\langle\\text{eos}\\rangle$) such that the conditional likelihood is maximized:\n",
        "\n",
        "\\begin{align*}\n",
        "y_1^*, \\ldots, y_T^*, y_{T+1}^* &= \\argmax{y_1, \\ldots, y_T, y_{T+1}} {\\Pr}_\\theta(y_1, \\ldots, y_T \\mid x_1, \\ldots, x_S)\\\\\n",
        "&= \\argmax{y_1, \\ldots, y_T, y_{T+1}} \\prod_{t=1}^{T+1} {\\Pr}_\\theta(y_t \\mid y_{<t}, x_1, \\ldots, x_S)\\\\\n",
        "\\end{align*}\n",
        "\n",
        "In previous labs and project segments, we used _greedy decoding_, i.e., taking $\\hat{y}_1= \\argmax{y_1} {\\Pr}_\\theta(y_1 \\mid y_{0}, x_1, \\ldots, x_S)$, $\\hat{y}_2= \\argmax{y_2} {\\Pr}_\\theta(y_2 \\mid y_{0}, \\hat{y}_1, x_1, \\ldots, x_S)$, ..., $\\hat{y}_{T+1}= \\argmax{y_{T+1}} {\\Pr}_\\theta(y_{T+1} \\mid y_{0}, \\hat{y}_1, \\ldots, \\hat{y}_{T}, x_1, \\ldots, x_S)$, until $\\hat{y}_{T+1}=\\langle\\text{eos}\\rangle$.\n",
        "\n",
        "**Question:**  Does greedy decoding guarantee finding the optimal sequence (the sequence with the highest conditional likelihood)? Why or why not?\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: open_response_decoding\n",
        "manual: true\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4075773",
      "metadata": {
        "id": "d4075773"
      },
      "source": [
        "Greedy decoding does not guarantee finding the best sequence because it focuses only on local probabilities, selecting the most likely word at each step without considering the global context of the sequence. This can lead to suboptimal results, as it may miss better sequences that require temporarily choosing less likely words to achieve a higher overall probability. Since it does not explore beyond the immediate choices, greedy decoding can get stuck in local optima. Methods like beam search address this limitation by exploring multiple paths and balancing local and global probabilities, increasing the chances of finding the optimal sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "640977c7",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "640977c7"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "## Beam search decoding\n",
        "\n",
        "Beam search decoding is the most commonly used decoding method in sequence-to-sequence approaches. Like greedy decoding, it uses a left-to-right search process. But instead of only keeping the single argmax at each position, beam search maintains the $K$ best partial hypotheses $H_t = \\{(y_1^{(k)}, \\ldots, y_t^{(k)}): k\\in\\{1, \\ldots, K\\}\\}$ at every step $t$. To proceed to $t+1$, we compute the scores of sequences  $y_1^{(k)}, \\ldots, y_t^{(k)}, y_{t+1}$ for every possible extension $y_{t+1}\\in \\mathcal{V}$ and every possible prefix $(y_1^{(k)}, \\ldots, y_t^{(k)}) \\in H_t$, where $\\mathcal{V}$ is the vocabulary. Among these $K\\times |\\mathcal{V}|$ sequences, we only keep the top $K$ sequences with the best partial scores, and that becomes $H_{t+1} = \\{(y_1^{(k)}, \\ldots, y_{t+1}^{(k)}): k\\in\\{1, \\ldots, K\\}\\}$. To start at $t=1$, $H_1 = \\{(y): y \\in \\text{K-argmax}_{y_1\\in\\mathcal{V}} \\log P(y_1|y_0=bos) \\}$. Here $K$ is called the beam size.\n",
        "\n",
        "To summarize,\n",
        "\n",
        "\\begin{align*}\n",
        "H_1 &= \\{(y): y \\in \\operatorname*{K-argmax}_{y_1\\in\\mathcal{V}} \\log P(y_1|y_0=bos) \\}\\\\\n",
        "H_{t+1} &= \\operatorname*{K-argmax}\\limits_{\\{(y_1, y_2, \\ldots, y_{t+1})\\in \\mathcal{V}^{t+1}: (y_1, \\ldots, y_t) \\in H_t \\}} \\log P(y_1, \\ldots, y_{t+1} | x)\n",
        "\\end{align*}\n",
        "\n",
        "until we reach a pre-specified maximum search length, and we collect the completed hypotheses along the way. (By completed we mean ending with `<eos>`.) The finished hypothesis with the best score will then be returned.\n",
        "\n",
        "**Question:** Is beam search better than greedy search when $K=1$? Is it better when $K>1$? Why? How big a $K$ value do we need to get a guarantee that we can find the globally best sequence (assuming a maximum sequence length $T$ and vocabulary size $|\\mathcal{V}|$).\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: open_response_beam_search_greedy\n",
        "manual: true\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ae0a8d9",
      "metadata": {
        "id": "6ae0a8d9"
      },
      "source": [
        "When K=1, beam search is the same as greedy search because it only keeps one hypothesis, making the same local decisions at each step. It does not improve over greedy search.  \n",
        "\n",
        "When K>1, beam search is better because it explores multiple possible sequences, reducing the risk of getting stuck in a local maximum. It increases the chance of finding a higher-probability sequence by considering more options at each step.  \n",
        "\n",
        "To guarantee finding the globally best sequence, we need **K = |V|^T**, meaning we must explore all possible sequences of length **T** in the vocabulary **V**. However, this is computationally impossible for large vocabularies, so in practice, we use a reasonable **K**  to balance quality and efficiency."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95c1b35d",
      "metadata": {
        "id": "95c1b35d"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "\n",
        "\n",
        "Under the probabilistic formulation of sequence-to-sequence models, the partial scores are decomposable over time steps: $\\log {\\Pr}_\\theta(y_1, \\ldots, y_T \\mid x) = \\sum_{t=1}^T \\log {\\Pr}_\\theta(y_t \\mid y_{<t}, x)$. Therefore, we can save computation in the above process by maintaining the partial sums $\\sum_{t'=1}^t \\log {\\Pr}_\\theta(y_{t'}^{(k)} \\mid y_{<t'}^{(k)}, x)$, such that we only need to compute $\\log {\\Pr}_\\theta(y_{t+1} \\mid y_{<t+1}^{(k)})$ when we want to go from $t$ to $t+1$.\n",
        "\n",
        "Here is pseudo-code for the beam search algorithm to decode a single example `x` of maximum length `max_T` using a beam size of `K`.\n",
        "\n",
        "```\n",
        " 1.  def beam_search(x, K, max_T):\n",
        " 2.      finished = []        # for storing completed hypotheses\n",
        "         # Initialize the beam\n",
        " 3.      beams = [Beam(hyp=(bos), score=0)] # initial hypothesis: bos, initial score: 0\n",
        "\n",
        " 4.      for t in [1..max_T]  # main body of search over time steps           \n",
        " 5.          hypotheses = []\n",
        "\n",
        "             # Expand each beam by all possible tokens y_{t+1}\n",
        " 6.          for beam in beams:\n",
        " 7.              y_{1:t}, score = beam.hyp, beam.score\n",
        " 8.              for y_{t+1} in V:\n",
        " 9.                  y_{1:t+1} = y_{1:t} + [y_{t+1}]\n",
        " 10.                 new_score = score + log P(y_{t+1} | y_{1:t}, x)\n",
        " 11.                 hypotheses.append(Beam(hyp=y_{1:t+1}, score=new_score))\n",
        "\n",
        "             # Find K best next beams\n",
        " 12.         beams = sorted(hypotheses, key=lambda beam: -beam.score)[:K]\n",
        "\n",
        "             # Set aside finished beams (those that end in <eos>)\n",
        " 13.         for beam in beams:\n",
        " 14.             y_{t+1} = beam.hyp[-1]\n",
        " 15.             if y_{t+1} == eos:\n",
        " 16.                 finished.append(beam)\n",
        " 17.                 beams.remove(beam)\n",
        "\n",
        "             # Break the loop if everything is finished\n",
        " 18.         if len(beams) == 0:\n",
        " 19.             break              \n",
        " 20.     return sorted(finished, key=lambda beam: -beam.score)[0] # return the best finished hypothesis\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7603cb01",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "7603cb01"
      },
      "source": [
        "Implement function `beam_search` in the below code. Note that there are some differences from the pseudo-code: first, we maintained a `decoder_state` in addition to $y_{1:t}$ and score such that we can compute $P(y_{t+1} \\mid y_{<t+1},x)$ efficiently; second, instead of creating a list of actual hypotheses as in lines 8-11, we use tensors to get pointers to the beam id and $y_{t+1}$ that are among the best $K$ next beams.\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: beam_search\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "ce5d7aeb",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-02T21:15:00.709300Z",
          "iopub.status.busy": "2025-02-02T21:15:00.709058Z",
          "iopub.status.idle": "2025-02-02T21:15:00.720990Z",
          "shell.execute_reply": "2025-02-02T21:15:00.720141Z"
        },
        "id": "ce5d7aeb"
      },
      "outputs": [],
      "source": [
        "MAX_T = 15     # max target length\n",
        "\n",
        "class Beam():\n",
        "  \"\"\"Helper class for storing a hypothesis, its score and its decoder hidden state.\"\"\"\n",
        "  def __init__(self, decoder_state, tokens, score):\n",
        "    self.decoder_state = decoder_state\n",
        "    self.tokens = tokens\n",
        "    self.score = score\n",
        "\n",
        "class BeamSearcher():\n",
        "  \"\"\"Main class for beam search.\"\"\"\n",
        "  def __init__(self, model):\n",
        "    self.model = model\n",
        "    self.bos_id = model.bos_id\n",
        "    self.eos_id = model.eos_id\n",
        "    self.V = model.V_tgt\n",
        "\n",
        "\n",
        "  def beam_search(self, src, src_lengths, K, max_T=MAX_T):\n",
        "    \"\"\"Performs beam search decoding.\n",
        "\n",
        "    Arguments:\n",
        "        src: src batch of size (1, max_src_len)\n",
        "        src_lengths: src lengths of size (1)\n",
        "        K: beam size\n",
        "        max_T: max possible target length considered\n",
        "    Returns:\n",
        "        a list of token ids\n",
        "    \"\"\"\n",
        "    finished = []\n",
        "    # Initialize the beam\n",
        "    self.model.eval()\n",
        "    #TODO - fill in encoder_final_state and init_beam below\n",
        "    encoder_final_state = self.model.forward_encoder(src, src_lengths)\n",
        "    init_beam = Beam(decoder_state=encoder_final_state, tokens=[self.bos_id], score=0)\n",
        "    beams = [init_beam]\n",
        "\n",
        "    for t in range(max_T): # main body of search over time steps\n",
        "\n",
        "        # Expand each beam by all possible tokens y_{t+1}\n",
        "        all_total_scores = []\n",
        "        for beam in beams:\n",
        "            y_1_to_t, score, decoder_state = beam.tokens, beam.score, beam.decoder_state\n",
        "            y_t = y_1_to_t[-1]\n",
        "            #TODO - finish the code below\n",
        "            # Hint: you might want to use `model.forward_decoder_incrementally`\n",
        "            logits, decoder_state = self.model.forward_decoder_incrementally(decoder_state, torch.tensor([y_t], device=src.device))\n",
        "            total_scores = logits + score\n",
        "            all_total_scores.append(total_scores)\n",
        "            beam.decoder_state = decoder_state # update decoder state in the beam\n",
        "        all_total_scores = torch.stack(all_total_scores) # (K, V) when t>0, (1, V) when t=0\n",
        "\n",
        "        # Find K best next beams\n",
        "        # The below code has the same functionality as line 6-12, but is more efficient\n",
        "        all_scores_flattened = all_total_scores.view(-1) # K*V when t>0, 1*V when t=0\n",
        "        topk_scores, topk_ids = all_scores_flattened.topk(K, 0)\n",
        "        beam_ids = topk_ids.div(self.V, rounding_mode='floor')\n",
        "        next_tokens = topk_ids - beam_ids * self.V\n",
        "        new_beams = []\n",
        "        for k in range(K):\n",
        "            beam_id = beam_ids[k]       # which beam it comes from\n",
        "            y_t_plus_1 = next_tokens[k] # which y_{t+1}\n",
        "            score = topk_scores[k]\n",
        "            beam = beams[beam_id]\n",
        "            decoder_state = beam.decoder_state\n",
        "            y_1_to_t = beam.tokens\n",
        "            #TODO\n",
        "            new_beam = []\n",
        "        for k in range(K):\n",
        "            beam_id = beam_ids[k]\n",
        "            y_t_plus_1 = next_tokens[k]\n",
        "            score = topk_scores[k]\n",
        "\n",
        "            beam = beams[beam_id]\n",
        "            decoder_state = beam.decoder_state\n",
        "            y_1_to_t = beam.tokens + [y_t_plus_1.item()]\n",
        "\n",
        "            new_beams.append(Beam(decoder_state, y_1_to_t, score))\n",
        "\n",
        "        # Set aside completed beams\n",
        "        # TODO - move completed beams to `finished` (and remove them from `beams`)\n",
        "        for beam in new_beams:\n",
        "            if beam.tokens[-1] == self.eos_id:\n",
        "                finished.append(beam)\n",
        "                new_beams.remove(beam)\n",
        "        beams = new_beams\n",
        "\n",
        "\n",
        "        # Break the loop if everything is completed\n",
        "        if len(beams) == 0:\n",
        "            break\n",
        "\n",
        "    # Return the best hypothesis\n",
        "    if len(finished) > 0:\n",
        "        finished = sorted(finished, key=lambda beam: -beam.score)\n",
        "        return [token for token in finished[0].tokens]\n",
        "    else: # when nothing is finished, return an unfinished hypothesis\n",
        "        return [token for token in beams[0].tokens]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "f330c0d2",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "f330c0d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "bd3c4ad9-6a5a-43a9-fde5-0f01b3fed718"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "grader.check(\"beam_search\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b2bf4e0",
      "metadata": {
        "id": "6b2bf4e0"
      },
      "source": [
        "Now we can use beam search decoding to predict the outputs for the test set inputs using the trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "b221d614",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-02T21:15:32.228526Z",
          "iopub.status.busy": "2025-02-02T21:15:32.228282Z",
          "iopub.status.idle": "2025-02-02T21:15:47.776332Z",
          "shell.execute_reply": "2025-02-02T21:15:47.775386Z"
        },
        "id": "b221d614",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b753071-a780-45b3-943f-68e92d538ec3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source:       1. sixteen thousand eight hundred and thirty two\n",
            "Prediction:   1 6 8 3 2\n",
            "Ground truth: 1 6 8 3 2\n",
            "\n",
            "Source:       2. sixty seven million six hundred and eighty five thousand two hundred and thirty\n",
            "Prediction:   6 7 6 8 5 2 3 0\n",
            "Ground truth: 6 7 6 8 5 2 3 0\n",
            "\n",
            "Source:       3. six thousand two hundred and twelve\n",
            "Prediction:   6 2 1 2\n",
            "Ground truth: 6 2 1 2\n",
            "\n",
            "Source:       4. seven hundred and ninety eight million three hundred and thirty one thousand eight hundred and eighteen\n",
            "Prediction:   7 9 8 3 3 1 8 1 8\n",
            "Ground truth: 7 9 8 3 3 1 8 1 8\n",
            "\n",
            "Source:       5. eighty eight million four hundred and thirteen thousand nine hundred and eighteen\n",
            "Prediction:   8 8 4 1 3 9 1 8\n",
            "Ground truth: 8 8 4 1 3 9 1 8\n",
            "\n",
            "Source:       6. three hundred and seventy four thousand two hundred and seventy\n",
            "Prediction:   3 7 4 2 7 0\n",
            "Ground truth: 3 7 4 2 7 0\n",
            "\n",
            "Source:       7. ninety eight million three hundred and seventy thousand five hundred and forty five\n",
            "Prediction:   9 8 3 7 0 5 4 5\n",
            "Ground truth: 9 8 3 7 0 5 4 5\n",
            "\n",
            "Source:       8. ninety seven thousand seven hundred and sixty two\n",
            "Prediction:   9 7 7 6 2\n",
            "Ground truth: 9 7 7 6 2\n",
            "\n",
            "Source:       9. four hundred and ten thousand two hundred and three\n",
            "Prediction:   4 1 0 2 0 3\n",
            "Ground truth: 4 1 0 2 0 3\n",
            "\n",
            "Source:       10. five hundred and ninety eight thousand three hundred and ninety seven\n",
            "Prediction:   5 9 8 3 9 7\n",
            "Ground truth: 5 9 8 3 9 7\n",
            "\n",
            "Accuracy: 0.92\n"
          ]
        }
      ],
      "source": [
        "DEBUG_FIRST = 10  # set to 0 to disable printing predictions\n",
        "K = 5             # beam size 5\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# create beam searcher\n",
        "beam_searcher = BeamSearcher(model)\n",
        "\n",
        "for index, batch in enumerate(test_iter, start=1):\n",
        "  # Input and output\n",
        "  src = batch['src_ids']\n",
        "  src_lengths = batch['src_lengths']\n",
        "  # Predict\n",
        "  prediction = beam_searcher.beam_search(src, src_lengths, K)\n",
        "  # Convert to string\n",
        "  prediction = hf_tgt_tokenizer.decode(prediction,\n",
        "                                       skip_special_tokens=True)\n",
        "  ground_truth = hf_tgt_tokenizer.decode(batch['tgt_ids'][0],\n",
        "                                         skip_special_tokens=True)\n",
        "  # Print out the first few examples\n",
        "  if DEBUG_FIRST >= index :\n",
        "    src = hf_src_tokenizer.decode(src[0], skip_special_tokens=True)\n",
        "    print (f'Source:       {index}. {src}\\n'\n",
        "           f'Prediction:   {prediction}\\n'\n",
        "           f'Ground truth: {ground_truth}\\n')\n",
        "  if ground_truth == prediction:\n",
        "    correct += 1\n",
        "  total += 1\n",
        "\n",
        "print (f'Accuracy: {correct/total:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "293f8141",
      "metadata": {
        "id": "293f8141"
      },
      "source": [
        "You might have noticed that using a larger $K$ might lead to very similar performance as using $K=1$ (greedy decoding). This is largely due to the fact that there are no dependencies among target tokens in our dataset (e.g., knowing that $y_1$ is 1 does not affect our prediction on $y_2$ conditioned on the source). In real world applications, people usually find using a fixed value of $K>1$ (such as $K=5$) performs better than greedy decoding."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf9c0d1b",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "bf9c0d1b"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "# Lab debrief\n",
        "\n",
        "**Question:** We're interested in any thoughts you have about this lab so that we can improve this lab for later years, and to inform later labs for this year. Please list any issues that arose or comments you have to improve the lab. Useful things to comment on might include the following, but you're not restricted to these:\n",
        "\n",
        "* What *specific single* change to the lab would have made your learning more efficient? This might be an addition of a concept that was not explained, or an example that would clarify a concept, or a problem that would have captured a concept in a better way, or anything else you can think of that would have made this a better lab.\n",
        "* Was the lab too long or too short?\n",
        "* Were the readings appropriate for the lab?\n",
        "* Was it clear (at least after you completed the lab) what the points of the exercises were?\n",
        "* Are there additions or changes you think would make the lab better?\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: open_response_debrief\n",
        "manual: true\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e51d3511",
      "metadata": {
        "id": "e51d3511"
      },
      "source": [
        "The lab was well-structured and provided a solid understanding of sequence-to-sequence models and beam search decoding. One improvement could be adding a step-by-step walkthrough of beam search with a small example, as implementing it required careful attention to tensor operations. The length felt appropriate, balancing theoretical understanding and practical implementation. The readings were relevant and helped reinforce the concepts needed for the exercises. After completing the lab, the objectives became clear, particularly how beam search improves over greedy decoding. Overall, this was a valuable learning experience, and a visual explanation of the beam search process could further enhance clarity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fb96e0c",
      "metadata": {
        "id": "5fb96e0c"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "\n",
        "\n",
        "# End of Lab 4-3 {-}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b981793",
      "metadata": {
        "id": "7b981793"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "8e015fd3",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "8e015fd3"
      },
      "source": [
        "---\n",
        "\n",
        "To double-check your work, the cell below will rerun all of the autograder tests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "a97372ed",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "a97372ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "outputId": "045ff0c1-fdd7-4c95-fc00-378cee8f4d5f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "beam_search:\n",
              "\n",
              "    All tests passed!\n",
              "    \n",
              "\n",
              "encoder_decoder_ppl:\n",
              "\n",
              "    All tests passed!\n",
              "    \n"
            ],
            "text/html": [
              "<p><strong>beam_search:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n",
              "<p><strong>encoder_decoder_ppl:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "grader.check_all()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "236299",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "title": "CS187 Lab 4-4: Sequence-to-sequence models",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7858ea8e609a42a791d6c7fb2a2adc7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2735ef48034b4d6192b569166262e0d5",
              "IPY_MODEL_f5873677159b43c7883c3638c3f04ac4",
              "IPY_MODEL_061896529f7f4f30884a1911d96168f2"
            ],
            "layout": "IPY_MODEL_315c69bcdf4d4809b9b4413728c353f3"
          }
        },
        "2735ef48034b4d6192b569166262e0d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bcf156c0a54a45439996df7beadc7326",
            "placeholder": "​",
            "style": "IPY_MODEL_cb124c76508a402782d4f1f83f89fcae",
            "value": "Generating train split: "
          }
        },
        "f5873677159b43c7883c3638c3f04ac4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e50935035b64b84bc89185389b95480",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_67151d11c4ee42a78ff66d8d5ad25db2",
            "value": 1
          }
        },
        "061896529f7f4f30884a1911d96168f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb677e2847ca4d0a942209399cc11d23",
            "placeholder": "​",
            "style": "IPY_MODEL_f72fbc07ce5f4cc682697f5290e33116",
            "value": " 65022/0 [00:00&lt;00:00, 280414.60 examples/s]"
          }
        },
        "315c69bcdf4d4809b9b4413728c353f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcf156c0a54a45439996df7beadc7326": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb124c76508a402782d4f1f83f89fcae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e50935035b64b84bc89185389b95480": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "67151d11c4ee42a78ff66d8d5ad25db2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bb677e2847ca4d0a942209399cc11d23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f72fbc07ce5f4cc682697f5290e33116": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e82a07c4df045f1a74ba0364bbcc55a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_890e7a0fb956433296819dc669dd5bf4",
              "IPY_MODEL_2cb6f291b6a44ea4a951c14e7b67a8b9",
              "IPY_MODEL_5191adcff46147d5b84e7c5816ca795f"
            ],
            "layout": "IPY_MODEL_36094a8d542044b29a896f5425914b05"
          }
        },
        "890e7a0fb956433296819dc669dd5bf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0efdc19e8cdb446b81d4735fd15b0c95",
            "placeholder": "​",
            "style": "IPY_MODEL_3fa1973979df47ac95837964880ba9a3",
            "value": "Generating val split: "
          }
        },
        "2cb6f291b6a44ea4a951c14e7b67a8b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08fc086d3d974cdb8a7d73e1bbd6a97e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4d3f8b2729394f6fa6eb6ef08148c8a1",
            "value": 1
          }
        },
        "5191adcff46147d5b84e7c5816ca795f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f9f97955dc34ae1be0b0c4062887cf6",
            "placeholder": "​",
            "style": "IPY_MODEL_6c6f47c2312540d298e1908d0ae43909",
            "value": " 700/0 [00:00&lt;00:00, 20598.83 examples/s]"
          }
        },
        "36094a8d542044b29a896f5425914b05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0efdc19e8cdb446b81d4735fd15b0c95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3fa1973979df47ac95837964880ba9a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "08fc086d3d974cdb8a7d73e1bbd6a97e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "4d3f8b2729394f6fa6eb6ef08148c8a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8f9f97955dc34ae1be0b0c4062887cf6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c6f47c2312540d298e1908d0ae43909": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "24278aa3a6b94d7c90d173ade7c3d842": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_00d4c697177c48dab61d1858373468d5",
              "IPY_MODEL_56afd2d0c99048c6bf43589974f97d46",
              "IPY_MODEL_fbe1140ba1fe445fb51ce0bbe5df86b1"
            ],
            "layout": "IPY_MODEL_797af22d48984c81a538915e35d8f16c"
          }
        },
        "00d4c697177c48dab61d1858373468d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa80fef5905045e98afbc1990bbfdd5e",
            "placeholder": "​",
            "style": "IPY_MODEL_01f19c85f67440559a2943934e870550",
            "value": "Generating test split: "
          }
        },
        "56afd2d0c99048c6bf43589974f97d46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50120b33eb784f53b89576bb9d278e27",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4bc59238ca894686b254f396b1f72eac",
            "value": 1
          }
        },
        "fbe1140ba1fe445fb51ce0bbe5df86b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_163a913fadf648a18ab668c233d7e9a6",
            "placeholder": "​",
            "style": "IPY_MODEL_aed2de9c28914d8db2e7aa63a8195b51",
            "value": " 700/0 [00:00&lt;00:00, 17846.58 examples/s]"
          }
        },
        "797af22d48984c81a538915e35d8f16c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa80fef5905045e98afbc1990bbfdd5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01f19c85f67440559a2943934e870550": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "50120b33eb784f53b89576bb9d278e27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "4bc59238ca894686b254f396b1f72eac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "163a913fadf648a18ab668c233d7e9a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aed2de9c28914d8db2e7aa63a8195b51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "251a4f2e14c24ab99fd459a4ffff5e19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_29cd6b57cd6347f8b823d9ca90bf1619",
              "IPY_MODEL_4695b4224e1945bfb78ad264a0444183",
              "IPY_MODEL_3d89c688ab144b19818b3227bd7c188a"
            ],
            "layout": "IPY_MODEL_8b12b3726f414e52950e71b435aadee5"
          }
        },
        "29cd6b57cd6347f8b823d9ca90bf1619": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45a86a96421b4009b7345160a1d66220",
            "placeholder": "​",
            "style": "IPY_MODEL_9fae26f3cc1c4ec88e28ba784495ff0a",
            "value": "Map: 100%"
          }
        },
        "4695b4224e1945bfb78ad264a0444183": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9b5b7ae96e24ad8b1db14c86fa275ee",
            "max": 65022,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1bf20950292243918ec2085cc55da258",
            "value": 65022
          }
        },
        "3d89c688ab144b19818b3227bd7c188a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c757bb251654a1fbd7b1faf1682a05e",
            "placeholder": "​",
            "style": "IPY_MODEL_0e44090808244703b6973007da9694b5",
            "value": " 65022/65022 [00:15&lt;00:00, 4441.50 examples/s]"
          }
        },
        "8b12b3726f414e52950e71b435aadee5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45a86a96421b4009b7345160a1d66220": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fae26f3cc1c4ec88e28ba784495ff0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d9b5b7ae96e24ad8b1db14c86fa275ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1bf20950292243918ec2085cc55da258": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6c757bb251654a1fbd7b1faf1682a05e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e44090808244703b6973007da9694b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "61608a0b44bb42988672a561336ec95e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b7309c39b6dd466fa2a691391de3381b",
              "IPY_MODEL_f9d979f17dd54327913962fbb352b3c2",
              "IPY_MODEL_331be9c2186f4b1688c0d5aa6ccb3479"
            ],
            "layout": "IPY_MODEL_3e272d7774ee414aa90bcb2aca937b9b"
          }
        },
        "b7309c39b6dd466fa2a691391de3381b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cbcf94f8a9764b42ac87182eda58444a",
            "placeholder": "​",
            "style": "IPY_MODEL_2c3ba1a9d7b2404b9fb3e27388d13398",
            "value": "Map: 100%"
          }
        },
        "f9d979f17dd54327913962fbb352b3c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91ed3319dae34d39b39401fc6f172e87",
            "max": 700,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ac98b7f9e20541c2835490db9caac781",
            "value": 700
          }
        },
        "331be9c2186f4b1688c0d5aa6ccb3479": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fff228b004094ec7bc0d489f129842f1",
            "placeholder": "​",
            "style": "IPY_MODEL_e8b434242ca4455c9464036bd58e496e",
            "value": " 700/700 [00:00&lt;00:00, 4058.16 examples/s]"
          }
        },
        "3e272d7774ee414aa90bcb2aca937b9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbcf94f8a9764b42ac87182eda58444a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c3ba1a9d7b2404b9fb3e27388d13398": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91ed3319dae34d39b39401fc6f172e87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac98b7f9e20541c2835490db9caac781": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fff228b004094ec7bc0d489f129842f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8b434242ca4455c9464036bd58e496e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d2d055657e3c440ca238e4d9893feb03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_edfc2c34747f4f54b12534f00c417ef8",
              "IPY_MODEL_d8ab479345054862991aecfc197d9d90",
              "IPY_MODEL_8922d01d910c4ba48072958c66cf28e6"
            ],
            "layout": "IPY_MODEL_b2ed7fc95cac4752a4bd0f67ace5546b"
          }
        },
        "edfc2c34747f4f54b12534f00c417ef8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf11fd54a020424c9b1e83e41b8ac88d",
            "placeholder": "​",
            "style": "IPY_MODEL_708bf08bacce485a81d5e2ed374329a4",
            "value": "Map: 100%"
          }
        },
        "d8ab479345054862991aecfc197d9d90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dae9c6967488407d88d95dd0ea54c6f1",
            "max": 700,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_494bf533344c49e68a2c58373f7f25fe",
            "value": 700
          }
        },
        "8922d01d910c4ba48072958c66cf28e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cafb7422aaa94edfb05b9e89a25b37b4",
            "placeholder": "​",
            "style": "IPY_MODEL_5350afb6e2d040ebb2a0381c6ae66183",
            "value": " 700/700 [00:00&lt;00:00, 3871.36 examples/s]"
          }
        },
        "b2ed7fc95cac4752a4bd0f67ace5546b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf11fd54a020424c9b1e83e41b8ac88d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "708bf08bacce485a81d5e2ed374329a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dae9c6967488407d88d95dd0ea54c6f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "494bf533344c49e68a2c58373f7f25fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cafb7422aaa94edfb05b9e89a25b37b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5350afb6e2d040ebb2a0381c6ae66183": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}