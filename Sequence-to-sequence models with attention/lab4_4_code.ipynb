{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 186,
      "id": "f7befbee",
      "metadata": {
        "deletable": false,
        "editable": false,
        "execution": {
          "iopub.execute_input": "2025-02-03T08:14:01.601767Z",
          "iopub.status.busy": "2025-02-03T08:14:01.601444Z",
          "iopub.status.idle": "2025-02-03T08:14:02.999328Z",
          "shell.execute_reply": "2025-02-03T08:14:02.998396Z"
        },
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": true
        },
        "id": "f7befbee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47a1e44f-c862-4a54-abc4-89a4c6ef9dbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Completed with errors. Exit status: 256\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "256"
            ]
          },
          "metadata": {},
          "execution_count": 186
        }
      ],
      "source": [
        "# Please do not change this cell because some hidden tests might depend on it.\n",
        "import os\n",
        "\n",
        "# Otter grader does not handle ! commands well, so we define and use our\n",
        "# own function to execute shell commands.\n",
        "def shell(commands, warn=True):\n",
        "    \"\"\"Executes the string `commands` as a sequence of shell commands.\n",
        "\n",
        "       Prints the result to stdout and returns the exit status.\n",
        "       Provides a printed warning on non-zero exit status unless `warn`\n",
        "       flag is unset.\n",
        "    \"\"\"\n",
        "    file = os.popen(commands)\n",
        "    print (file.read().rstrip('\\n'))\n",
        "    exit_status = file.close()\n",
        "    if warn and exit_status != None:\n",
        "        print(f\"Completed with errors. Exit status: {exit_status}\\n\")\n",
        "    return exit_status\n",
        "\n",
        "shell(\"\"\"\n",
        "ls requirements.txt >/dev/null 2>&1\n",
        "if [ ! $? = 0 ]; then\n",
        " rm -rf .tmp\n",
        " git clone https://github.com/cs236299-2024-winter/lab4-4.git .tmp\n",
        " mv .tmp/tests ./\n",
        " mv .tmp/requirements.txt ./\n",
        " rm -rf .tmp\n",
        "fi\n",
        "pip install -q -r requirements.txt\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Monter Google Drive pour accéder à vos fichiers\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Copier les fichiers depuis Google Drive vers l'environnement Colab\n",
        "!cp -r /content/drive/MyDrive/nlp_lab/lab4-4* .\n",
        "\n",
        "# Installer les dépendances depuis le fichier requirements.txt\n",
        "!pip install -r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt\n",
        "\n",
        "# Installer Otter Grader (si nécessaire)\n",
        "!pip install otter-grader\n",
        "\n",
        "# Fonction pour exécuter les commandes shell\n",
        "import os\n",
        "\n",
        "def shell(commands, warn=True):\n",
        "    \"\"\"Exécute des commandes shell et affiche les résultats.\"\"\"\n",
        "    file = os.popen(commands)\n",
        "    print(file.read().rstrip('\\n'))\n",
        "    exit_status = file.close()\n",
        "    if warn and exit_status is not None:\n",
        "        print(f\"Command failed with exit code {exit_status}\")\n",
        "    return exit_status\n",
        "\n",
        "# Vérifier si requirements.txt existe et télécharger le dépôt si nécessaire\n",
        "shell(\"\"\"\n",
        "ls requirements.txt >/dev/null 2>&1\n",
        "if [ ! $? = 0 ]; then\n",
        "    rm -rf .tmp\n",
        "    git clone https://github.com/cs236299-2024-winter/lab1-4.git .tmp\n",
        "    mv .tmp/tests ./tests\n",
        "    mv .tmp/requirements.txt ./requirements.txt\n",
        "    rm -rf .tmp\n",
        "fi\n",
        "pip install -q -r requirements.txt\n",
        "\"\"\")\n",
        "\n",
        "# Copier les fichiers tests depuis Google Drive si le dossier 'tests' est introuvable\n",
        "if not os.path.exists('tests/token_count.py'):\n",
        "    print(\"Téléchargement des fichiers nécessaires depuis Google Drive...\")\n",
        "    !mkdir -p tests\n",
        "    !cp -r /content/drive/MyDrive/nlp_lab/lab4-4/tests/* ./tests/\n",
        "else:\n",
        "    print(\"Les fichiers de tests sont déjà présents.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_WYn3ynDne3",
        "outputId": "5b03d4ff-f150-4ad6-9c78-6278047facf0"
      },
      "id": "H_WYn3ynDne3",
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: otter-grader==1.0.0 in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 1)) (1.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 2)) (2.5.1+cu124)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 3)) (3.10.0)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 4)) (3.2)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 5)) (0.21.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 6)) (4.47.1)\n",
            "Requirement already satisfied: datasets>=2.14.6 in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 7)) (3.2.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 1)) (6.0.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 1)) (5.10.4)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (from otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 1)) (7.34.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.11/dist-packages (from otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 1)) (7.16.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 1)) (4.67.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 1)) (75.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 1)) (2.2.2)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.11/dist-packages (from otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 1)) (6.4.2)\n",
            "Requirement already satisfied: docker in /usr/local/lib/python3.11/dist-packages (from otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 1)) (7.1.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 1)) (3.1.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 1)) (0.3.8)\n",
            "Requirement already satisfied: pdfkit in /usr/local/lib/python3.11/dist-packages (from otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 1)) (1.0.0)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (from otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 1)) (3.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 2)) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 2)) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 2)) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 2)) (2024.9.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 2)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 2)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 2)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 2)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 2)) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 2)) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 2)) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 2)) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 2)) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 2)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 2)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 2)) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 2)) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 2)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 3)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 3)) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 3)) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 3)) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 3)) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 3)) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 3)) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 5)) (0.27.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 6)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 6)) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 6)) (0.5.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.14.6->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 7)) (17.0.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.14.6->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 7)) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.14.6->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 7)) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.14.6->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 7)) (3.11.11)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.14.6->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 7)) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.14.6->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 7)) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.14.6->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 7)) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.14.6->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 7)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.14.6->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 7)) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.14.6->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 7)) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.14.6->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 7)) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 3)) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 6)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 6)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 6)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 6)) (2024.12.14)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 1)) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 1)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 1)) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 1)) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 1)) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 1)) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 1)) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 1)) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 1)) (3.0.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 1)) (4.12.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 1)) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 1)) (0.7.1)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.11/dist-packages (from nbconvert->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 1)) (5.7.2)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 1)) (0.3.0)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 1)) (3.1.1)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 1)) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 1)) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 1)) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 1)) (4.23.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 1)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 1)) (2025.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 1)) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 1)) (1.4.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 1)) (0.8.4)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 1)) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 1)) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 1)) (0.22.3)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.7->nbconvert->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 1)) (4.3.6)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from nbclient>=0.5.0->nbconvert->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 1)) (6.1.12)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 1)) (0.2.13)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 1)) (2.6)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert->otter-grader==1.0.0->-r /content/drive/MyDrive/nlp_lab/lab4-4/requirements.txt (line 1)) (24.0.1)\n",
            "Requirement already satisfied: otter-grader in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from otter-grader) (6.0.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from otter-grader) (5.10.4)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (from otter-grader) (7.34.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.11/dist-packages (from otter-grader) (7.16.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from otter-grader) (4.67.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from otter-grader) (75.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from otter-grader) (2.2.2)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.11/dist-packages (from otter-grader) (6.4.2)\n",
            "Requirement already satisfied: docker in /usr/local/lib/python3.11/dist-packages (from otter-grader) (7.1.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from otter-grader) (3.1.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from otter-grader) (0.3.8)\n",
            "Requirement already satisfied: pdfkit in /usr/local/lib/python3.11/dist-packages (from otter-grader) (1.0.0)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (from otter-grader) (3.0.1)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from docker->otter-grader) (2.32.3)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker->otter-grader) (2.3.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->otter-grader) (3.0.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert->otter-grader) (4.12.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert->otter-grader) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert->otter-grader) (0.7.1)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.11/dist-packages (from nbconvert->otter-grader) (5.7.2)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert->otter-grader) (0.3.0)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert->otter-grader) (3.1.1)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert->otter-grader) (0.10.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from nbconvert->otter-grader) (24.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert->otter-grader) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->otter-grader) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->otter-grader) (4.23.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas->otter-grader) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->otter-grader) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->otter-grader) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->otter-grader) (2025.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->otter-grader) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert->otter-grader) (1.4.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython->otter-grader) (0.8.4)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->otter-grader) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->otter-grader) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->otter-grader) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->otter-grader) (0.22.3)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.7->nbconvert->otter-grader) (4.3.6)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from nbclient>=0.5.0->nbconvert->otter-grader) (6.1.12)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython->otter-grader) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->otter-grader) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->otter-grader) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->docker->otter-grader) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->docker->otter-grader) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->docker->otter-grader) (2024.12.14)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert->otter-grader) (2.6)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert->otter-grader) (24.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from referencing>=0.28.4->jsonschema>=2.6->nbformat->otter-grader) (4.12.2)\n",
            "\n",
            "Command failed with exit code 256\n",
            "Téléchargement des fichiers nécessaires depuis Google Drive...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "id": "dfb32344",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "dfb32344"
      },
      "outputs": [],
      "source": [
        "# Initialize Otter\n",
        "import otter\n",
        "grader = otter.Notebook()"
      ]
    },
    {
      "cell_type": "raw",
      "id": "df4a4c41",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "df4a4c41"
      },
      "source": [
        "%%latex\n",
        "\\newcommand{\\vect}[1]{\\mathbf{#1}}\n",
        "\\newcommand{\\cnt}[1]{\\sharp(#1)}\n",
        "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
        "\\newcommand{\\softmax}{\\operatorname{softmax}}\n",
        "\\newcommand{\\Prob}{\\Pr}\n",
        "\\newcommand{\\given}{\\,|\\,}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "438f777f",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "438f777f"
      },
      "source": [
        "$$\n",
        "\\renewcommand{\\vect}[1]{\\mathbf{#1}}\n",
        "\\renewcommand{\\cnt}[1]{\\sharp(#1)}\n",
        "\\renewcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
        "\\renewcommand{\\softmax}{\\operatorname{softmax}}\n",
        "\\renewcommand{\\Prob}{\\Pr}\n",
        "\\renewcommand{\\given}{\\,|\\,}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "135f59d4",
      "metadata": {
        "tags": [
          "remove_for_latex"
        ],
        "id": "135f59d4"
      },
      "source": [
        "# Course 236299\n",
        "## Lab 4-4 - Sequence-to-sequence models with attention"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43f2e5c9",
      "metadata": {
        "id": "43f2e5c9"
      },
      "source": [
        "In lab 4-3, you built a sequence-to-sequence model in its most basic form and applied it to the task of words-to-numbers conversion. That model first encodes the source sequence into a fixed-size vector (encoder final states), and then decodes based on that vector. Since the only way information from the source side can flow to the target side is through this fixed-size vector, it presents a bottleneck in the encoder-decoder model: no matter how long the source sentence is, it must always be compressed into this fixed-size vector.\n",
        "\n",
        "An _attention mechanism_ (proposed in [this seminal paper](https://arxiv.org/abs/1409.0473)), and discussed in segment 2, offers a workaround by providing the decoder a dynamic view of the source-side as the decoding proceeds. Instead of compressing the source sequence into a *fixed-size* vector, we preserve the \"resolution\" and encode the source sequence into a *set of vectors* (usually with the same size as the source sequence) which is sometimes called a *memory bank*. When predicting each word, the decoder \"attends to\" this memory bank and assigns a weight to each vector in the set, and the weighted sum of those vectors will be used to make a prediction. Hopefully, the decoder will assign higher weights to more relevant source words when predicting a target word, which we'll test in this lab."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fac036ea",
      "metadata": {
        "id": "fac036ea"
      },
      "source": [
        "New bits of Pytorch used in this lab, and which you may find useful include:\n",
        "\n",
        "* [torch.transpose](https://pytorch.org/docs/stable/generated/torch.transpose.html): swaps two dimensions of a tensor.\n",
        "* [torch.reshape](https://pytorch.org/docs/stable/generated/torch.reshape.html): reshapes a tensor.\n",
        "* [torch.bmm](https://pytorch.org/docs/stable/generated/torch.bmm.html): Performs batched matrix multiplication.\n",
        "* [torch.nn.utils.rnn.pack_padded_sequence](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html) (imported as `pack`): Handles paddings. A more detailed explanation can be found [here](https://stackoverflow.com/a/56211056).\n",
        "* [torch.nn.utils.rnn.pad_packed_sequence](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_packed_sequence.html) (imported as `unpack`): Handles paddings.\n",
        "* [torch.masked_fill](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.masked_fill): Fills tensor elements with a value in spots where mask is `True`.\n",
        "* [torch.softmax](https://pytorch.org/docs/stable/_modules/torch/nn/functional.html#softmax): Computes softmax.\n",
        "* [torch.repeat](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.repeat): Repeats a tensor along the specified dimensions.\n",
        "* [torch.triu](https://pytorch.org/docs/stable/generated/torch.triu.html): Returns the upper triangular part of a matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03de1182",
      "metadata": {
        "id": "03de1182"
      },
      "source": [
        "# Preparation - Loading data {-}\n",
        "\n",
        "We use the same data as in lab 4-3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "id": "3b79ea00",
      "metadata": {
        "deletable": false,
        "execution": {
          "iopub.execute_input": "2025-02-03T08:14:03.003458Z",
          "iopub.status.busy": "2025-02-03T08:14:03.002997Z",
          "iopub.status.idle": "2025-02-03T08:14:08.889696Z",
          "shell.execute_reply": "2025-02-03T08:14:08.888863Z"
        },
        "id": "3b79ea00"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import csv\n",
        "import math\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import wget\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.pre_tokenizers import WhitespaceSplit\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "from tokenizers import normalizers\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
        "from torch.nn.utils.rnn import pad_packed_sequence as unpack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "id": "8e655b2b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "execution": {
          "iopub.execute_input": "2025-02-03T08:14:08.893199Z",
          "iopub.status.busy": "2025-02-03T08:14:08.892879Z",
          "iopub.status.idle": "2025-02-03T08:14:08.905904Z",
          "shell.execute_reply": "2025-02-03T08:14:08.905070Z"
        },
        "id": "8e655b2b",
        "outputId": "5a53ce03-f3d8-4f0a-80da-85fa41bc5e34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "# Specify matplotlib configuration\n",
        "%matplotlib inline\n",
        "plt.style.use('tableau-colorblind10')\n",
        "\n",
        "# GPU check, make sure to use GPU where available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "id": "9515a76e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-03T08:14:08.908789Z",
          "iopub.status.busy": "2025-02-03T08:14:08.908395Z",
          "iopub.status.idle": "2025-02-03T08:14:11.757082Z",
          "shell.execute_reply": "2025-02-03T08:14:11.756264Z"
        },
        "id": "9515a76e"
      },
      "outputs": [],
      "source": [
        "# Download data\n",
        "def download_if_needed(source, dest, filename):\n",
        "    os.makedirs(dest, exist_ok=True) # ensure destination\n",
        "    os.path.exists(f\"./{dest}{filename}\") or wget.download(source + filename, out=dest)\n",
        "\n",
        "local_dir = \"data/\"\n",
        "remote_dir = \"https://github.com/nlp-236299/data/raw/master/Words2Num/\"\n",
        "os.makedirs(local_dir, exist_ok=True)\n",
        "\n",
        "for filename in [\n",
        "    \"train.src\",\n",
        "    \"train.tgt\",\n",
        "    \"dev.src\",\n",
        "    \"dev.tgt\",\n",
        "    \"test.src\",\n",
        "    \"test.tgt\",\n",
        "]:\n",
        "    download_if_needed(remote_dir, local_dir, filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "873748d3",
      "metadata": {
        "id": "873748d3"
      },
      "source": [
        "As in lab 4-3, we process the dataset by extracting the sequences and their corresponding labels and save it in CSV format. Then, we load the data from the CSV files, train the tokenizers, prepend `<bos>` and appended `<eos>` to target sentences, and convert the data to sequences of token ids."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "id": "3b11339b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-03T08:14:11.760573Z",
          "iopub.status.busy": "2025-02-03T08:14:11.760322Z",
          "iopub.status.idle": "2025-02-03T08:14:26.712494Z",
          "shell.execute_reply": "2025-02-03T08:14:26.711639Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320,
          "referenced_widgets": [
            "2d9ad64d58da471b8f8790f0fe815232",
            "5fc059065cfd4c42b939e79112dfabd6",
            "506717f4061c4619a721ce8e3f4844b1",
            "aedd41dd02694af2b819729e04ea4fa5",
            "a3da00aaf77f440d8585c3c4703052f7",
            "2b5c59b5556046548b4e8680cde508ea",
            "15d9ad1f824b47adad2ddbaf336dd6be",
            "be8280ed4e99468d885cb7d2777406e1",
            "194d78b24b144db08c4bc2a65cc3a16e",
            "f08ee8ed36ca41ab8a11e14ea1c31e2c",
            "c195e1c8a55b49a9adca7f49c6de447c",
            "fcf8752babb44cce957ca82e05084730",
            "da3630d237e14b0faaa3f15b743139b6",
            "1d6a6a4849734501afdc39551110f362",
            "a6d8a8d74ccb4703b63bfbc5ee705361",
            "f36b397db4a44747a3f4c81ef298d51e",
            "852a37eeeac04d63b268b9e837b5a7a1",
            "6a17303d2b994d17bc9ec1dd910f7b7d",
            "bfa7ea3a6993419d9aeb2c64e12736b2",
            "1ff11861244b462d9be647c438c32225",
            "6fbf655a6b36422f81c34995fa8bb6b0",
            "18c4ebb3c4874691bba588856b0a737c",
            "d2d8e18180334e43bcd1bc8c5f96df3a",
            "2e28b3a7f07543f5a5397e0808b42634",
            "6fa5aa292094453382abbb86f0825908",
            "76aba658d6b74c85b208024ff2d012e2",
            "191cd842d284466aa04bbcff95911ec8",
            "b61da649ea434eb780bcd0c575894f1a",
            "2d23866d7e4c4302a12205e37f8715f2",
            "f9977b0d3f6d405aa3c96f253b977b7f",
            "cadb85842aec4ad5a757594af82bd251",
            "7b19517bb6944a28bb24a41cbb556d00",
            "a07b51b4f6134da2adb305469fcd6be9",
            "bce6a31ddab14b058b26cfde8227c7b8",
            "cef0adb956fd4b73958c19fee46e0fe2",
            "45785db7487242c794f3a3263ef799e8",
            "0029f49d79d84299a275ab7460df7076",
            "908df4d697204b45b4da54dce30c2370",
            "7392551ffd374c1082d7efe575de40a9",
            "adc1ce8ebe5f48d09542322cc073865f",
            "549cbbbadb77428c97834ecc4c484fce",
            "32b48bff085f4fdab90dbf80dda45a67",
            "fb3d3d1116ca4aebad580adbdb92d790",
            "328b52e075ef4372b30fc266754b798d",
            "2cad6f20488c4758ae2a4846f414924c",
            "9334b665a6d94bafbced613a149c2c5e",
            "d5731c336b484cce9a3a0bb3b182c4ec",
            "1c003eb3f7a644fe808df053121ce636",
            "6e43480232794f4ca89b349bcf8d4ee4",
            "6ab7ef4dd4da43a4a7e8940392ce2e16",
            "a1301694c812416dbd227ea01bfee594",
            "b08baacda7504d5ea5f2fd490c874094",
            "2f331d90f33d4017a5ccbd4fe8ac32cd",
            "7944c8b7a6bd4e30940aea29fe057afa",
            "0d7509da3a0d4e979c1a9015aab30e9e",
            "738149b8f23c4737b540047b215f44d8",
            "c2e3aa0f31634404abebe159882a9c5c",
            "0359d67730d74a31b639f10316f02683",
            "8cacb7fac2c4455aa4a831774023af1c",
            "2c9ea7f11764424d8731dd893693ce9c",
            "8e9282c1e5554442a2786de175698724",
            "4c66a900283147a78be308e9860b7bf2",
            "494dd4c476474c6d852e66ab46131a10",
            "323ad88270b940ab9084aec5ce2319c3",
            "bae4b13897ae4a7fa545d488917b3b79",
            "1e05916022f047719e491a5bda45fb78"
          ]
        },
        "id": "3b11339b",
        "outputId": "c11110a7-890a-428f-8dae-d680d133538e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2d9ad64d58da471b8f8790f0fe815232"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating val split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fcf8752babb44cce957ca82e05084730"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d2d8e18180334e43bcd1bc8c5f96df3a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/65022 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bce6a31ddab14b058b26cfde8227c7b8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/700 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2cad6f20488c4758ae2a4846f414924c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/700 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "738149b8f23c4737b540047b215f44d8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of src vocab: 34\n",
            "Size of tgt vocab: 14\n",
            "Index for src padding: 0\n",
            "Index for tgt padding: 0\n",
            "Index for start of sequence token: 2\n",
            "Index for end of sequence token: 3\n"
          ]
        }
      ],
      "source": [
        "# Process data\n",
        "for split in ['train', 'dev', 'test']:\n",
        "    src_in_file = f'{local_dir}{split}.src'\n",
        "    tgt_in_file = f'{local_dir}{split}.tgt'\n",
        "    out_file = f'{local_dir}{split}.csv'\n",
        "\n",
        "    with open(src_in_file, 'r') as f_src_in, open(tgt_in_file, 'r') as f_tgt_in:\n",
        "        with open(out_file, 'w', newline='') as f_out:\n",
        "            src, tgt= [], []\n",
        "            writer = csv.writer(f_out)\n",
        "            writer.writerow(('src','tgt'))\n",
        "            for src_line, tgt_line in zip(f_src_in, f_tgt_in):\n",
        "                writer.writerow((src_line.strip(), tgt_line.strip()))\n",
        "\n",
        "dataset = load_dataset('csv', data_files={'train':f'{local_dir}train.csv', \\\n",
        "                                          'val': f'{local_dir}dev.csv', \\\n",
        "                                          'test': f'{local_dir}test.csv'})\n",
        "\n",
        "train_data = dataset['train']\n",
        "val_data = dataset['val']\n",
        "test_data = dataset['test']\n",
        "\n",
        "unk_token = '[UNK]'\n",
        "pad_token = '[PAD]'\n",
        "bos_token = '<bos>'\n",
        "eos_token = '<eos>'\n",
        "src_tokenizer = Tokenizer(WordLevel(unk_token=unk_token))\n",
        "src_tokenizer.pre_tokenizer = WhitespaceSplit()\n",
        "\n",
        "src_trainer = WordLevelTrainer(special_tokens=[pad_token, unk_token])\n",
        "src_tokenizer.train_from_iterator(train_data['src'], trainer=src_trainer)\n",
        "\n",
        "tgt_tokenizer = Tokenizer(WordLevel(unk_token=unk_token))\n",
        "tgt_tokenizer.pre_tokenizer = WhitespaceSplit()\n",
        "\n",
        "tgt_trainer = WordLevelTrainer(special_tokens=[pad_token, unk_token, bos_token, eos_token])\n",
        "\n",
        "tgt_tokenizer.train_from_iterator(train_data['tgt'], trainer=tgt_trainer)\n",
        "\n",
        "tgt_tokenizer.post_processor = TemplateProcessing(single=f\"{bos_token} $A {eos_token}\", special_tokens=[(bos_token, tgt_tokenizer.token_to_id(bos_token)), (eos_token, tgt_tokenizer.token_to_id(eos_token))])\n",
        "\n",
        "hf_src_tokenizer = PreTrainedTokenizerFast(tokenizer_object=src_tokenizer, pad_token=pad_token, unk_token=unk_token)\n",
        "hf_tgt_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tgt_tokenizer, pad_token=pad_token, unk_token=unk_token, bos_token=bos_token, eos_token=eos_token)\n",
        "\n",
        "def encode(example):\n",
        "    example['src_ids'] = hf_src_tokenizer(example['src']).input_ids\n",
        "    example['tgt_ids'] = hf_tgt_tokenizer(example['tgt']).input_ids\n",
        "    return example\n",
        "\n",
        "train_data = train_data.map(encode)\n",
        "val_data = val_data.map(encode)\n",
        "test_data = test_data.map(encode)\n",
        "\n",
        "# Compute size of vocabulary\n",
        "src_vocab = src_tokenizer.get_vocab()\n",
        "tgt_vocab = tgt_tokenizer.get_vocab()\n",
        "\n",
        "print(f\"Size of src vocab: {len(src_vocab)}\")\n",
        "print(f\"Size of tgt vocab: {len(tgt_vocab)}\")\n",
        "print(f\"Index for src padding: {src_vocab[pad_token]}\")\n",
        "print(f\"Index for tgt padding: {tgt_vocab[pad_token]}\")\n",
        "print(f\"Index for start of sequence token: {tgt_vocab[bos_token]}\")\n",
        "print(f\"Index for end of sequence token: {tgt_vocab[eos_token]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58f66ae6",
      "metadata": {
        "id": "58f66ae6"
      },
      "source": [
        "To load data in batched tensors, we use `torch.utils.data.DataLoader` for data splits, which enables us to iterate over the dataset under a given `BATCH_SIZE`. For the test set, we use a batch size of 1, to make the decoding implementation easier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "id": "6073dfb0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-03T08:14:26.716401Z",
          "iopub.status.busy": "2025-02-03T08:14:26.716172Z",
          "iopub.status.idle": "2025-02-03T08:14:26.724807Z",
          "shell.execute_reply": "2025-02-03T08:14:26.724068Z"
        },
        "id": "6073dfb0"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32     # batch size for training and validation\n",
        "TEST_BATCH_SIZE = 1 # batch size for test; we use 1 to make implementation easier\n",
        "\n",
        "# Defines how to batch a list of examples together\n",
        "def collate_fn(examples):\n",
        "    batch = {}\n",
        "    bsz = len(examples)\n",
        "    src_ids, tgt_ids = [], []\n",
        "    for example in examples:\n",
        "        src_ids.append(example['src_ids'])\n",
        "        tgt_ids.append(example['tgt_ids'])\n",
        "\n",
        "    src_len = torch.LongTensor([len(word_ids) for word_ids in src_ids]).to(device)\n",
        "    src_max_length = max(src_len)\n",
        "    tgt_max_length = max([len(word_ids) for word_ids in tgt_ids])\n",
        "\n",
        "    src_batch = torch.zeros(bsz, src_max_length).long().fill_(src_vocab[pad_token]).to(device)\n",
        "    tgt_batch = torch.zeros(bsz, tgt_max_length).long().fill_(tgt_vocab[pad_token]).to(device)\n",
        "    for b in range(bsz):\n",
        "        src_batch[b][:len(src_ids[b])] = torch.LongTensor(src_ids[b]).to(device)\n",
        "        tgt_batch[b][:len(tgt_ids[b])] = torch.LongTensor(tgt_ids[b]).to(device)\n",
        "\n",
        "    batch['src_lengths'] = src_len\n",
        "    batch['src_ids'] = src_batch\n",
        "    batch['tgt_ids'] = tgt_batch\n",
        "    return batch\n",
        "\n",
        "train_iter = torch.utils.data.DataLoader(train_data,\n",
        "                                         batch_size=BATCH_SIZE,\n",
        "                                         shuffle=True,\n",
        "                                         collate_fn=collate_fn)\n",
        "val_iter = torch.utils.data.DataLoader(val_data,\n",
        "                                       batch_size=BATCH_SIZE,\n",
        "                                       shuffle=False,\n",
        "                                       collate_fn=collate_fn)\n",
        "test_iter = torch.utils.data.DataLoader(test_data,\n",
        "                                        batch_size=TEST_BATCH_SIZE,\n",
        "                                        shuffle=False,\n",
        "                                        collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2dc052c",
      "metadata": {
        "id": "a2dc052c"
      },
      "source": [
        "Let's take a look at a batch from these iterators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "id": "1ab33742",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-02-03T08:14:26.727727Z",
          "iopub.status.busy": "2025-02-03T08:14:26.727515Z",
          "iopub.status.idle": "2025-02-03T08:14:26.740578Z",
          "shell.execute_reply": "2025-02-03T08:14:26.739794Z"
        },
        "id": "1ab33742",
        "outputId": "4a1b265f-67d9-44f7-b8b7-b1f93def2532"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of src batch: torch.Size([32, 19])\n",
            "Third src sentence in batch: tensor([12,  5,  6,  3,  2, 30,  4, 12,  3,  2, 15, 12,  0,  0,  0,  0,  0,  0,\n",
            "         0])\n",
            "Length of the third src sentence in batch: 19\n",
            "Converted back to string: one million two hundred and ten thousand one hundred and thirty one [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Size of tgt batch: torch.Size([32, 12])\n",
            "Third tgt sentence in batch: tensor([ 2, 12,  7, 12, 13, 12,  4, 12,  3,  0,  0,  0])\n",
            "Converted back to string: <bos> 1 2 1 0 1 3 1 <eos> [PAD] [PAD] [PAD]\n"
          ]
        }
      ],
      "source": [
        "batch = next(iter(train_iter))\n",
        "src_ids = batch['src_ids']\n",
        "src_example = src_ids[2]\n",
        "print (f\"Size of src batch: {src_ids.size()}\")\n",
        "print (f\"Third src sentence in batch: {src_example}\")\n",
        "print (f\"Length of the third src sentence in batch: {len(src_example)}\")\n",
        "print (f\"Converted back to string: {hf_src_tokenizer.decode(src_example)}\")\n",
        "\n",
        "tgt_ids = batch['tgt_ids']\n",
        "tgt_example = tgt_ids[2]\n",
        "print (f\"Size of tgt batch: {tgt_ids.size()}\")\n",
        "print (f\"Third tgt sentence in batch: {tgt_example}\")\n",
        "print (f\"Converted back to string: {hf_tgt_tokenizer.decode(tgt_example)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b14e13f",
      "metadata": {
        "id": "6b14e13f"
      },
      "source": [
        "# The attention mechanism\n",
        "\n",
        "> **Note:** This is a rehash of the attention mechanism, as presented in segment 2. Use it for reference and feel free to skim it if you remember the mechanism well.\n",
        "\n",
        "\n",
        "Attention works by _querying_ a (dynamically sized) set of _keys_ associated with _values_. As usual, the query, keys, and values are represented as vectors. The query process provides a score that specifies how much each key should be attended to. The attention can then be summarized by taking an average of the values weighted by the attention score of the corresponding keys. This _context vector_ can then be used as another input to other processes.\n",
        "\n",
        "More formally, let's suppose we have a query vector $\\mathbf{q}\\in \\mathbb{R}^D$, a set of $S$ key-value pairs $\\{(\\mathbf{k}_i, \\mathbf{v}_i) \\in \\mathbb{R}^D \\times \\mathbb{R}^D: i \\in \\{1, 2, \\cdots, S\\} \\}$, where $D$ is the hidden size. What we want to do through the attention mechanism is to use the query to attend to the keys, and summarize those values associated with the \"relevant\" keys into a fixed-size context vector $\\mathbf{c}\\in\\mathbb{R}^D$. Note that this is different from directly compressing the key-value pairs into a fixed-size vector, since depending on the query, we might end up with different context vectors.\n",
        "\n",
        "To determine the score for a given query and key, it is standard to use a measure of similarity between the query and key. You've seen such similarity measures before, in labs 1-1 and 1-2. A good choice is simply the normalized dot product between query and key. We'll thus take the attention score for query $\\mathbf{q}$ and key $\\mathbf{k}_i$ to be\n",
        "$$\n",
        "a_i = \\frac{\\exp(\\mathbf{q} \\cdot \\mathbf{k}_i)}{Z},\n",
        "$$\n",
        "where $\\cdot$ denotes the dot product (inner product) and $\\exp$ is exponentiation which ensures that all scores are nonnegative, and\n",
        "$$Z = \\sum_{i=1}^{S} \\exp(\\mathbf{q} \\cdot \\mathbf{k}_i)$$\n",
        "is the normalizer to guarantee the scores all sum to one. (There are multiple ways of parameterizing the attention function, but the form we present here is the most popular one.) You might have noticed that the operation above is essentially a softmax over $\\mathbf{q}\\cdot\\mathbf{k}$.\n",
        "\n",
        "The attention scores $\\mathbf{a}$ lie on a *simplex* (meaning $a_i\\ge 0$ and $\\sum_i a_i=1$), which lends it some interpretability: the closer $a_i$ is to 1, the more \"relevant\" a key $k_i$ (and hence its value $v_i$) is to the given query. We will observe this later in the lab: When we are about to predict the target word \"3\", $a_i$ is close to 1 for the source word $x_i=\\text{\"three\"}$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b8748d6",
      "metadata": {
        "id": "0b8748d6"
      },
      "source": [
        "To compute the context vector $\\mathbf{c}$, we take the weighted sum of values using the corresponding attention scores as weights:\n",
        "$$\n",
        "\\mathbf{c} = \\sum_{i=1}^S a_i \\mathbf{v}_i\n",
        "$$\n",
        "The closer $a_i$ is to 1, the higher the weight $\\mathbf{v}_i$ receives."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "782036ec",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "782036ec"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "---\n",
        "**Question:** In the extreme, if there exists $i$ for which $a_i$ is 1, then what will the value of $\\mathbf{c}$ be?\n",
        "\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: open_response_a_i_1\n",
        "manual: true\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "406589ac",
      "metadata": {
        "id": "406589ac"
      },
      "source": [
        "If there is an index \\( i \\) where the attention weight \\( a_i \\) is exactly 1, the mechanism fully focuses on that specific key-value pair while completely ignoring all others. As a result, the context vector becomes identical to the corresponding value \\( v_i \\), without any contribution from the rest of the sequence.\n",
        "\n",
        "In this case, the attention mechanism acts as a hard selection filter, isolating the most relevant part of the input without blending different pieces of information. While this is useful for one-to-one mappings (such as converting \"three\" to \"3\"), it may not be ideal for tasks requiring a broader contextual understanding, where multiple tokens collectively influence the meaning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d80dbff",
      "metadata": {
        "id": "5d80dbff"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "\n",
        "\n",
        "In practice, instead of computing the context vector once for each query, we want to batch computations for different queries together for parallel processing on GPUs. This will become especially useful for the transformer implementation. We use a matrix $Q\\in\\mathbb{R}^{{T} \\times D}$ to store $T$ queries, a matrix $K\\in\\mathbb{R}^{S \\times D}$ to store $S$ keys, and a matrix $V\\in\\mathbb{R}^{S\\times D}$ to store the corresponding values. Then we can write down how we compute the attention scores $A\\in\\mathbb{R}^{T \\times S}$ in a matrix form:\n",
        "\n",
        "$$\n",
        "A = \\operatorname{softmax} (Q K^{\\top}, \\text{dim}=-1),\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e5bdabd",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "2e5bdabd"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "---\n",
        "**Question:** What is the shape of $A$? What does $A_{ij}$ represent?\n",
        "\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: open_response_a_ij_meaning\n",
        "manual: true\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15444700",
      "metadata": {
        "id": "15444700"
      },
      "source": [
        "the shape of a is T × S where  \n",
        "\n",
        "- T is the number of queries i e the number of decoding time steps  \n",
        "- S is the number of keys i e the number of encoding time steps  \n",
        "\n",
        "each element Aij represents the attention weight assigned to the j th key encoder output when computing the i th query decoder state  \n",
        "\n",
        "in other words Aij indicates how much influence the j th source token has on generating the i th target token the sum of each row in a is equal to 1 as softmax ensures that attention weights are normalized across all source tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebc331e1",
      "metadata": {
        "id": "ebc331e1"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "\n",
        "\n",
        "To get the context matrix $C \\in \\mathbb{R}^{T \\times D}$:\n",
        "\n",
        "$$\n",
        "C = A V\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c59b4bf",
      "metadata": {
        "id": "3c59b4bf"
      },
      "source": [
        "You are given the implementation to the attention function from lab 2-4, which takes the $Q$, $K$, and $V$ matrices and returns the $A$ and $C$ matrices. Note that for these matrices, there is one additional dimension for the batching, so instead of $Q\\in \\mathbb{R}^{T \\times D}$, $K,V\\in \\mathbb{R}^{S \\times D}$, $A\\in \\mathbb{R}^{T \\times S}$, $C\\in \\mathbb{R}^{T \\times D}$, we have $Q\\in\\mathbb{R}^{B \\times T \\times D}$, $K,V\\in \\mathbb{R}^{B \\times S\\times D}$, $A\\in \\mathbb{R}^{B\\times T \\times S}$, $C\\in \\mathbb{R}^{B \\times T \\times D}$, where $B$ is the batch size.  In addition, the function below also takes an argument `mask` of size $\\mathbb{R}^{B\\times T \\times S}$ to mark where attentions are disallowed. This is useful not only in disallowing attending to padding symbols, but also in implementing the transformer model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "id": "bc038832",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-03T08:14:26.744118Z",
          "iopub.status.busy": "2025-02-03T08:14:26.743899Z",
          "iopub.status.idle": "2025-02-03T08:14:26.750873Z",
          "shell.execute_reply": "2025-02-03T08:14:26.750021Z"
        },
        "id": "bc038832"
      },
      "outputs": [],
      "source": [
        "def attention(batched_Q, batched_K, batched_V, mask=None):\n",
        "  \"\"\"\n",
        "  Performs the attention operation and returns the attention matrix\n",
        "  `batched_A` and the context matrix `batched_C` using queries\n",
        "  `batched_Q`, keys `batched_K`, and values `batched_V`.\n",
        "\n",
        "  Arguments:\n",
        "      batched_Q: (bsz, q_len, D)\n",
        "      batched_K: (bsz, k_len, D)\n",
        "      batched_V: (bsz, k_len, D)\n",
        "      mask: (bsz, q_len, k_len). An optional boolean mask *disallowing*\n",
        "            attentions where the mask value is *`False`*.\n",
        "  Returns:\n",
        "      batched_A: the normalized attention scores (bsz, q_len, k_len)\n",
        "      batched_C: a tensor of size (bsz, q_len, D).\n",
        "  \"\"\"\n",
        " # Check sizes\n",
        "  D = batched_Q.size(-1)\n",
        "  bsz = batched_Q.size(0)\n",
        "  q_len = batched_Q.size(1)\n",
        "  k_len = batched_K.size(1)\n",
        "\n",
        "  assert batched_K.size(-1) == D and batched_V.size(-1) == D\n",
        "  assert batched_K.size(0) == bsz and batched_V.size(0) == bsz\n",
        "  assert batched_V.size(1) == k_len\n",
        "  if mask is not None:\n",
        "    assert mask.size() == torch.Size([bsz, q_len, k_len])\n",
        "\n",
        "  q = batched_Q                 # bsz, q_len, hidden\n",
        "  k = batched_K.transpose(1, 2) # bsz, hidden, k_len\n",
        "  # Compute unnormalized attention scores\n",
        "  scores = torch.bmm(q, k) / math.sqrt(D) # bsz, q_len, k_len\n",
        "  # Mask attention scores to -inf where mask is False\n",
        "  if mask is not None:\n",
        "    scores = scores.masked_fill(mask == False, -torch.inf)\n",
        "  batched_A = torch.softmax(scores, dim=-1) # bsz, q_len, k_len\n",
        "  batched_C = torch.bmm(batched_A, batched_V) # bsz, q_len, D\n",
        "  # Verify that things sum up to one properly.\n",
        "  assert torch.all(torch.isclose(batched_A.sum(-1),\n",
        "                                 torch.ones(bsz, q_len).to(device)))\n",
        "  return batched_A, batched_C"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4aeada8",
      "metadata": {
        "id": "d4aeada8"
      },
      "source": [
        "## Neural encoder-decoder models with attention\n",
        "\n",
        "Now we can add an attention mechanism to our encoder-decoder model. As in lab 4-3, we use a bidirectional LSTM as the encoder, and a unidirectional LSTM as the decoder, and initialize the decoder state with the encoder final state. However, instead of directly projecting the decoder hidden state to logits, we use it as a query vector and attend to all encoder outputs (used as both keys and values), and then concatanate the resulting context vector with the query vector, and project to logits. In addition, we add the context vector to the word embedding at the next time step, so that the LSTM can be aware of the previous attention results.\n",
        "\n",
        "<img src=\"https://github.com/nlp-course/data/raw/master/img/encoder_decoder_attn.png\" alt=\"encoder-decoder-attn illustration\" />\n",
        "\n",
        "In the above illustration, at the first time step, we use $q_1$ to denote the decoder output. Instead of directly projecting that to logits as in lab 4-3, we use $q_1$ as the query vector, and use it to attend to the memory bank (which is the set of encoder outputs) and get the context vector $c_1$. We concatenate $c_1$ with $q_1$, and project the result to the vocabulary size to get logits. At the next step, we first embed $y_1$ into embeddings, and then **add** $c_1$ to it (via componentwise addition) and use the sum as the decoder input. This process continues until an end-of-sequence is produced.\n",
        "\n",
        "You'll need to implement `forward_encoder` and `forward_decoder_incrementally` in the code below. The `forward_encoder` function will return a \"memory bank\" in addition to the final states. The \"memory bank\" is simply the encoder outputs at all time steps, which is the first returned value of `torch.nn.LSTM`.\n",
        "\n",
        "The `forward_decoder_incrementally` function forwards the LSTM cell for a single time step. It takes the initial decoder state, the memory bank, and the input word at the current time step and returns logits for this time step. In addition, it needs to return the context vector and the updated decoder state, which will be used for the next time step. Note that here you need to consider **batch sizes greater than 1**, as this function is used in `forward_decoder`, which is used during training.\n",
        "\n",
        "In summary, the steps in decoding are:\n",
        "\n",
        "1. Map the target words to word embeddings. Add the context vector from the previous time step if any. Use the result as the input to the decoder.\n",
        "\n",
        "2. Forward the decoder RNN for one time step. Use the decoder output as query, the memory bank as **both keys and values**, and compute the context vector through the attention mechanism. Since we don't want to attend to padding symbols at the source side, we also need to pass in a proper `mask` to the attention function.\n",
        "\n",
        "3. Concatenate the context vector with the decoder output, and project the concatenation to vocabulary size as (unnormalized) logits. Normalize them using `torch.log_softmax` if `normalize` is `True`.\n",
        "\n",
        "4. Update the decoder hidden state and the context vector, which will be used in the next time step.\n",
        "\n",
        "Before proceeding, let's consider a simple question: in lab 4-3, we tried to avoid `for` loops, but if you read the code of `forward_decoder` in this lab, you might notice a `for` loop. Is this unavoidable?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cda53d6",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "9cda53d6"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "---\n",
        "**Question:** Recall that in the `forward_decoder` function in lab 4-3 we didn't use any for loops but instead used a single call to `self.decoder_rnn`. Why do we need a `for` loop in the function `forward_decoder` below? Is it possible to get rid of the for loop to make the code more efficient?\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: open_response_for_loop\n",
        "manual: true\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9ab28da",
      "metadata": {
        "id": "a9ab28da"
      },
      "source": [
        "in lab 4 3 we did not use a for loop in forward decoder because we could process the entire sequence at once by passing it to self decoder rnn this was possible because during training we already had the full target sequence  \n",
        "\n",
        "however in forward decoder here we use **auto regressive decoding** which means each output depends on the previous one during inference we do not know the full target sequence in advance so we must generate it **step by step** feeding each predicted word back into the model this makes it **impossible to process everything at once** and requires a for loop  \n",
        "\n",
        "to remove the for loop we would need **non autoregressive models** which generate all words at once but these models are harder to train and often less accurate most modern models like transformers still use **autoregressive decoding** but optimize it with techniques like **beam search** to make it faster"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77e19bc6",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "77e19bc6"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "---\n",
        "Now let's implement `forward_encoder` and `forward_decoder_incrementally`.\n",
        "\n",
        "> Hint on using `pack`: if you use `pack` to handle paddings and pass the result as encoder inputs, you need to use `unpack` and extract the first returned value as the memory bank. An example for using `pack` can be found [here](https://stackoverflow.com/a/55805785), but note that our input is already the padded sequences.\n",
        "\n",
        "> Hint on ignoring source-side paddings in the attention mechanism: what `mask` should we pass into the `attention` function??\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: attn_encoder_decoder\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "id": "d88382d0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-03T08:14:26.754134Z",
          "iopub.status.busy": "2025-02-03T08:14:26.753919Z",
          "iopub.status.idle": "2025-02-03T08:14:26.778542Z",
          "shell.execute_reply": "2025-02-03T08:14:26.777864Z"
        },
        "id": "d88382d0"
      },
      "outputs": [],
      "source": [
        "#TODO - implement `forward_encoder` and `forward_decoder_incrementally`.\n",
        "class AttnEncoderDecoder(nn.Module):\n",
        "  def __init__(self, hf_src_tokenizer, hf_tgt_tokenizer, hidden_size=64, layers=3):\n",
        "    \"\"\"\n",
        "    Initializer. Creates network modules and loss function.\n",
        "    Arguments:\n",
        "        hf_src_tokenizer: hf src tokenizer\n",
        "        hf_tgt_tokenizer: hf tgt tokenizer\n",
        "        hidden_size: hidden layer size of both encoder and decoder\n",
        "        layers: number of layers of both encoder and decoder\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.hf_src_tokenizer = hf_src_tokenizer\n",
        "    self.hf_tgt_tokenizer = hf_tgt_tokenizer\n",
        "\n",
        "    # Keep the vocabulary sizes available\n",
        "    self.V_src = len(self.hf_src_tokenizer)\n",
        "    self.V_tgt = len(self.hf_tgt_tokenizer)\n",
        "\n",
        "    # Get special word ids\n",
        "    self.padding_id_src = self.hf_src_tokenizer.pad_token_id\n",
        "    self.padding_id_tgt = self.hf_tgt_tokenizer.pad_token_id\n",
        "    self.bos_id = self.hf_tgt_tokenizer.bos_token_id\n",
        "    self.eos_id = self.hf_tgt_tokenizer.eos_token_id\n",
        "\n",
        "    # Keep hyper-parameters available\n",
        "    self.embedding_size = hidden_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.layers = layers\n",
        "\n",
        "    # Create essential modules\n",
        "    self.word_embeddings_src = nn.Embedding(self.V_src, self.embedding_size)\n",
        "    self.word_embeddings_tgt = nn.Embedding(self.V_tgt, self.embedding_size)\n",
        "\n",
        "    # RNN cells\n",
        "    self.encoder_rnn = nn.LSTM(\n",
        "      input_size    = self.embedding_size,\n",
        "      hidden_size   = hidden_size // 2, # to match decoder hidden size\n",
        "      num_layers    = layers,\n",
        "      batch_first=True,\n",
        "      bidirectional = True              # bidirectional encoder\n",
        "    )\n",
        "    self.decoder_rnn = nn.LSTM(\n",
        "      input_size    = self.embedding_size,\n",
        "      hidden_size   = hidden_size,\n",
        "      num_layers    = layers,\n",
        "      batch_first=True,\n",
        "      bidirectional = False             # unidirectional decoder\n",
        "    )\n",
        "\n",
        "    # Final projection layer\n",
        "    self.hidden2output = nn.Linear(2*hidden_size, self.V_tgt) # project the concatenation to logits\n",
        "\n",
        "    # Create loss function\n",
        "    self.loss_function = nn.CrossEntropyLoss(reduction='sum',\n",
        "                                             ignore_index=self.padding_id_tgt)\n",
        "\n",
        "  def forward_encoder(self, src, src_lengths):\n",
        "    \"\"\"\n",
        "    Encodes source words `src`.\n",
        "    Arguments:\n",
        "        src: src batch of size (bsz, max_src_len)\n",
        "        src_lengths: src lengths of size (bsz)\n",
        "    Returns:\n",
        "        memory_bank: a tensor of size (bsz, src_len, hidden_size)\n",
        "        (final_state, context): `final_state` is a tuple (h, c) where h/c is of size\n",
        "                                (layers, bsz, hidden_size), and `context` is `None`.\n",
        "    \"\"\"\n",
        "    #TODO\n",
        "    embedded = self.word_embeddings_src(src)\n",
        "    packed = nn.utils.rnn.pack_padded_sequence(embedded, src_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "    packed_outputs, (h, c) = self.encoder_rnn(packed)\n",
        "    memory_bank, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first=True)\n",
        "    h = h.view(self.layers, 2, h.size(1), h.size(2))  # (layers, 2, batch, hidden/2)\n",
        "    h = torch.cat([h[:, 0], h[:, 1]], dim=-1)  # Merge both directions\n",
        "\n",
        "    c = c.view(self.layers, 2, c.size(1), c.size(2))\n",
        "    c = torch.cat([c[:, 0], c[:, 1]], dim=-1)  # Merge both directions\n",
        "\n",
        "\n",
        "    memory_bank = memory_bank\n",
        "    final_state = (h, c)\n",
        "    context = None\n",
        "    return memory_bank, (final_state, context)\n",
        "\n",
        "  def forward_decoder(self, encoder_final_state, tgt_in, memory_bank, src_mask):\n",
        "    \"\"\"\n",
        "    Decodes based on encoder final state, memory bank, src_mask, and ground truth\n",
        "    target words.\n",
        "    Arguments:\n",
        "        encoder_final_state: (final_state, None) where final_state is the encoder\n",
        "                             final state used to initialize decoder. None is the\n",
        "                             initial context (there's no previous context at the\n",
        "                             first step).\n",
        "        tgt_in: a tensor of size (bsz, tgt_len)\n",
        "        memory_bank: a tensor of size (bsz, src_len, hidden_size), encoder outputs\n",
        "                     at every position\n",
        "        src_mask: a tensor of size (bsz, src_len): a boolean tensor, `False` where\n",
        "                  src is padding (we disallow decoder to attend to those places).\n",
        "    Returns:\n",
        "        Logits of size (bsz, tgt_len, V_tgt) (before the softmax operation)\n",
        "    \"\"\"\n",
        "    max_tgt_length = tgt_in.size(1)\n",
        "\n",
        "    # Initialize decoder state, note that it's a tuple (state, context) here\n",
        "    decoder_states = encoder_final_state\n",
        "\n",
        "    all_logits = []\n",
        "    for i in range(max_tgt_length):\n",
        "      logits, decoder_states, attn = \\\n",
        "        self.forward_decoder_incrementally(decoder_states,\n",
        "                                           tgt_in[:, i],\n",
        "                                           memory_bank,\n",
        "                                           src_mask,\n",
        "                                           normalize=False)\n",
        "      all_logits.append(logits)             # list of bsz, vocab_tgt\n",
        "    all_logits = torch.stack(all_logits, 1) # bsz, tgt_len, vocab_tgt\n",
        "    return all_logits\n",
        "\n",
        "  def forward(self, src, src_lengths, tgt_in):\n",
        "    \"\"\"\n",
        "    Performs forward computation, returns logits.\n",
        "    Arguments:\n",
        "        src: src batch of size (bsz, max_src_len)\n",
        "        src_lengths: src lengths of size (bsz)\n",
        "        tgt_in:  a tensor of size (bsz, tgt_len)\n",
        "    \"\"\"\n",
        "    src_mask = src.ne(self.padding_id_src) # bsz, max_src_len\n",
        "    # Forward encoder\n",
        "    memory_bank, encoder_final_state = self.forward_encoder(src, src_lengths)\n",
        "    # Forward decoder\n",
        "    logits = self.forward_decoder(encoder_final_state, tgt_in, memory_bank, src_mask)\n",
        "    return logits\n",
        "\n",
        "  def forward_decoder_incrementally(self, prev_decoder_states, tgt_in_onestep,\n",
        "                                    memory_bank, src_mask,\n",
        "                                    normalize=True):\n",
        "    \"\"\"\n",
        "    Forward the decoder for a single step with token `tgt_in_onestep`.\n",
        "    This function will be used both in `forward_decoder` and in beam search.\n",
        "    Note that bsz can be greater than 1.\n",
        "    Arguments:\n",
        "        prev_decoder_states: a tuple (prev_decoder_state, prev_context). `prev_context`\n",
        "                             is `None` for the first step\n",
        "        tgt_in_onestep: a tensor of size (bsz), tokens at one step\n",
        "        memory_bank: a tensor of size (bsz, src_len, hidden_size), encoder outputs\n",
        "                     at every position\n",
        "        src_mask: a tensor of size (bsz, src_len): a boolean tensor, `False` where\n",
        "                  src is padding (we disallow decoder to attend to those places).\n",
        "        normalize: use log_softmax to normalize or not. Beam search needs to normalize,\n",
        "                   while `forward_decoder` does not\n",
        "    Returns:\n",
        "        logits: log probabilities for `tgt_in_token` of size (bsz, V_tgt)\n",
        "        decoder_states: (`decoder_state`, `context`) which will be used for the\n",
        "                        next incremental update\n",
        "        attn: normalized attention scores at this step (bsz, src_len)\n",
        "    \"\"\"\n",
        "    prev_decoder_state, prev_context = prev_decoder_states\n",
        "    # Initialize decoder_state and context with previous states if available\n",
        "    decoder_state = prev_decoder_state\n",
        "    context = prev_context\n",
        "    #TODO\n",
        "    tgt_embedded = self.word_embeddings_tgt(tgt_in_onestep).unsqueeze(1)\n",
        "\n",
        "    # Update decoder_states after they are calculated\n",
        "    decoder_states = (decoder_state, context)\n",
        "\n",
        "    attn = None #attn_scores.squeeze(1) # Initialize attn\n",
        "    if prev_context is not None:\n",
        "        tgt_embedded = tgt_embedded + prev_context\n",
        "\n",
        "    decoder_output, decoder_state = self.decoder_rnn(tgt_embedded, prev_decoder_state)\n",
        "\n",
        "    # Calculate and update attention scores and context\n",
        "    attn_scores, context = attention(decoder_output, memory_bank, memory_bank, mask=src_mask.unsqueeze(1))\n",
        "    attn = attn_scores.squeeze(1)  # Update attn with the calculated scores\n",
        "\n",
        "    concat_context = torch.cat([decoder_output.squeeze(1), context.squeeze(1)], dim=-1)\n",
        "\n",
        "    logits = self.hidden2output(concat_context)\n",
        "\n",
        "    if normalize:\n",
        "      logits = torch.log_softmax(logits, dim=-1)\n",
        "\n",
        "\n",
        "    return logits, decoder_states, attn\n",
        "\n",
        "  def evaluate_ppl(self, iterator):\n",
        "    \"\"\"Returns the model's perplexity on a given dataset `iterator`.\"\"\"\n",
        "    # Switch to eval mode\n",
        "    self.eval()\n",
        "    total_loss = 0\n",
        "    total_words = 0\n",
        "    for batch in iterator:\n",
        "      # Input and target\n",
        "      src = batch['src_ids']              # bsz, max_src_len\n",
        "      src_lengths = batch['src_lengths']  # bsz\n",
        "      tgt_in = batch['tgt_ids'][:, :-1] # Remove <eos> for decode input (y_0=<bos>, y_1, y_2)\n",
        "      tgt_out = batch['tgt_ids'][:, 1:] # Remove <bos> as target        (y_1, y_2, y_3=<eos>)\n",
        "      # Forward to get logits\n",
        "      logits = self.forward(src, src_lengths, tgt_in) # bsz, tgt_len, V_tgt\n",
        "      # Compute cross entropy loss\n",
        "      loss = self.loss_function(logits.reshape(-1, self.V_tgt), tgt_out.reshape(-1))\n",
        "      total_loss += loss.item()\n",
        "      total_words += tgt_out.ne(self.padding_id_tgt).float().sum().item()\n",
        "    return math.exp(total_loss/total_words)\n",
        "\n",
        "  def train_all(self, train_iter, val_iter, epochs=10, learning_rate=0.001):\n",
        "    \"\"\"Train the model.\"\"\"\n",
        "    # Switch the module to training mode\n",
        "    self.train()\n",
        "    # Use Adam to optimize the parameters\n",
        "    optim = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
        "    best_validation_ppl = float('inf')\n",
        "    best_model = None\n",
        "    # Run the optimization for multiple epochs\n",
        "    for epoch in range(epochs):\n",
        "      total_words = 0\n",
        "      total_loss = 0.0\n",
        "      for batch in tqdm(train_iter):\n",
        "        # Zero the parameter gradients\n",
        "        self.zero_grad()\n",
        "        # Input and target\n",
        "        tgt = batch['tgt_ids']              # bsz, max_tgt_len\n",
        "        src = batch['src_ids']              # bsz, max_src_len\n",
        "        src_lengths = batch['src_lengths']  # bsz\n",
        "        tgt_in = tgt[:, :-1].contiguous() # Remove <eos> for decode input (y_0=<bos>, y_1, y_2)\n",
        "        tgt_out = tgt[:, 1:].contiguous() # Remove <bos> as target        (y_1, y_2, y_3=<eos>)\n",
        "        bsz = tgt.size(0)\n",
        "        # Run forward pass and compute loss along the way.\n",
        "        logits = self.forward(src, src_lengths, tgt_in)\n",
        "        loss = self.loss_function(logits.view(-1, self.V_tgt), tgt_out.view(-1))\n",
        "        # Training stats\n",
        "        num_tgt_words = tgt_out.ne(self.padding_id_tgt).float().sum().item()\n",
        "        total_words += num_tgt_words\n",
        "        total_loss += loss.item()\n",
        "        # Perform backpropagation\n",
        "        loss.div(bsz).backward()\n",
        "        optim.step()\n",
        "\n",
        "      # Evaluate and track improvements on the validation dataset\n",
        "      validation_ppl = self.evaluate_ppl(val_iter)\n",
        "      self.train()\n",
        "      if validation_ppl < best_validation_ppl:\n",
        "        best_validation_ppl = validation_ppl\n",
        "        self.best_model = copy.deepcopy(self.state_dict())\n",
        "      epoch_loss = total_loss / total_words\n",
        "      print (f'Epoch: {epoch} Training Perplexity: {math.exp(epoch_loss):.4f} '\n",
        "             f'Validation Perplexity: {validation_ppl:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "id": "3691f83b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-03T08:14:26.781313Z",
          "iopub.status.busy": "2025-02-03T08:14:26.781107Z",
          "iopub.status.idle": "2025-02-03T08:17:55.234795Z",
          "shell.execute_reply": "2025-02-03T08:17:55.233934Z"
        },
        "id": "3691f83b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0fdff67-b312-47f6-c20a-3707329a0a40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2032/2032 [04:49<00:00,  7.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Training Perplexity: 3.0782 Validation Perplexity: 2.1350\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2032/2032 [04:44<00:00,  7.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 Training Perplexity: 1.8141 Validation Perplexity: 1.5896\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 176
        }
      ],
      "source": [
        "EPOCHS = 2 # epochs, we highly recommend starting with a smaller number like 1\n",
        "LEARNING_RATE = 2e-3 # learning rate\n",
        "\n",
        "# Instantiate and train classifier\n",
        "model = AttnEncoderDecoder(hf_src_tokenizer, hf_tgt_tokenizer,\n",
        "  hidden_size    = 64,\n",
        "  layers         = 3,\n",
        ").to(device)\n",
        "\n",
        "model.train_all(train_iter, val_iter, epochs=EPOCHS, learning_rate=LEARNING_RATE)\n",
        "model.load_state_dict(model.best_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b451b52",
      "metadata": {
        "id": "5b451b52"
      },
      "source": [
        "Since the task we consider here is very simple, we should expect a perplexity very close to 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "id": "f41a0310",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-03T08:17:55.238062Z",
          "iopub.status.busy": "2025-02-03T08:17:55.237671Z",
          "iopub.status.idle": "2025-02-03T08:18:00.667698Z",
          "shell.execute_reply": "2025-02-03T08:18:00.666789Z"
        },
        "id": "f41a0310",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "523eadd1-34ca-4ed1-f1ce-c249de595c87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test perplexity: 1.578\n"
          ]
        }
      ],
      "source": [
        "# Evaluate model performance, the expected value should be < 1.05\n",
        "print (f'Test perplexity: {model.evaluate_ppl(test_iter):.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "id": "459bc254",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "459bc254",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "af5512cb-89af-4985-94eb-5993675b7d5a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 178
        }
      ],
      "source": [
        "grader.check(\"encoder_decoder_ppl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c584f21f",
      "metadata": {
        "id": "c584f21f"
      },
      "source": [
        "## Beam search decoding\n",
        "\n",
        "We can reuse most of our beam search code in lab 4-3 here: we only need to modify the code a bit to pass in `memory_bank` and `src_mask`. For reference here is the same pseudo-code used in lab 4-3, where we want to decode a single example `x` of maximum length `max_T` using a beam size of `K`.\n",
        "\n",
        "```\n",
        " 1.  def beam_search(x, K, max_T):\n",
        " 2.      finished = []       # for storing completed hypotheses\n",
        "         # Initialize the beam\n",
        " 3.      beams = [Beam(hyp=(bos), score=0)] # initial hypothesis: bos, initial score: 0\n",
        "\n",
        " 4.      for t in [1..max_T]  # main body of search over time steps           \n",
        " 5.          hypotheses = []\n",
        "\n",
        "             # Expand each beam by all possible tokens y_{t+1}\n",
        " 6.          for beam in beams:\n",
        " 7.              y_{1:t}, score = beam.hyp, beam.score\n",
        " 8.              for y_{t+1} in V:\n",
        " 9.                  y_{1:t+1} = y_{1:t} + [y_{t+1}]\n",
        " 10.                 new_score = score + log P(y_{t+1} | y_{1:t}, x)\n",
        " 11.                 hypotheses.append(Beam(hyp=y_{1:t+1}, score=new_score))\n",
        "\n",
        "             # Find K best next beams\n",
        " 12.         beams = sorted(hypotheses, key=lambda beam: -beam.score)[:K]\n",
        "\n",
        "             # Set aside finished beams (those that end in <eos>)\n",
        " 13.         for beam in beams:\n",
        " 14.             y_{t+1} = beam.hyp[-1]\n",
        " 15.             if y_{t+1} == eos:\n",
        " 16.                 finished.append(beam)\n",
        " 17.                 beams.remove(beam)\n",
        "\n",
        "             # Break the loop if everything is finished\n",
        " 18.         if len(beams) == 0:\n",
        " 19.             break              \n",
        " 20.     return sorted(finished, key=lambda beam: -beam.score)[0] # return the best finished hypothesis\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80775362",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "80775362"
      },
      "source": [
        "Implement function `beam_search` in the code below. In addition to the predicted target sequence, this function also returns a list of attentions `all_attns`.\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: beam_search\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "id": "881ad63e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-03T08:18:11.516363Z",
          "iopub.status.busy": "2025-02-03T08:18:11.516130Z",
          "iopub.status.idle": "2025-02-03T08:18:11.529040Z",
          "shell.execute_reply": "2025-02-03T08:18:11.528194Z"
        },
        "id": "881ad63e"
      },
      "outputs": [],
      "source": [
        "MAX_T = 15\n",
        "class Beam():\n",
        "  \"\"\"\n",
        "  Helper class for storing a hypothesis, its score and its decoder hidden state.\n",
        "  \"\"\"\n",
        "  def __init__(self, decoder_state, tokens, score):\n",
        "    self.decoder_state = decoder_state\n",
        "    self.tokens = tokens\n",
        "    self.score = score\n",
        "\n",
        "class BeamSearcher():\n",
        "  \"\"\"\n",
        "  Main class for beam search.\n",
        "  \"\"\"\n",
        "  def __init__(self, model):\n",
        "    self.model = model\n",
        "    self.bos_id = model.bos_id\n",
        "    self.eos_id = model.eos_id\n",
        "    self.padding_id_src = model.padding_id_src\n",
        "    self.V = model.V_tgt\n",
        "\n",
        "\n",
        "  def beam_search(self, src, src_lengths, K, max_T=MAX_T):\n",
        "    \"\"\"\n",
        "    Performs beam search decoding.\n",
        "    Arguments:\n",
        "        src: src batch of size (1, max_src_len)\n",
        "        src_lengths: src lengths of size (1)\n",
        "        K: beam size\n",
        "        max_T: max possible target length considered\n",
        "    Returns:\n",
        "        a list of token ids and a list of attentions\n",
        "    \"\"\"\n",
        "    finished = []\n",
        "    all_attns = []\n",
        "\n",
        "    self.model.eval()\n",
        "\n",
        "    memory_bank, encoder_final_state = self.model.forward_encoder(src, src_lengths)\n",
        "    src_mask = src.ne(self.padding_id_src)\n",
        "\n",
        "    init_decoder_state = encoder_final_state\n",
        "    init_beam = Beam(decoder_state=init_decoder_state, tokens=[self.bos_id], score=0)\n",
        "    beams = [init_beam]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for t in range(max_T):\n",
        "            all_total_scores = []\n",
        "\n",
        "            for beam in beams:\n",
        "                y_1_to_t, score, decoder_state = beam.tokens, beam.score, beam.decoder_state\n",
        "                y_t = y_1_to_t[-1]\n",
        "\n",
        "                logits, decoder_state, attn = self.model.forward_decoder_incrementally(\n",
        "                    prev_decoder_states=decoder_state,\n",
        "                    tgt_in_onestep=torch.tensor([y_t], device=src.device),\n",
        "                    memory_bank=memory_bank,\n",
        "                    src_mask=src_mask,\n",
        "                    normalize=True\n",
        "                )\n",
        "\n",
        "                total_scores = score + logits.squeeze(0)  # (V,)\n",
        "                all_total_scores.append(total_scores)\n",
        "                all_attns.append(attn)\n",
        "\n",
        "                beam.decoder_state = decoder_state\n",
        "\n",
        "            all_total_scores = torch.stack(all_total_scores)\n",
        "\n",
        "            all_scores_flattened = all_total_scores.view(-1)\n",
        "            topk_scores, topk_ids = all_scores_flattened.topk(K, 0)\n",
        "            beam_ids = topk_ids.div(self.V, rounding_mode='floor')\n",
        "            next_tokens = topk_ids - beam_ids * self.V\n",
        "\n",
        "            new_beams = []\n",
        "            for k in range(K):\n",
        "                beam_id = beam_ids[k].item()\n",
        "                y_t_plus_1 = next_tokens[k].item()\n",
        "                score = topk_scores[k].item()\n",
        "                decoder_state = beams[beam_id].decoder_state\n",
        "                y_1_to_t = beams[beam_id].tokens\n",
        "\n",
        "                new_beam = Beam(decoder_state=decoder_state, tokens=y_1_to_t + [y_t_plus_1], score=score)\n",
        "\n",
        "                if y_t_plus_1 == self.eos_id:\n",
        "                    finished.append(new_beam)\n",
        "                else:\n",
        "                    new_beams.append(new_beam)\n",
        "\n",
        "            beams = new_beams  # Only keep non-finished beams\n",
        "\n",
        "            if len(beams) == 0:\n",
        "                break\n",
        "\n",
        "    # Return the best hypothesis\n",
        "    if len(finished) > 0:\n",
        "        finished = sorted(finished, key=lambda beam: -beam.score)\n",
        "        return finished[0].tokens, all_attns\n",
        "    elif len(beams) > 0:  # If nothing finished, return the highest scoring unfinished beam\n",
        "        return beams[0].tokens, all_attns\n",
        "    else:  # In rare cases where no valid beams remain\n",
        "        return [self.bos_id], all_attns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bIVCihKyPlgr"
      },
      "id": "bIVCihKyPlgr"
    },
    {
      "cell_type": "markdown",
      "id": "4c835a0c",
      "metadata": {
        "id": "4c835a0c"
      },
      "source": [
        "Now we can use beam search decoding to predict the outputs for the test set inputs using the trained model. You should expect an accuracy close to 100%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "id": "2de2ca45",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "2de2ca45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "2cf70ae1-7a82-469d-efcd-57df06f31466"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 206
        }
      ],
      "source": [
        "grader.check(\"beam_search\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "id": "5b15a437",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-03T08:18:46.452625Z",
          "iopub.status.busy": "2025-02-03T08:18:46.452247Z",
          "iopub.status.idle": "2025-02-03T08:18:51.888643Z",
          "shell.execute_reply": "2025-02-03T08:18:51.887735Z"
        },
        "id": "5b15a437",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de1c7249-757a-4f31-8201-3a926f41754b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source: sixteen thousand eight hundred and thirty two\n",
            "Prediction:   1 6 8 3 2\n",
            "Ground truth: 1 6 8 3 2\n",
            "Source: sixty seven million six hundred and eighty five thousand two hundred and thirty\n",
            "Prediction:   6 8 5 2 3 0\n",
            "Ground truth: 6 7 6 8 5 2 3 0\n",
            "Source: six thousand two hundred and twelve\n",
            "Prediction:   6 2\n",
            "Ground truth: 6 2 1 2\n",
            "Source: seven hundred and ninety eight million three hundred and thirty one thousand eight hundred and eighteen\n",
            "Prediction:   7 9 8\n",
            "Ground truth: 7 9 8 3 3 1 8 1 8\n",
            "Source: eighty eight million four hundred and thirteen thousand nine hundred and eighteen\n",
            "Prediction:   8\n",
            "Ground truth: 8 8 4 1 3 9 1 8\n",
            "Source: three hundred and seventy four thousand two hundred and seventy\n",
            "Prediction:   3 7 4 2 7 4 2 7 4 2 7 4 2 7 4\n",
            "Ground truth: 3 7 4 2 7 0\n",
            "Source: ninety eight million three hundred and seventy thousand five hundred and forty five\n",
            "Prediction:   9 8 3 7 0 5 4 5 4 5 4 5 4 5 4\n",
            "Ground truth: 9 8 3 7 0 5 4 5\n",
            "Source: ninety seven thousand seven hundred and sixty two\n",
            "Prediction:   9 7 6 2\n",
            "Ground truth: 9 7 7 6 2\n",
            "Source: four hundred and ten thousand two hundred and three\n",
            "Prediction:   4 1 0 3\n",
            "Ground truth: 4 1 0 2 0 3\n",
            "Accuracy: 0.16\n"
          ]
        }
      ],
      "source": [
        "DEBUG_FIRST = 10  # set to 0 to disable printing predictions\n",
        "K = 1             # beam size 1\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# create beam searcher\n",
        "beam_searcher = BeamSearcher(model)\n",
        "\n",
        "for index, batch in enumerate(test_iter, start=1):\n",
        "  # Input and output\n",
        "  src = batch['src_ids']\n",
        "  src_lengths = batch['src_lengths']\n",
        "  # Predict\n",
        "  prediction, _ = beam_searcher.beam_search(src, src_lengths, K)\n",
        "  # Convert to string\n",
        "  prediction = hf_tgt_tokenizer.decode(prediction, skip_special_tokens=True)\n",
        "  ground_truth = hf_tgt_tokenizer.decode(batch['tgt_ids'][0], skip_special_tokens=True)\n",
        "  if DEBUG_FIRST > index:\n",
        "    src = hf_src_tokenizer.decode(src[0], skip_special_tokens=True)\n",
        "    print (f'Source: {src}')\n",
        "    print (f'Prediction:   {prediction}')\n",
        "    print (f'Ground truth: {ground_truth}')\n",
        "  if ground_truth == prediction:\n",
        "    correct += 1\n",
        "  total += 1\n",
        "\n",
        "print (f'Accuracy: {correct/total:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5b990ec",
      "metadata": {
        "id": "d5b990ec"
      },
      "source": [
        "# Visualizing attention\n",
        "\n",
        "We can visualize how each query distributes its attention scores over each source word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "id": "4a2d7cde",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-03T08:18:51.891698Z",
          "iopub.status.busy": "2025-02-03T08:18:51.891463Z",
          "iopub.status.idle": "2025-02-03T08:18:52.050379Z",
          "shell.execute_reply": "2025-02-03T08:18:52.049698Z"
        },
        "id": "4a2d7cde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577
        },
        "outputId": "9a21f037-ae2e-4502-c831-53b9cd43f75a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source: sixteen thousand eight hundred and thirty two\n",
            "Prediction:   1 6 8 3 2\n",
            "Ground truth: 1 6 8 3 2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "K = 1 # this code only works for beam size 1\n",
        "\n",
        "# Create beam searcher\n",
        "beam_searcher = BeamSearcher(model)\n",
        "batch = next(iter(test_iter))\n",
        "# Input and output\n",
        "src = batch['src_ids']\n",
        "src_lengths = batch['src_lengths']\n",
        "# Predict and get attentions\n",
        "prediction, all_attns = beam_searcher.beam_search(src, src_lengths, K)\n",
        "all_attns = torch.stack(all_attns, 0)\n",
        "# Convert to string\n",
        "prediction = hf_tgt_tokenizer.decode(prediction, skip_special_tokens=True)\n",
        "ground_truth = hf_tgt_tokenizer.decode(batch['tgt_ids'][0], skip_special_tokens=True)\n",
        "src =  hf_src_tokenizer.decode(src[0], skip_special_tokens=True)\n",
        "print (f'Source: {src}')\n",
        "print (f'Prediction:   {prediction}')\n",
        "print (f'Ground truth: {ground_truth}')\n",
        "\n",
        "# Plot\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "ax.imshow(all_attns[:,0,:].detach().cpu())\n",
        "ax.set_yticks(list(range(1+len(prediction.split()))));\n",
        "ax.set_yticklabels(prediction.split() + ['eos']);\n",
        "ax.set_xticks(list(range(len(src.split()))));\n",
        "ax.set_xticklabels(src.split());\n",
        "\n",
        "# Uncomment the line below if the plot does not show up\n",
        "# Make sure to comment that before submitting to gradescope\n",
        "# since there would be some autograder issues with plt.show()\n",
        "#plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b42c1cc7",
      "metadata": {
        "id": "b42c1cc7"
      },
      "source": [
        "Do these attentions make sense? Do you see how the attention mechanism solves the bottleneck problem in vanilla seq2seq?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e081cedc",
      "metadata": {
        "id": "e081cedc"
      },
      "source": [
        "# The transformer architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26fda71d",
      "metadata": {
        "id": "26fda71d"
      },
      "source": [
        "In lab 2-5 you have used the transformer architecture in a decoder-only model. Here you will use it for both the encoder and decoder.\n",
        "\n",
        "As a reminder, in RNN-based neural encoder-decoder models, we used recurrence to model the dependencies among words. For example, by running a unidirectional RNN from $y_{1}$ to $y_{t}$, we can consider the past history when predicting $y_{t+1}$. However, running an RNN over a sequence is a serial process: we need to wait for it to finish running from $y_1$ to $y_t$ before being able to compute the outputs at $y_{t+1}$. This serial process cannot be parallelized on GPUs along the sequence length dimension: even during training where all $y_t$'s are available, we cannot compute the logits for $y_t$ and the logits for $y_{t+1}$ in parallel.\n",
        "\n",
        "The attention mechanism provides an alternative, and most importantly, parallelizable solution. [The transformer model](https://arxiv.org/abs/1706.03762) completely gets rid of recurrence and only uses attention to model the dependencies among words. For example, we can use attention to incorporate the representations from $y_1$ to $y_t$ when predicting $y_{t+1}$, simply by attending to their word embeddings. This is called _decoder self-attention_.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d018cac",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "2d018cac"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "**Question:** By getting rid of recurrence and only using decoder self-attention, can we compute the logits for any two different words $y_{t_1}$ and $y_{t_2}$ in parallel at training time (only consider decoder for now)? Why?\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: open_response_transformer_parallel\n",
        "manual: true\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e93bcf6",
      "metadata": {
        "id": "4e93bcf6"
      },
      "source": [
        "Yes, we can compute the logits for any two different words \\( y_{t1} \\) and \\( y_{t2} \\) in parallel at training time when using decoder self-attention.  \n",
        "\n",
        "In an RNN-based decoder, each token must be processed sequentially because the prediction of \\( y_{t+1} \\) depends on the hidden state from \\( y_t \\). This prevents parallelization, as we need to complete the computation for one step before moving to the next.  \n",
        "\n",
        "However, in a transformer decoder, self-attention allows the model to process all tokens at once. Since all target tokens are available during training, the model can attend to the entire sequence in a **single computation step**. This means the logits for \\( y_{t1} \\) and \\( y_{t2} \\) can be computed in parallel, significantly improving efficiency.  \n",
        "\n",
        "In conclusion, by replacing recurrence with self-attention, transformers eliminate the sequential bottleneck of RNNs, making training much faster and more scalable."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd4c8224",
      "metadata": {
        "id": "fd4c8224"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "\n",
        "\n",
        "Similarly, at the encoder side, for each word $x_i$, we let it attend to the embeddings of $x_1, \\ldots, x_S$, to model the context in which $x_i$ appears. This is called _encoder self-attention_. It is different from decoder self-attention in that here every word attends to all words, but at the decoder side, every word can only attend to the previous words (since the prediction of word $y_t$ cannot use the information from any $y_{\\ge t}$).\n",
        "\n",
        "To incorporate source-side information at the decoder side, at each time step, we let the decoder attend to the top-layer encoder outputs, as we did in the RNN-based encoder-decoder model above. This is called _cross-attention_. Note that there's no initialization of decoder hidden state here, since we no longer use an RNN.\n",
        "\n",
        "The process we describe above is only a single layer of attention. In practice, transformers stack multiple layers of attention and feedforward layers, using the outputs from the layer below as the inputs to the layer above, as shown in the illustration below.\n",
        "\n",
        "<img src=\"https://github.com/nlp-course/data/raw/master/img/transformer.png\" alt=\"transformer illustration\" />\n",
        "\n",
        "In the above illustration, due to space limits, we omitted the details of encoder self-attention and decoder self-attention, and we describe it here, using encoder-self-attention at layer 0 as an example. First, we use three linear projections to project each hidden state $h_{0,i}$ to a query vector $q_{0,i}$, a key vector $k_{0,i}$, and a value vector $v_{0,i}$. Then at each position $i$, we use $q_i$ as the query, and $\\{(k_{0,j}, v_{0,j}): j \\in \\{1, \\ldots, S\\}\\}$ as keys/values to produce a context vector $c_{0,i}$. Note that the keys/values are the same for different positions, and the only difference is that a different query vector is used for each position.\n",
        "\n",
        "A clear difference between the transformer architecture and the RNN-based encoder decoder architecture is that there are no horizontal arrows in the transformer model: transformers only use position-wise operations and attention operations. The dependencies among words are **only introduced by the attention operations**, while the other operations such as feedforward, nonlinearity, and normalization are position-wise, that is, they do not depend on other positions, and can thus be performed in parallel."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7c838f7",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "b7c838f7"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "**Question:** In the above transformer model, if we shuffle the input words $x_1, \\ldots, x_4$, would we get a different distribution over $y$? Why or why not?\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: open_response_transformer_shuffle\n",
        "manual: true\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c264c288",
      "metadata": {
        "id": "c264c288"
      },
      "source": [
        "_Type your answer here, replacing this text._"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4e0eaf1",
      "metadata": {
        "id": "b4e0eaf1"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "\n",
        "\n",
        "Since the transformer model itself doesn't have any sense of position or order, we encode the position of each word in the sentence, and add it to the word embedding as part of the input representation, as illustrated below.\n",
        "\n",
        "<img src=\"https://github.com/nlp-course/data/raw/master/img/transformer_pos.png\" alt=\"transformer w/ positional encoding illustration\" />\n",
        "\n",
        "> The illustrations above also omitted residual connections, which add the inputs to certain operations (such as attention and feedforward) to the outputs. More details can be found in the code below."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2e5a9ba",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "a2e5a9ba"
      },
      "source": [
        "## Causal attention mask\n",
        "\n",
        "To efficiently train the transformer model, we want to batch the attention operations together such that they can be fully parallelized along the sequence length dimension. (The non-attention operations are position-wise so they are trivally parallelizable.) This is quite straightforward for encoder self-attention and decoder-encoder cross-attention given our batched implementation of the `attention` function. However, things are a bit trickier for the decoder: each word $y_t$ attends to $t-1$ previous words $y_1, \\ldots, y_{t-1}$, which means each word $y_t$ has a different set of key-value pairs. Is it possible to batch them together?\n",
        "\n",
        "The solution is to use *attention masks*. For every word $y_t$, we give it all key-value pairs at $y_1, \\ldots, y_T$, and we disallow attending to future words $y_{t}, y_{t+1},\\ldots, y_T$ through an attention mask. (Recall that the `attention` function takes a `mask` argument.) We usually call this attention mask a _causal attention mask_, as it prevents the leakage of information from the future into the past. Since every $y_t$ has the same set of (key, value) pairs, we can batch them and compute the context vectors using a single call to the function `attention`.\n",
        "\n",
        "What should such a mask be? Implement the `causal_mask` function below to generate this mask.\n",
        "\n",
        "> Hint: you might find [`torch.triu`](https://pytorch.org/docs/stable/generated/torch.triu.html) or [`torch.tril`](https://pytorch.org/docs/stable/generated/torch.tril.html) useful.\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: causal_attention_mask\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "id": "c75195db",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-03T08:18:52.056280Z",
          "iopub.status.busy": "2025-02-03T08:18:52.054614Z",
          "iopub.status.idle": "2025-02-03T08:18:52.061150Z",
          "shell.execute_reply": "2025-02-03T08:18:52.060532Z"
        },
        "id": "c75195db"
      },
      "outputs": [],
      "source": [
        "#TODO - implement this function, which returns a causal attention mask\n",
        "def causal_mask(T):\n",
        "  \"\"\"\n",
        "  Generate a causal mask.\n",
        "  Arguments:\n",
        "      T: the length of target sequence\n",
        "  Returns:\n",
        "      mask: a T x T tensor, where `mask[i, j]` should be `True`\n",
        "      if y_i can attend to y_{j-1} (there's a \"-1\" since the first\n",
        "      token in decoder input is <bos>) and `False` if y_i cannot\n",
        "      attend to y_{j-1}\n",
        "  \"\"\"\n",
        "  mask = torch.tril(torch.ones(T, T, dtype=torch.bool))\n",
        "  return mask.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "id": "e6e31cff",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "e6e31cff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "4923dda3-47bd-46c0-c478-f860fcc4f5c6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 211
        }
      ],
      "source": [
        "grader.check(\"causal_attention_mask\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d73c932",
      "metadata": {
        "id": "5d73c932"
      },
      "source": [
        "We can visualize the attention mask and manually check if it's what we expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "id": "8e243146",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-03T08:18:52.124289Z",
          "iopub.status.busy": "2025-02-03T08:18:52.124087Z",
          "iopub.status.idle": "2025-02-03T08:18:52.305784Z",
          "shell.execute_reply": "2025-02-03T08:18:52.304880Z"
        },
        "id": "8e243146",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "outputId": "56dd6b93-777b-458e-8148-b9daae5b374f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7a81bef4b9d0>"
            ]
          },
          "metadata": {},
          "execution_count": 212
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "T = 7\n",
        "mask = causal_mask(T)\n",
        "ax.imshow(mask.cpu())\n",
        "\n",
        "# Uncomment the line below if the plot does not show up\n",
        "# Make sure to comment that before submitting to gradescope\n",
        "# since there would be some autograder issues with `plt.show()`\n",
        "#plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fe96668",
      "metadata": {
        "id": "2fe96668"
      },
      "source": [
        "As we have emphasized multiple times, unlike RNN-based encoder-decoders, transformer encoder/decoders are parallelizable in the sequence length dimension, even for the decoder: by using causal masks, all positions (at the same layer) can be computed all at once (once the lower layer has been computed). The parallelizability of transformers is the key to its success, since it allows for training it on vast amounts of data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "beaddb93",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "beaddb93"
      },
      "source": [
        "Now we are ready to complete the implementation of the transformer encoder-decoder model. The code is structured as a set of classes: `TransformerEncoderLayer`\\*, `TransformerEncoder`, `TransformDecoderLayer`\\*, `TransformDecoder`, `PositionalEmbedding`, and `TransformerEncoderDecoder`\\*. We've provided almost all the necessary code. In particular, we provide code for all position-wise operations. Your job is only to implement the parts involving attention and to figure out the correct attention masks, which involves only the three classes marked above with a star.\n",
        "\n",
        "> Hint: Completing this transformer implementation should require very little code, just a few lines.\n",
        "\n",
        "> Hint: The causal mask is a 2-D matrix, but we want to add a batch dimension, and expand it to be of the desired size. For this purpose, you can use [`torch.repeat`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.repeat).\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: transformer\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "id": "eb483566",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-03T08:18:52.308990Z",
          "iopub.status.busy": "2025-02-03T08:18:52.308724Z",
          "iopub.status.idle": "2025-02-03T08:18:52.322929Z",
          "shell.execute_reply": "2025-02-03T08:18:52.322160Z"
        },
        "id": "eb483566"
      },
      "outputs": [],
      "source": [
        "#TODO - implement `forward_encoder` and `forward_decoder`.\n",
        "# `TransformerEncoderDecoder` inherits most functions from `AttnEncoderDecoder`\n",
        "class TransformerEncoderDecoder(AttnEncoderDecoder):\n",
        "  def __init__(self, hf_src_tokenizer, hf_tgt_tokenizer, hidden_size=64, layers=3):\n",
        "    \"\"\"\n",
        "    Initializer. Creates network modules and loss function.\n",
        "    Arguments:\n",
        "        hf_src_tokenizer: hf src tokenizer\n",
        "        hf_tgt_tokenizer: hf tgt tokenizer\n",
        "        hidden_size: hidden layer size of both encoder and decoder\n",
        "        layers: number of layers of both encoder and decoder\n",
        "    \"\"\"\n",
        "    super(AttnEncoderDecoder, self).__init__()\n",
        "    self.hf_src_tokenizer = hf_src_tokenizer\n",
        "    self.hf_tgt_tokenizer = hf_tgt_tokenizer\n",
        "\n",
        "    # Keep the vocabulary sizes available\n",
        "    self.V_src = len(self.hf_src_tokenizer)\n",
        "    self.V_tgt = len(self.hf_tgt_tokenizer)\n",
        "\n",
        "    # Get special word ids or tokens\n",
        "    self.padding_id_src = self.hf_src_tokenizer.pad_token_id\n",
        "    self.padding_id_tgt = self.hf_tgt_tokenizer.pad_token_id\n",
        "    self.bos_id = self.hf_tgt_tokenizer.bos_token_id\n",
        "    self.eos_id = self.hf_tgt_tokenizer.eos_token_id\n",
        "\n",
        "    # Keep hyper-parameters available\n",
        "    self.embedding_size = hidden_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.layers = layers\n",
        "\n",
        "    # Create essential modules\n",
        "    self.encoder = TransformerEncoder(self.V_src, hidden_size, layers)\n",
        "    self.decoder = TransformerDecoder(self.V_tgt, hidden_size, layers)\n",
        "\n",
        "    # Final projection layer\n",
        "    self.hidden2output = nn.Linear(hidden_size, self.V_tgt)\n",
        "\n",
        "    # Create loss function\n",
        "    self.loss_function = nn.CrossEntropyLoss(reduction='sum',\n",
        "                                             ignore_index=self.padding_id_tgt)\n",
        "\n",
        "  def forward_encoder(self, src, src_lengths):\n",
        "    \"\"\"\n",
        "    Encodes source words `src`.\n",
        "    Arguments:\n",
        "        src: src batch of size (bsz, max_src_len)\n",
        "        src_lengths: src lengths (bsz)\n",
        "    Returns:\n",
        "        memory_bank: a tensor of size (bsz, src_len, hidden_size)\n",
        "    \"\"\"\n",
        "    # The reason we don't directly pass in src_mask as in `forward_decoder` is to\n",
        "    # enable us to reuse beam search implemented for RNN-based encoder-decoder\n",
        "    src_len = src.size(1)\n",
        "    #TODO - compute `encoder_self_attn_mask`\n",
        "    encoder_self_attn_mask = src.ne(self.padding_id_src).unsqueeze(1).repeat(1, src_len, 1)\n",
        "    memory_bank = self.encoder(src, encoder_self_attn_mask)\n",
        "    return memory_bank, None\n",
        "\n",
        "  def forward_decoder(self, tgt_in, memory_bank, src_mask):\n",
        "    \"\"\"\n",
        "    Decodes based on memory bank, and ground truth target words.\n",
        "    Arguments:\n",
        "        tgt_in: a tensor of size (bsz, tgt_len)\n",
        "        memory_bank: a tensor of size (bsz, src_len, hidden_size), encoder outputs\n",
        "                     at every position\n",
        "        src_mask: a tensor of size (bsz, src_len) which is `False` for source paddings\n",
        "    Returns:\n",
        "        Logits of size (bsz, tgt_len, V_tgt) (before the softmax operation)\n",
        "    \"\"\"\n",
        "    tgt_len = tgt_in.size(1)\n",
        "    bsz = tgt_in.size(0)\n",
        "    #TODO - compute `cross_attn_mask` and `decoder_self_attn_mask`\n",
        "    cross_attn_mask = src_mask.unsqueeze(1).repeat(1, tgt_len, 1)\n",
        "    decoder_self_attn_mask = causal_mask(tgt_len).unsqueeze(0).repeat(bsz, 1, 1)\n",
        "\n",
        "    outputs = self.decoder(tgt_in, memory_bank, cross_attn_mask, decoder_self_attn_mask)\n",
        "    logits = self.hidden2output(outputs)\n",
        "    return logits\n",
        "\n",
        "  def forward(self, src, src_lengths, tgt_in):\n",
        "    \"\"\"\n",
        "    Performs forward computation, returns logits.\n",
        "    Arguments:\n",
        "        src: src batch of size (bsz, max_src_len)\n",
        "        src_lengths: src lengths of size (bsz)\n",
        "        tgt_in:  a tensor of size (bsz, tgt_len)\n",
        "    \"\"\"\n",
        "    src_mask = src.ne(self.padding_id_src) # bsz, max_src_len\n",
        "    # Forward encoder\n",
        "    memory_bank, _ = self.forward_encoder(src, src_lengths)\n",
        "    # Forward decoder\n",
        "    logits = self.forward_decoder(tgt_in, memory_bank, src_mask)\n",
        "    return logits\n",
        "\n",
        "  def forward_decoder_incrementally(self, prev_decoder_states, tgt_in_onestep,\n",
        "                                    memory_bank, src_mask, normalize=True):\n",
        "    \"\"\"\n",
        "    Forward the decoder at `decoder_state` for a single step with token `tgt_in_onestep`.\n",
        "    This function will be used in beam search. Note that the implementation here is\n",
        "    very inefficient, since we do not cache any decoder state, but instead we only\n",
        "    cache previously generated tokens in `prev_decoder_states`, and do a fresh\n",
        "    `forward_decoder`.\n",
        "    Arguments:\n",
        "        prev_decoder_states: previous tgt words. None for the first step.\n",
        "        tgt_in_onestep: a tensor of size (bsz), tokens at one step\n",
        "        memory_bank: a tensor of size (bsz, src_len, hidden_size), src hidden states\n",
        "                     at every position\n",
        "        src_mask: a tensor of size (bsz, src_len): a boolean tensor, `False` where\n",
        "                  src is padding.\n",
        "        normalize: use log_softmax to normalize or not. Beam search needs to normalize,\n",
        "                   while `forward_decoder` does not\n",
        "    Returns:\n",
        "        logits: Log probabilities for `tgt_in_token` of size (bsz, V_tgt)\n",
        "        decoder_states: we use tgt words up to now as states, a tensor of size (bsz, len)\n",
        "        None: to keep output format the same as AttnEncoderDecoder, such that we can\n",
        "              reuse beam search code\n",
        "\n",
        "    \"\"\"\n",
        "    prev_tgt_in = prev_decoder_states # bsz, tgt_len\n",
        "    src_len = memory_bank.size(1)\n",
        "    bsz = memory_bank.size(0)\n",
        "    tgt_in_onestep = tgt_in_onestep.view(-1, 1) # bsz, 1\n",
        "    if prev_tgt_in is not None:\n",
        "      tgt_in = torch.cat((prev_tgt_in, tgt_in_onestep), 1) # bsz, tgt_len+1\n",
        "    else:\n",
        "      tgt_in = tgt_in_onestep\n",
        "    tgt_len = tgt_in.size(1)\n",
        "\n",
        "    logits = self.forward_decoder(tgt_in, memory_bank, src_mask)\n",
        "    logits = logits[:, -1]\n",
        "    if normalize:\n",
        "      logits = torch.log_softmax(logits, dim=-1)\n",
        "    decoder_states = tgt_in\n",
        "    return logits, decoder_states, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "id": "d588138f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-03T08:18:52.326045Z",
          "iopub.status.busy": "2025-02-03T08:18:52.325656Z",
          "iopub.status.idle": "2025-02-03T08:18:52.356248Z",
          "shell.execute_reply": "2025-02-03T08:18:52.355450Z"
        },
        "id": "d588138f"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "  r\"\"\"TransformerEncoder is an embedding layer and a stack of N encoder layers.\n",
        "  Arguments:\n",
        "      hidden_size: hidden size.\n",
        "      layers: the number of encoder layers.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, vocab_size, hidden_size, layers):\n",
        "    super().__init__()\n",
        "    self.embed = PositionalEmbedding(vocab_size, hidden_size)\n",
        "    encoder_layer = TransformerEncoderLayer(hidden_size)\n",
        "    self.layers = _get_clones(encoder_layer, layers)\n",
        "    self.norm = nn.LayerNorm(hidden_size)\n",
        "\n",
        "  def forward(self, src, encoder_self_attn_mask):\n",
        "    r\"\"\"Pass the input through the word embedding layer, followed by\n",
        "    the encoder layers in turn.\n",
        "    Arguments:\n",
        "        src: src batch of size (bsz, max_src_len)\n",
        "        encoder_self_attn_mask: the mask for encoder self-attention, it's of size\n",
        "                                (bsz, max_src_len, max_src_len)\n",
        "    Returns:\n",
        "        a tensor of size (bsz, max_src_len, hidden_size)\n",
        "    \"\"\"\n",
        "    output = self.embed(src)\n",
        "    for mod in self.layers:\n",
        "      output = mod(output, encoder_self_attn_mask=encoder_self_attn_mask)\n",
        "    output = self.norm(output)\n",
        "    return output\n",
        "\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "  r\"\"\"TransformerEncoderLayer is made up of self-attn and feedforward network.\n",
        "  Arguments:\n",
        "      hidden_size: hidden size.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, hidden_size):\n",
        "    super(TransformerEncoderLayer, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    fwd_hidden_size = hidden_size * 4\n",
        "\n",
        "    # Create modules\n",
        "    self.linear1 = nn.Linear(hidden_size, fwd_hidden_size)\n",
        "    self.linear2 = nn.Linear(fwd_hidden_size, hidden_size)\n",
        "    self.norm1 = nn.LayerNorm(hidden_size)\n",
        "    self.norm2 = nn.LayerNorm(hidden_size)\n",
        "    self.activation = nn.ReLU()\n",
        "    # Attention related\n",
        "    self.q_proj = nn.Linear(hidden_size, hidden_size)\n",
        "    self.k_proj = nn.Linear(hidden_size, hidden_size)\n",
        "    self.v_proj = nn.Linear(hidden_size, hidden_size)\n",
        "    self.context_proj = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "\n",
        "  def forward(self, src, encoder_self_attn_mask):\n",
        "    r\"\"\"Pass the input through the encoder layer.\n",
        "    Arguments:\n",
        "        src: an input tensor of size (bsz, max_src_len, hidden_size).\n",
        "        encoder_self_attn_mask: attention mask of size (bsz, max_src_len, max_src_len),\n",
        "                                it's `False` where the corresponding attention is disabled\n",
        "    Returns:\n",
        "        a tensor of size (bsz, max_src_len, hidden_size).\n",
        "    \"\"\"\n",
        "    # Attend\n",
        "    q = self.q_proj(src) / math.sqrt(self.hidden_size) # a trick needed to make transformer work\n",
        "    k = self.k_proj(src)\n",
        "    v = self.v_proj(src)\n",
        "    #TODO - compute `context`\n",
        "    attn_scores = torch.bmm(q, k.transpose(1, 2))  # (bsz, max_src_len, max_src_len)\n",
        "    attn_scores = attn_scores.masked_fill(~encoder_self_attn_mask, float('-inf'))\n",
        "    attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "    context = torch.bmm(attn_weights, v)  # (bsz, max_src_len, hidden_size)\n",
        "    src2 = self.context_proj(context)\n",
        "    # Residual connection\n",
        "    src = src + src2\n",
        "    src = self.norm1(src)\n",
        "    # Feedforward for each position\n",
        "    src2 = self.linear2(self.activation(self.linear1(src)))\n",
        "    src = src + src2\n",
        "    src = self.norm2(src)\n",
        "    return src\n",
        "\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "  r\"\"\"TransformerDecoder is an embedding layer and a stack of N decoder layers.\n",
        "  Arguments:\n",
        "      hidden_size: hidden size.\n",
        "      layers: the number of sub-encoder-layers in the encoder.\n",
        "  \"\"\"\n",
        "  def __init__(self, vocab_size, hidden_size, layers):\n",
        "    super(TransformerDecoder, self).__init__()\n",
        "    self.embed = PositionalEmbedding(vocab_size, hidden_size)\n",
        "    decoder_layer = TransformerDecoderLayer(hidden_size)\n",
        "    self.layers = _get_clones(decoder_layer, layers)\n",
        "    self.norm = nn.LayerNorm(hidden_size)\n",
        "\n",
        "  def forward(self, tgt_in, memory, cross_attn_mask, decoder_self_attn_mask):\n",
        "    r\"\"\"Pass the inputs (and mask) through the word embedding layer, followed by\n",
        "    the decoder layer in turn.\n",
        "    Arguments:\n",
        "        tgt_in: tgt batch of size (bsz, max_tgt_len)\n",
        "        memory: the outputs of the encoder (bsz, max_src_len, hidden_size)\n",
        "        cross_attn_mask: attention mask of size (bsz, max_tgt_len, max_src_len),\n",
        "                         it's `False` where the cross-attention is disallowed.\n",
        "        decoder_self_attn_mask: attention mask of size (bsz, max_tgt_len, max_tgt_len),\n",
        "                                it's `False` where the self-attention is disallowed.\n",
        "    Returns:\n",
        "        a tensor of size (bsz, max_tgt_len, hidden_size)\n",
        "    \"\"\"\n",
        "    output = self.embed(tgt_in)\n",
        "    for mod in self.layers:\n",
        "      output = mod(output, memory, cross_attn_mask=cross_attn_mask, \\\n",
        "                   decoder_self_attn_mask=decoder_self_attn_mask)\n",
        "\n",
        "    output = self.norm(output)\n",
        "    return output\n",
        "\n",
        "\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "  r\"\"\"TransformerDecoderLayer is made up of self-attn, cross-attn, and\n",
        "  feedforward network.\n",
        "  Arguments:\n",
        "      hidden_size: hidden size.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, hidden_size):\n",
        "    super(TransformerDecoderLayer, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    fwd_hidden_size = hidden_size * 4\n",
        "\n",
        "    # Create modules\n",
        "    self.linear1 = nn.Linear(hidden_size, fwd_hidden_size)\n",
        "    self.linear2 = nn.Linear(fwd_hidden_size, hidden_size)\n",
        "\n",
        "    self.activation = nn.ReLU()\n",
        "\n",
        "    self.norm1 = nn.LayerNorm(hidden_size)\n",
        "    self.norm2 = nn.LayerNorm(hidden_size)\n",
        "    self.norm3 = nn.LayerNorm(hidden_size)\n",
        "\n",
        "    # Attention related\n",
        "    self.q_proj_self = nn.Linear(hidden_size, hidden_size)\n",
        "    self.k_proj_self = nn.Linear(hidden_size, hidden_size)\n",
        "    self.v_proj_self = nn.Linear(hidden_size, hidden_size)\n",
        "    self.context_proj_self = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    self.q_proj_cross = nn.Linear(hidden_size, hidden_size)\n",
        "    self.k_proj_cross = nn.Linear(hidden_size, hidden_size)\n",
        "    self.v_proj_cross = nn.Linear(hidden_size, hidden_size)\n",
        "    self.context_proj_cross = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "  def forward(self, tgt, memory, cross_attn_mask, decoder_self_attn_mask):\n",
        "    r\"\"\"Pass the inputs (and mask) through the decoder layer.\n",
        "    Arguments:\n",
        "        tgt: an input tensor of size (bsz, max_tgt_len, hidden_size).\n",
        "        memory: encoder outputs of size (bsz, max_src_len, hidden_size).\n",
        "        cross_attn_mask: attention mask of size (bsz, max_tgt_len, max_src_len),\n",
        "                         it's `False` where the cross-attention is disallowed.\n",
        "        decoder_self_attn_mask: attention mask of size (bsz, max_tgt_len, max_tgt_len),\n",
        "                                it's `False` where the self-attention is disallowed.\n",
        "    Returns:\n",
        "        a tensor of size (bsz, max_tgt_len, hidden_size)\n",
        "    \"\"\"\n",
        "    # Self attention (decoder-side)\n",
        "    q = self.q_proj_self(tgt) / math.sqrt(self.hidden_size)\n",
        "    k = self.k_proj_self(tgt)\n",
        "    v = self.v_proj_self(tgt)\n",
        "    #TODO - compute `context`\n",
        "    attn_scores = torch.bmm(q, k.transpose(1, 2))\n",
        "    attn_scores = attn_scores.masked_fill(~decoder_self_attn_mask, float('-inf'))\n",
        "    attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "    context = torch.bmm(attn_weights, v)\n",
        "    tgt2 = self.context_proj_self(context)\n",
        "    tgt = tgt + tgt2\n",
        "    tgt = self.norm1(tgt)\n",
        "    # Cross attention (decoder attends to encoder)\n",
        "    q = self.q_proj_cross(tgt) / math.sqrt(self.hidden_size)\n",
        "    k = self.k_proj_cross(memory)\n",
        "    v = self.v_proj_cross(memory)\n",
        "    #TODO - compute `context`\n",
        "    attn_scores = torch.bmm(q, k.transpose(1, 2))  # (bsz, max_tgt_len, max_src_len)\n",
        "    attn_scores = attn_scores.masked_fill(~cross_attn_mask, float('-inf'))\n",
        "    attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "    context = torch.bmm(attn_weights, v)  # (bsz, max_tgt_len, hidden_size)\n",
        "\n",
        "    tgt2 = self.context_proj_cross(context)\n",
        "    tgt = tgt + tgt2\n",
        "    tgt = self.norm2(tgt)\n",
        "    tgt2 = self.linear2(self.activation(self.linear1(tgt)))\n",
        "    tgt = tgt + tgt2\n",
        "    tgt = self.norm3(tgt)\n",
        "    return tgt\n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "  \"\"\"\"Embeds a word both by its word id and by its position in the sentence.\"\"\"\n",
        "  def __init__(self, vocab_size, embedding_size, max_len=1024):\n",
        "    super(PositionalEmbedding, self).__init__()\n",
        "    self.embedding_size = embedding_size\n",
        "\n",
        "    self.embed = nn.Embedding(vocab_size, embedding_size)\n",
        "    pe = torch.zeros(max_len, embedding_size)\n",
        "    position = torch.arange(0, max_len).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, embedding_size, 2) *\n",
        "                         -(math.log(10000.0) / embedding_size))\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "    pe = pe.unsqueeze(0) # 1, max_len, embedding_size\n",
        "    self.register_buffer('pe', pe)\n",
        "\n",
        "  def forward(self, batch):\n",
        "    x = self.embed(batch) * math.sqrt(self.embedding_size) # type embedding\n",
        "    # Add positional encoding to type embedding\n",
        "    x = x + self.pe[:, :x.size(1)].detach()\n",
        "    return x\n",
        "\n",
        "\n",
        "def _get_clones(module, N):\n",
        "  \"\"\"Copies a module `N` times\"\"\"\n",
        "  return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "id": "df1fcc77",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-03T08:18:52.359127Z",
          "iopub.status.busy": "2025-02-03T08:18:52.358913Z",
          "iopub.status.idle": "2025-02-03T08:21:04.817453Z",
          "shell.execute_reply": "2025-02-03T08:21:04.816571Z"
        },
        "id": "df1fcc77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d5dae43-7b2e-44f2-d6a3-171d8f86261a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2032/2032 [02:51<00:00, 11.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Training Perplexity: 1.7230 Validation Perplexity: 1.1467\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2032/2032 [02:42<00:00, 12.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 Training Perplexity: 1.1286 Validation Perplexity: 1.0453\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 215
        }
      ],
      "source": [
        "EPOCHS = 2 # epochs, we highly recommend starting with a smaller number like 1\n",
        "LEARNING_RATE = 2e-3 # learning rate\n",
        "\n",
        "# Instantiate and train classifier\n",
        "model_transformer = TransformerEncoderDecoder(hf_src_tokenizer, hf_tgt_tokenizer,\n",
        "  hidden_size    = 64,\n",
        "  layers         = 3,\n",
        ").to(device)\n",
        "\n",
        "model_transformer.train_all(train_iter, val_iter, epochs=EPOCHS, learning_rate=LEARNING_RATE)\n",
        "model_transformer.load_state_dict(model_transformer.best_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a05ede1",
      "metadata": {
        "id": "0a05ede1"
      },
      "source": [
        "You might notice that in these experiments training transformers doesn't appear to be faster than training RNNs. There are two reasons for that: first, we are not using GPUs; second, even if you use GPUs, the sequences here are too short to observe the benefits of parallelizing along the horizontal direction. In real datasets with long sentences, training transformers is much faster than training RNNs, so under the same computational budget, using transformers allows for training on much larger datasets. This is one of the primary reasons transformers dominate NLP research these days."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fbf95e5",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "2fbf95e5"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "**Question:** We argued above that *training* transformers can be much faster than training RNNs. What about *generation* using transformers? Would there be any speed advantage of decoding (generation) using transformers compared to RNNs? Why or why not?\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: open_response_transformer_decoding_speed\n",
        "manual: true\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b11c555",
      "metadata": {
        "id": "0b11c555"
      },
      "source": [
        "During training, transformers are significantly faster than RNNs because they process entire sequences in parallel. This allows them to utilize GPUs efficiently and handle larger datasets within the same computational budget. However, when it comes to generation (decoding), the speed advantage is not as significant.\n",
        "\n",
        "In text generation, models like GPT use auto-regressive decoding, meaning they generate one token at a time. This process is sequential since each new token depends on the previously generated tokens. Unlike training, where transformers can process all tokens simultaneously, generation cannot be fully parallelized. As a result, transformers do not have a major speed advantage over RNNs during decoding.\n",
        "\n",
        "In conclusion, while transformers are much faster for training, their speed benefit during generation is limited due to the sequential nature of the decoding process."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd2702b1",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "dd2702b1"
      },
      "source": [
        "<!-- END QUESTION -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "id": "c57c66b0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-03T08:21:04.820709Z",
          "iopub.status.busy": "2025-02-03T08:21:04.820493Z",
          "iopub.status.idle": "2025-02-03T08:21:07.492535Z",
          "shell.execute_reply": "2025-02-03T08:21:07.491662Z"
        },
        "id": "c57c66b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87850c75-9c1f-4204-8e4e-1faa48a9be7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test perplexity: 1.049\n"
          ]
        }
      ],
      "source": [
        "# Evaluate model performance, the expected value should be < 1.5\n",
        "print (f'Test perplexity: {model_transformer.evaluate_ppl(test_iter):.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "id": "a4426746",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "a4426746",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "ca37e848-d0cd-4874-949c-a9882a3c7b79"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 217
        }
      ],
      "source": [
        "grader.check(\"transformer_ppl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca23ac05",
      "metadata": {
        "id": "ca23ac05"
      },
      "source": [
        "Now that we have a trained model, we can decode from it using our previously implemented beam search function. If the code below throws any errors, you might need to modify your beam search code such that it generalizes here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "id": "98e0645e",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "98e0645e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "a8e77cd1-e13f-4055-8b5a-9bc9ffd5fbeb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 218
        }
      ],
      "source": [
        "grader.check(\"transformer_beam_search\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "id": "541e533b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-03T08:22:18.146082Z",
          "iopub.status.busy": "2025-02-03T08:22:18.145867Z",
          "iopub.status.idle": "2025-02-03T08:22:29.425140Z",
          "shell.execute_reply": "2025-02-03T08:22:29.424249Z"
        },
        "id": "541e533b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40b66ffd-36b5-4dde-d059-26495c121a42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source: sixteen thousand eight hundred and thirty two\n",
            "Prediction:   1 6 8 3 2\n",
            "Ground truth: 1 6 8 3 2\n",
            "Source: sixty seven million six hundred and eighty five thousand two hundred and thirty\n",
            "Prediction:   6 7 6 8 5 2 3 0\n",
            "Ground truth: 6 7 6 8 5 2 3 0\n",
            "Source: six thousand two hundred and twelve\n",
            "Prediction:   6 2 1 2\n",
            "Ground truth: 6 2 1 2\n",
            "Source: seven hundred and ninety eight million three hundred and thirty one thousand eight hundred and eighteen\n",
            "Prediction:   7 9 8 3 3 1 8 1 8\n",
            "Ground truth: 7 9 8 3 3 1 8 1 8\n",
            "Source: eighty eight million four hundred and thirteen thousand nine hundred and eighteen\n",
            "Prediction:   8 8 4 1 3 9 1 8\n",
            "Ground truth: 8 8 4 1 3 9 1 8\n",
            "Source: three hundred and seventy four thousand two hundred and seventy\n",
            "Prediction:   3 7 4 2 7 0\n",
            "Ground truth: 3 7 4 2 7 0\n",
            "Source: ninety eight million three hundred and seventy thousand five hundred and forty five\n",
            "Prediction:   9 8 3 7 0 5 4 5\n",
            "Ground truth: 9 8 3 7 0 5 4 5\n",
            "Source: ninety seven thousand seven hundred and sixty two\n",
            "Prediction:   9 7 7 6 2\n",
            "Ground truth: 9 7 7 6 2\n",
            "Source: four hundred and ten thousand two hundred and three\n",
            "Prediction:   4 1 0 2 0 3\n",
            "Ground truth: 4 1 0 2 0 3\n",
            "Accuracy: 0.89\n"
          ]
        }
      ],
      "source": [
        "DEBUG_FIRST = 10 # set to False to disable printing predictions\n",
        "K = 1 # beam size 1\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# create beam searcher\n",
        "beam_searcher = BeamSearcher(model_transformer)\n",
        "\n",
        "for index, batch in enumerate(test_iter, start=1):\n",
        "  # Input and output\n",
        "  src = batch['src_ids']\n",
        "  src_lengths = batch['src_lengths']\n",
        "  # Predict\n",
        "  model.all_attns = []\n",
        "  prediction, _ = beam_searcher.beam_search(src, src_lengths, K)\n",
        "  # Convert to string\n",
        "  prediction = hf_tgt_tokenizer.decode(prediction, skip_special_tokens=True)\n",
        "  ground_truth = hf_tgt_tokenizer.decode(batch['tgt_ids'][0], skip_special_tokens=True)\n",
        "  if DEBUG_FIRST > index:\n",
        "    src = hf_src_tokenizer.decode(src[0], skip_special_tokens=True)\n",
        "    print (f'Source: {src}')\n",
        "    print (f'Prediction:   {prediction}')\n",
        "    print (f'Ground truth: {ground_truth}')\n",
        "  if ground_truth == prediction:\n",
        "    correct += 1\n",
        "  total += 1\n",
        "\n",
        "print (f'Accuracy: {correct/total:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f2db02f",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "5f2db02f"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "**Question:** When we discussed attention above, adding it to an RNN model, we noted that\n",
        "\n",
        "> The attention scores $\\mathbf{a}$ lie on a *simplex* (meaning $a_i\\ge 0$ and $\\sum_i a_i=1$), which lends it some interpretability: the closer $a_i$ is to 1, the more \"relevant\" a key $k_i$ (and hence its value $v_i$) is to the given query. We will observe this later in the lab: When we are about to predict the target word \"3\", $a_i$ is close to 1 for the source word $x_i=\\text{\"three\"}$.\n",
        "\n",
        "Can we interpret the attentions in a multi-layer transformer similarly? If so, what would you expect the attention scores to correspond to? If not, explain why.\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: open_response_attn_transformer\n",
        "manual: true\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b428f663",
      "metadata": {
        "id": "b428f663"
      },
      "source": [
        "In a multi-layer transformer, attention scores can still be interpreted, but their meaning is more complex than in an RNN with attention.\n",
        "\n",
        "In an RNN with attention, the scores\n",
        "𝛼\n",
        "𝑖\n",
        "α\n",
        "i\n",
        "​\n",
        "  clearly indicate which input tokens are most relevant to the current decoding step. Since attention is applied directly to the input sequence, we can interpret high attention weights as a strong alignment between specific input words and output words.\n",
        "\n",
        "In a transformer, attention is applied at multiple layers and heads, meaning that each head in each layer can focus on different aspects of the input. Some heads might capture syntactic relationships (e.g., subject-verb connections), while others might capture semantic similarities. Because of this complexity, attention scores in transformers do not always provide a clear, single alignment between input and output words.\n",
        "\n",
        "However, we can still analyze attention maps to gain insights into how the model processes information. For example, in early layers, attention scores may correspond to local word dependencies, while in later layers, they might capture broader sentence-level meanings.\n",
        "\n",
        "In conclusion, while attention scores in transformers can still be analyzed, they do not have the same direct interpretability as in an RNN with attention, because they operate across multiple layers and attention heads, capturing different types of relationships in the data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8caed012",
      "metadata": {
        "id": "8caed012"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "\n",
        "\n",
        "You might have noticed that the transformer model underperforms the RNN-based encoder-decoder on this particular task. This might be due to several reasons:\n",
        "\n",
        "* Transformers tend to be data hungry, sometimes requiring billions of words to train.\n",
        "* The transformer formulation presented in this lab is not in its full form: for instance, instead of only doing attention once at each position for each layer, researchers usually use multiple attention operations in the hope of capturing different aspects of \"relevance\", which is called \"multi-headed attention\". For example, one attention head might be focusing on pronoun resolution, while the other might be looking for similar contexts before.\n",
        "* Transformers are usually sensitive to hyper-parameters and require heavy tuning. For example, while we used a fixed learning rate, researchers usually use a customized learning rate scheduler which first warms up the learning rate, and then gradually decreases it. If you are interested, more details can be found in [the original paper](https://arxiv.org/abs/1706.03762).\n",
        "\n",
        "In real-world applications, many state-of-the-art NLP approaches are based on transformers. For further (relatively old, but still very relevant) readings if you are interested, we recommend [BERT](https://arxiv.org/abs/1810.04805) and [GPT-3](https://arxiv.org/abs/2005.14165)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d7cbbc8",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "4d7cbbc8"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "# Lab debrief\n",
        "\n",
        "**Question:** We're interested in any thoughts you have about this lab so that we can improve this lab for later years, and to inform later labs for this year. Please list any issues that arose or comments you have to improve the lab. Useful things to comment on might include the following, but you're not restricted to these:\n",
        "\n",
        "* What *specific single* change to the lab would have made your learning more efficient? This might be an addition of a concept that was not explained, or an example that would clarify a concept, or a problem that would have captured a concept in a better way, or anything else you can think of that would have made this a better lab.\n",
        "* Was the lab too long or too short?\n",
        "* Were the readings appropriate for the lab?\n",
        "* Was it clear (at least after you completed the lab) what the points of the exercises were?\n",
        "* Are there additions or changes you think would make the lab better?\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: open_response_debrief\n",
        "manual: true\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eca292ee",
      "metadata": {
        "id": "eca292ee"
      },
      "source": [
        "This lab was a valuable learning experience, allowing me to explore key concepts related to transformers and attention mechanisms. The exercises provided a good balance between theory and practice, helping to reinforce my understanding through hands-on implementation.\n",
        "\n",
        "One of the strongest aspects of the lab was how it demonstrated the differences between RNN-based attention and transformer attention. It made clear why transformers are more efficient for certain tasks and how attention mechanisms evolve across multiple layers. The structured exercises guided me well through the concepts, making them more concrete.\n",
        "\n",
        "However, there were some areas that could be improved. The lab was a bit long, and some explanations could have been more concise to improve efficiency. A few additional clarifications, especially on how to interpret multi-head attention weights in deep transformers, would have been helpful. An example with a more intuitive breakdown of attention scores across layers could make the learning process even smoother.\n",
        "\n",
        "Overall, the lab was well-designed and provided a solid foundation for understanding attention mechanisms in modern deep learning architectures."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3774fcf6",
      "metadata": {
        "id": "3774fcf6"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "\n",
        "\n",
        "# End of Lab 4-4 {-}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f76c23e7",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "f76c23e7"
      },
      "source": [
        "---\n",
        "\n",
        "To double-check your work, the cell below will rerun all of the autograder tests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "id": "127bb91e",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "127bb91e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "318740bf-5098-4a00-8222-4821ee0d762b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "beam_search:\n",
              "\n",
              "    All tests passed!\n",
              "    \n",
              "\n",
              "causal_attention_mask:\n",
              "\n",
              "    All tests passed!\n",
              "    \n",
              "\n",
              "encoder_decoder_ppl:\n",
              "\n",
              "    All tests passed!\n",
              "    \n",
              "\n",
              "transformer_beam_search:\n",
              "\n",
              "    All tests passed!\n",
              "    \n",
              "\n",
              "transformer_ppl:\n",
              "\n",
              "    All tests passed!\n",
              "    \n"
            ],
            "text/html": [
              "<p><strong>beam_search:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n",
              "<p><strong>causal_attention_mask:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n",
              "<p><strong>encoder_decoder_ppl:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n",
              "<p><strong>transformer_beam_search:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n",
              "<p><strong>transformer_ppl:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n"
            ]
          },
          "metadata": {},
          "execution_count": 220
        }
      ],
      "source": [
        "grader.check_all()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "236299",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "title": "CS187 Lab 4-5: Sequence-to-sequence models with attention",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2d9ad64d58da471b8f8790f0fe815232": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5fc059065cfd4c42b939e79112dfabd6",
              "IPY_MODEL_506717f4061c4619a721ce8e3f4844b1",
              "IPY_MODEL_aedd41dd02694af2b819729e04ea4fa5"
            ],
            "layout": "IPY_MODEL_a3da00aaf77f440d8585c3c4703052f7"
          }
        },
        "5fc059065cfd4c42b939e79112dfabd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b5c59b5556046548b4e8680cde508ea",
            "placeholder": "​",
            "style": "IPY_MODEL_15d9ad1f824b47adad2ddbaf336dd6be",
            "value": "Generating train split: "
          }
        },
        "506717f4061c4619a721ce8e3f4844b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be8280ed4e99468d885cb7d2777406e1",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_194d78b24b144db08c4bc2a65cc3a16e",
            "value": 1
          }
        },
        "aedd41dd02694af2b819729e04ea4fa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f08ee8ed36ca41ab8a11e14ea1c31e2c",
            "placeholder": "​",
            "style": "IPY_MODEL_c195e1c8a55b49a9adca7f49c6de447c",
            "value": " 65022/0 [00:00&lt;00:00, 168614.90 examples/s]"
          }
        },
        "a3da00aaf77f440d8585c3c4703052f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b5c59b5556046548b4e8680cde508ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15d9ad1f824b47adad2ddbaf336dd6be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be8280ed4e99468d885cb7d2777406e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "194d78b24b144db08c4bc2a65cc3a16e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f08ee8ed36ca41ab8a11e14ea1c31e2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c195e1c8a55b49a9adca7f49c6de447c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fcf8752babb44cce957ca82e05084730": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_da3630d237e14b0faaa3f15b743139b6",
              "IPY_MODEL_1d6a6a4849734501afdc39551110f362",
              "IPY_MODEL_a6d8a8d74ccb4703b63bfbc5ee705361"
            ],
            "layout": "IPY_MODEL_f36b397db4a44747a3f4c81ef298d51e"
          }
        },
        "da3630d237e14b0faaa3f15b743139b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_852a37eeeac04d63b268b9e837b5a7a1",
            "placeholder": "​",
            "style": "IPY_MODEL_6a17303d2b994d17bc9ec1dd910f7b7d",
            "value": "Generating val split: "
          }
        },
        "1d6a6a4849734501afdc39551110f362": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bfa7ea3a6993419d9aeb2c64e12736b2",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1ff11861244b462d9be647c438c32225",
            "value": 1
          }
        },
        "a6d8a8d74ccb4703b63bfbc5ee705361": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6fbf655a6b36422f81c34995fa8bb6b0",
            "placeholder": "​",
            "style": "IPY_MODEL_18c4ebb3c4874691bba588856b0a737c",
            "value": " 700/0 [00:00&lt;00:00, 7938.77 examples/s]"
          }
        },
        "f36b397db4a44747a3f4c81ef298d51e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "852a37eeeac04d63b268b9e837b5a7a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a17303d2b994d17bc9ec1dd910f7b7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bfa7ea3a6993419d9aeb2c64e12736b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "1ff11861244b462d9be647c438c32225": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6fbf655a6b36422f81c34995fa8bb6b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18c4ebb3c4874691bba588856b0a737c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d2d8e18180334e43bcd1bc8c5f96df3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2e28b3a7f07543f5a5397e0808b42634",
              "IPY_MODEL_6fa5aa292094453382abbb86f0825908",
              "IPY_MODEL_76aba658d6b74c85b208024ff2d012e2"
            ],
            "layout": "IPY_MODEL_191cd842d284466aa04bbcff95911ec8"
          }
        },
        "2e28b3a7f07543f5a5397e0808b42634": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b61da649ea434eb780bcd0c575894f1a",
            "placeholder": "​",
            "style": "IPY_MODEL_2d23866d7e4c4302a12205e37f8715f2",
            "value": "Generating test split: "
          }
        },
        "6fa5aa292094453382abbb86f0825908": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9977b0d3f6d405aa3c96f253b977b7f",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cadb85842aec4ad5a757594af82bd251",
            "value": 1
          }
        },
        "76aba658d6b74c85b208024ff2d012e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b19517bb6944a28bb24a41cbb556d00",
            "placeholder": "​",
            "style": "IPY_MODEL_a07b51b4f6134da2adb305469fcd6be9",
            "value": " 700/0 [00:00&lt;00:00, 7971.88 examples/s]"
          }
        },
        "191cd842d284466aa04bbcff95911ec8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b61da649ea434eb780bcd0c575894f1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d23866d7e4c4302a12205e37f8715f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f9977b0d3f6d405aa3c96f253b977b7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "cadb85842aec4ad5a757594af82bd251": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7b19517bb6944a28bb24a41cbb556d00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a07b51b4f6134da2adb305469fcd6be9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bce6a31ddab14b058b26cfde8227c7b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cef0adb956fd4b73958c19fee46e0fe2",
              "IPY_MODEL_45785db7487242c794f3a3263ef799e8",
              "IPY_MODEL_0029f49d79d84299a275ab7460df7076"
            ],
            "layout": "IPY_MODEL_908df4d697204b45b4da54dce30c2370"
          }
        },
        "cef0adb956fd4b73958c19fee46e0fe2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7392551ffd374c1082d7efe575de40a9",
            "placeholder": "​",
            "style": "IPY_MODEL_adc1ce8ebe5f48d09542322cc073865f",
            "value": "Map: 100%"
          }
        },
        "45785db7487242c794f3a3263ef799e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_549cbbbadb77428c97834ecc4c484fce",
            "max": 65022,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_32b48bff085f4fdab90dbf80dda45a67",
            "value": 65022
          }
        },
        "0029f49d79d84299a275ab7460df7076": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb3d3d1116ca4aebad580adbdb92d790",
            "placeholder": "​",
            "style": "IPY_MODEL_328b52e075ef4372b30fc266754b798d",
            "value": " 65022/65022 [00:31&lt;00:00, 2822.67 examples/s]"
          }
        },
        "908df4d697204b45b4da54dce30c2370": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7392551ffd374c1082d7efe575de40a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adc1ce8ebe5f48d09542322cc073865f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "549cbbbadb77428c97834ecc4c484fce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32b48bff085f4fdab90dbf80dda45a67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fb3d3d1116ca4aebad580adbdb92d790": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "328b52e075ef4372b30fc266754b798d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2cad6f20488c4758ae2a4846f414924c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9334b665a6d94bafbced613a149c2c5e",
              "IPY_MODEL_d5731c336b484cce9a3a0bb3b182c4ec",
              "IPY_MODEL_1c003eb3f7a644fe808df053121ce636"
            ],
            "layout": "IPY_MODEL_6e43480232794f4ca89b349bcf8d4ee4"
          }
        },
        "9334b665a6d94bafbced613a149c2c5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ab7ef4dd4da43a4a7e8940392ce2e16",
            "placeholder": "​",
            "style": "IPY_MODEL_a1301694c812416dbd227ea01bfee594",
            "value": "Map: 100%"
          }
        },
        "d5731c336b484cce9a3a0bb3b182c4ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b08baacda7504d5ea5f2fd490c874094",
            "max": 700,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2f331d90f33d4017a5ccbd4fe8ac32cd",
            "value": 700
          }
        },
        "1c003eb3f7a644fe808df053121ce636": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7944c8b7a6bd4e30940aea29fe057afa",
            "placeholder": "​",
            "style": "IPY_MODEL_0d7509da3a0d4e979c1a9015aab30e9e",
            "value": " 700/700 [00:00&lt;00:00, 3240.48 examples/s]"
          }
        },
        "6e43480232794f4ca89b349bcf8d4ee4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ab7ef4dd4da43a4a7e8940392ce2e16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1301694c812416dbd227ea01bfee594": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b08baacda7504d5ea5f2fd490c874094": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f331d90f33d4017a5ccbd4fe8ac32cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7944c8b7a6bd4e30940aea29fe057afa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d7509da3a0d4e979c1a9015aab30e9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "738149b8f23c4737b540047b215f44d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c2e3aa0f31634404abebe159882a9c5c",
              "IPY_MODEL_0359d67730d74a31b639f10316f02683",
              "IPY_MODEL_8cacb7fac2c4455aa4a831774023af1c"
            ],
            "layout": "IPY_MODEL_2c9ea7f11764424d8731dd893693ce9c"
          }
        },
        "c2e3aa0f31634404abebe159882a9c5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e9282c1e5554442a2786de175698724",
            "placeholder": "​",
            "style": "IPY_MODEL_4c66a900283147a78be308e9860b7bf2",
            "value": "Map: 100%"
          }
        },
        "0359d67730d74a31b639f10316f02683": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_494dd4c476474c6d852e66ab46131a10",
            "max": 700,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_323ad88270b940ab9084aec5ce2319c3",
            "value": 700
          }
        },
        "8cacb7fac2c4455aa4a831774023af1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bae4b13897ae4a7fa545d488917b3b79",
            "placeholder": "​",
            "style": "IPY_MODEL_1e05916022f047719e491a5bda45fb78",
            "value": " 700/700 [00:00&lt;00:00, 2773.88 examples/s]"
          }
        },
        "2c9ea7f11764424d8731dd893693ce9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e9282c1e5554442a2786de175698724": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c66a900283147a78be308e9860b7bf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "494dd4c476474c6d852e66ab46131a10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "323ad88270b940ab9084aec5ce2319c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bae4b13897ae4a7fa545d488917b3b79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e05916022f047719e491a5bda45fb78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}